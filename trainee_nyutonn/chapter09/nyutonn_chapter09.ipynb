{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1f70bd",
   "metadata": {},
   "source": [
    "<h2 id=\"80-id番号への変換\">80. ID番号への変換</h2>\n",
    "<p>問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に<code class=\"language-plaintext highlighter-rouge\">1</code>，2番目に頻出する単語に<code class=\"language-plaintext highlighter-rouge\">2</code>，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて<code class=\"language-plaintext highlighter-rouge\">0</code>とせよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bdf237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10684 1336 1336\n",
      "10684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>409106</th>\n",
       "      <td>Selena Gomez exposes her derriere in VERY shor...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290867</th>\n",
       "      <td>Hillshire Says Tyson Foods Bid Superior to Pin...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36532</th>\n",
       "      <td>'Friends saw him hit me': Johnny Weir opens up...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358830</th>\n",
       "      <td>'As funny as a liver transplant!' Melissa McCa...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67622</th>\n",
       "      <td>Piers Morgan Delivers One Final Blow To Gun Vi...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    TITLE CATEGORY\n",
       "409106  Selena Gomez exposes her derriere in VERY shor...        e\n",
       "290867  Hillshire Says Tyson Foods Bid Superior to Pin...        b\n",
       "36532   'Friends saw him hit me': Johnny Weir opens up...        e\n",
       "358830  'As funny as a liver transplant!' Melissa McCa...        e\n",
       "67622   Piers Morgan Delivers One Final Blow To Gun Vi...        e"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ分割\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# csvファイルを読み込む\n",
    "path = \"/Users/nyuton/Documents/100knock-2023/trainee_nyutonn/chapter08/data/newsCorpora.csv\"\n",
    "df = pd.read_table(path, header=None, sep='\\\\t', engine='python')\n",
    "df.columns = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "\n",
    "# PUBLISHERが特定の行のみを取り出す\n",
    "publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n",
    "daily_mails = df[df['PUBLISHER'].isin(publishers)]\n",
    "\n",
    "# 訓練データ、検証データ、テストデータに分ける\n",
    "train_data, non_train, train_target, non_train_target = train_test_split(daily_mails[['TITLE', 'CATEGORY']], daily_mails['CATEGORY'], train_size=0.8, random_state=10, stratify=daily_mails['CATEGORY'])\n",
    "valid_data, test_data, valid_target, test_target = train_test_split(non_train, non_train_target, train_size=0.5, random_state=10,  stratify=non_train_target)\n",
    "print(len(train_data), len(valid_data), len(test_data))\n",
    "\n",
    "# テキストファイルに書き込む\n",
    "train_data.to_csv('work/train.txt', header=None, index=None, sep='\\t')\n",
    "valid_data.to_csv('work/valid.txt', header=None, index=None, sep='\\t')\n",
    "test_data.to_csv('work/test.txt', header=None, index=None, sep='\\t')\n",
    "print(len(train_data))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1574a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の回数を数え上げ\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "vocab = defaultdict(int)\n",
    "for id, (title, category) in train_data.iterrows():\n",
    "    # words = title.split()\n",
    "    words = word_tokenize(title)\n",
    "    for word in words:\n",
    "        vocab[word] += 1\n",
    "vocab = Counter(vocab)\n",
    "# vocab.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71ee98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kim Kardashian Takes The Plunge In A Simple Black Tee\n",
      "[39, 35, 581, 14, 4855, 20, 24, 8956, 794, 0]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 単語列から出現頻度インデックスを返す関数\n",
    "def sentence2index(sentence):\n",
    "    # 文を単語列に分割\n",
    "    words = word_tokenize(sentence)\n",
    "    # 単語のみのリストに分割する\n",
    "    vocab_order, cnt_list = zip(*vocab.most_common())\n",
    "    index_output = []\n",
    "    for word in words:\n",
    "        # 語彙にないときは 0\n",
    "        if word not in vocab:\n",
    "            index = 0\n",
    "        # 回数が1のときも0\n",
    "        elif cnt_list[vocab_order.index(word)] == 1:\n",
    "            index = 0\n",
    "        # 語彙にあるとき，0 インデックスなので +1 する\n",
    "        else:  \n",
    "            index = vocab_order.index(word) + 1\n",
    "\n",
    "        index_output.append(index)\n",
    "    return index_output\n",
    "\n",
    "\n",
    "sentence = \"Kim Kardashian Takes The Plunge In A Simple Black Tee\"\n",
    "print(sentence)\n",
    "print(sentence2index(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb47809c",
   "metadata": {},
   "source": [
    "<h2 id=\"81-rnnによる予測\">81. RNNによる予測</h2>\n",
    "<p>ID番号で表現された単語列\\(\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)\\)がある．ただし，\\(T\\)は単語列の長さ，\\(x_t \\in \\mathbb{R}^{V}\\)は単語のID番号のone-hot表記である（\\(V\\)は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列\\(\\boldsymbol{x}\\)からカテゴリ\\(y\\)を予測するモデルとして，次式を実装せよ．</p>\n",
    "\n",
    "<p>\\[\\overrightarrow{h}_0 = 0, \\\\\n",
    "\\overrightarrow{h}_t = {\\rm \\overrightarrow{RNN}}(\\mathrm{emb}(x_t), \\overrightarrow{h}_{t-1}), \\\\\n",
    "y = {\\rm softmax}(W^{(yh)} \\overrightarrow{h}_T + b^{(y)}))\\]</p>\n",
    "\n",
    "<p>ただし，\\(\\mathrm{emb}(x) \\in \\mathbb{R}^{d_w}\\)は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），\\(\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}\\)は時刻\\(t\\)の隠れ状態ベクトル，\\({\\rm \\overrightarrow{RNN}}(x,h)\\)は入力\\(x\\)と前時刻の隠れ状態\\(h\\)から次状態を計算するRNNユニット，\\(W^{(yh)} \\in \\mathbb{R}^{L \\times d_h}\\)は隠れ状態ベクトルからカテゴリを予測するための行列，\\(b^{(y)} \\in \\mathbb{R}^{L}\\)はバイアス項である（\\(d_w, d_h, L\\)はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニット\\({\\rm \\overrightarrow{RNN}}(x,h)\\)には様々な構成が考えられるが，典型例として次式が挙げられる．</p>\n",
    "\n",
    "<p>\\[{\\rm \\overrightarrow{RNN}}(x,h) = g(W^{(hx)} x + W^{(hh)}h + b^{(h)}))\\]</p>\n",
    "\n",
    "<p>ただし，\\(W^{(hx)} \\in \\mathbb{R}^{d_h \\times d_w}，W^{(hh)} \\in \\mathbb{R}^{d_h \\times d_h}, b^{(h)} \\in \\mathbb{R}^{d_h}\\)はRNNユニットのパラメータ，\\(g\\)は活性化関数（例えば\\(\\tanh\\)やReLUなど）である．</p>\n",
    "<p>なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータで\\(y\\)を計算するだけでよい．次元数などのハイパーパラメータは，\\(d_w = 300, d_h=50\\)など，適当な値に設定せよ（以降の問題でも同様である）．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "814548e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4) -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        # batch_first とは？\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        self.func = nn.Linear(hidden_size, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.batch_size = x.size()[0]\n",
    "        # h0 = self.init_hidden(x.device)  # h0 の初期化 ゼロベクトル\n",
    "        h0 = torch.zeros(1, self.hidden_size)\n",
    "        # h0 = torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "\n",
    "        # print(h_last.shape)\n",
    "        # print(h_last[:, -1].shape)\n",
    "        # print(h_last[:, -1])\n",
    "        # out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        out = self.func(h_last) #現在のhだけ取り出す\n",
    "        return out\n",
    "\n",
    "    # 隠れ層の初期化\n",
    "    def init_hidden(self, device):\n",
    "        hidden = torch.zeros(1, self.batch_size, self.hidden_size, device=device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1787e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # len(Dataset) で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    # Dataset[index] で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        text = self.X.iloc[index]\n",
    "        input_features = self.tokenizer(text)\n",
    "        label = self.y.iloc[index]\n",
    "        return {\n",
    "            'inputs': torch.tensor(input_features, dtype=torch.int64),\n",
    "            'labels': torch.tensor(label, dtype=torch.int64)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8b7ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10684\n",
      "{'inputs': tensor([1824,   58, 1825, 1016,  556, 6650,    2, 2535,  117]),\n",
      " 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# ラベル\n",
    "category_dict = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "y_train = train_data['CATEGORY'].map(category_dict)\n",
    "y_valid = valid_data['CATEGORY'].map(category_dict)\n",
    "y_test = test_data['CATEGORY'].map(category_dict)\n",
    "\n",
    "# 特徴量データセット\n",
    "X_train = CreateDataset(train_data['TITLE'], y_train, sentence2index)\n",
    "X_valid = CreateDataset(valid_data['TITLE'], y_valid, sentence2index)\n",
    "X_test = CreateDataset(test_data['TITLE'], y_test, sentence2index)\n",
    "\n",
    "# 使い方の例\n",
    "from pprint import pprint\n",
    "print(len(X_train))\n",
    "pprint(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fea873b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0番目\n",
      "入力ベクトル：{'inputs': tensor([ 145,  153, 6648,   59, 5079,    6, 2950, 1823,    0,  741, 6649,    2,\n",
      "          23,    3]), 'labels': tensor(2)}\n",
      "出力ベクトル：tensor([[-0.3896,  0.8900, -0.2229,  0.7795]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル　：1\n",
      "正解ラベル　：2\n",
      "1番目\n",
      "入力ベクトル：{'inputs': tensor([1824,   58, 1825, 1016,  556, 6650,    2, 2535,  117]), 'labels': tensor(0)}\n",
      "出力ベクトル：tensor([[ 0.3870,  0.4830, -0.5279,  0.1314]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル　：1\n",
      "正解ラベル　：0\n",
      "2番目\n",
      "入力ベクトル：{'inputs': tensor([6651, 6652, 1263,  224, 1637,    5,    7,  324, 4066,  533,   43,  169,\n",
      "         142,    0,    0,    5,   33,    3]), 'labels': tensor(2)}\n",
      "出力ベクトル：tensor([[-0.1642,  0.7868, -0.0113,  0.8425]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル　：3\n",
      "正解ラベル　：2\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "\n",
    "model = RNN(vocab_size, padding_idx, emb_size, hidden_size, n_labels)\n",
    "# model.eval()\n",
    "\n",
    "# 先頭3件の入出力を表示\n",
    "for i in range(3):\n",
    "    print(f\"{i}番目\")\n",
    "    print(f\"入力ベクトル：{X_train[i]}\")\n",
    "    print(f\"出力ベクトル：{model(X_train[i]['inputs'])}\")\n",
    "    print(f\"予測ラベル　：{model(X_train[i]['inputs']).argmax()}\")\n",
    "    print(f\"正解ラベル　：{X_train[i]['labels'].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c80b578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14])\n",
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# 確認 入力サイズがちがう．．．\n",
    "print(X_train[0]['inputs'].shape)\n",
    "print(X_train[1]['inputs'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa6347",
   "metadata": {},
   "source": [
    "<h2 id=\"82-確率的勾配降下法による学習\">82. 確率的勾配降下法による学習</h2>\n",
    "<p>確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1c985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 0.9860524150169131, train_acc = 0.6433919880194684, valid_loss = 0.9014563560207001, valid_acc = 0.687874251497006\n",
      "epoch1: train_loss = 0.8265681704164572, train_acc = 0.7137776113815051, valid_loss = 0.8279604763203875, valid_acc = 0.7200598802395209\n",
      "epoch2: train_loss = 0.7158381654228584, train_acc = 0.7523399475851741, valid_loss = 0.7523488583409891, valid_acc = 0.7372754491017964\n",
      "epoch3: train_loss = 0.6457718780873205, train_acc = 0.7710595282665669, valid_loss = 0.7497719074974135, valid_acc = 0.7312874251497006\n",
      "epoch4: train_loss = 0.6039787394605203, train_acc = 0.7903406963684013, valid_loss = 0.7173522087295043, valid_acc = 0.7574850299401198\n",
      "epoch5: train_loss = 0.56852603803889, train_acc = 0.7982029202545863, valid_loss = 0.8042106931848928, valid_acc = 0.7095808383233533\n",
      "epoch6: train_loss = 0.5483809195270353, train_acc = 0.8032572070385623, valid_loss = 0.8111276710339721, valid_acc = 0.7350299401197605\n",
      "epoch7: train_loss = 0.5061867906124584, train_acc = 0.8206664170722576, valid_loss = 0.7936406573299699, valid_acc = 0.7297904191616766\n",
      "epoch8: train_loss = 0.48014032383033367, train_acc = 0.8251591164357919, valid_loss = 0.8343401556341556, valid_acc = 0.7148203592814372\n",
      "epoch9: train_loss = 0.500744450807042, train_acc = 0.8213216023961063, valid_loss = 0.7985898589726994, valid_acc = 0.7327844311377245\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(model, output_path, total_epochs):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 指定した epoch 数だけ学習\n",
    "    for epoch in range(total_epochs):\n",
    "        train_total_loss = 0.\n",
    "        train_acc_cnt = 0\n",
    "\n",
    "        # パラメータ更新\n",
    "        model.train()\n",
    "        for data in X_train:\n",
    "            # print(data['inputs'])\n",
    "            # print(model(data['inputs']))\n",
    "            # print(data['labels'])\n",
    "            x = data['inputs']\n",
    "            y = data['labels']\n",
    "            y_pred = model(x)[0]\n",
    "            loss = loss_func(y_pred, y)  # 損失計算\n",
    "            optimizer.zero_grad()  # 勾配の初期化\n",
    "            loss.backward()  # 勾配計算\n",
    "            optimizer.step()  # パラメータ修正\n",
    "            train_total_loss += loss.item()\n",
    "\n",
    "            # 正解率の計算  # ここで計算するのはまずいかも，学習エポックが終わってからやったほうがよさそう\n",
    "            # 次の問題からは修正\n",
    "            if y.item() == y_pred.argmax():\n",
    "                train_acc_cnt += 1\n",
    "\n",
    "        # valid のロスと正解率の計算\n",
    "        model.eval()\n",
    "        valid_acc_cnt = 0\n",
    "        valid_total_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for data in X_valid:\n",
    "                x = data['inputs']\n",
    "                y = data['labels']\n",
    "                y_pred = model(x)[0]\n",
    "                loss = loss_func(y_pred, y)  # 損失計算\n",
    "                optimizer.zero_grad()  # 勾配の初期化\n",
    "                # loss.backward()  # 勾配計算\n",
    "                # optimizer.step()  # パラメータ修正\n",
    "                valid_total_loss += loss.item()\n",
    "\n",
    "                # 正解率の計算\n",
    "                if y.item() == y_pred.argmax():\n",
    "                    valid_acc_cnt += 1\n",
    "\n",
    "        # 表示\n",
    "        train_ave_loss = train_total_loss / len(X_train)\n",
    "        train_acc = train_acc_cnt / len(X_train)\n",
    "        valid_ave_loss = valid_total_loss / len(X_valid)\n",
    "        valid_acc = valid_acc_cnt / len(X_valid)\n",
    "        print(f\"epoch{epoch}: train_loss = {train_ave_loss}, train_acc = {train_acc}, valid_loss = {valid_ave_loss}, valid_acc = {valid_acc}\")\n",
    "\n",
    "    # パラメータを保存\n",
    "    torch.save(model.state_dict(), output_path)\n",
    "\n",
    "\n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "\n",
    "model = RNN(vocab_size, padding_idx, emb_size, hidden_size, n_labels)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, output_path, total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68116e3",
   "metadata": {},
   "source": [
    "<h2 id=\"83-ミニバッチ化gpu上での学習\">83. ミニバッチ化・GPU上での学習</h2>\n",
    "<p>問題82のコードを改変し，\\(B\\)事例ごとに損失・勾配を計算して学習を行えるようにせよ（\\(B\\)の値は適当に選べ）．また，GPU上で学習を実行せよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304325e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUにしても早くなってないような気がする．．．\n",
    "# 走らせながら正答率を計測する valid acc と 最後にまとめて計算する valid acc2 の結果が異なるのがかなり気になる．．．\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4, batch_size=64, device='cpu') -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        # batch_first とは？ -> batch_size と emb の２次元目のサイズが異なるときに合わせている？？\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        self.func = nn.Linear(hidden_size, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # バッチサイズを固定すると，一番最後の余りの分がおかしくなるので，動的に毎回決める！\n",
    "        self.batch_size = x.size()[0]\n",
    "        h0 = torch.zeros(1, self.batch_size, self.hidden_size, device=self.device) # ここを変更した\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "        out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す # ここを変更した\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        # out = self.func(h_last) #現在のhだけ取り出す\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762221e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # len(Dataset) で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    # Dataset[index] で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        titles = self.X.iloc[index]\n",
    "        # スライス記法のとき\n",
    "        if type(index) == slice:\n",
    "            labels = self.y.iloc[index]\n",
    "            input_features = []\n",
    "            labels_tensor = []\n",
    "            for title, label in zip(titles, labels):\n",
    "                input_feature = torch.tensor(self.tokenizer(title))\n",
    "                input_features.append(input_feature)\n",
    "                labels_tensor.append(torch.tensor(label, dtype=torch.int64))\n",
    "        else:\n",
    "            text = self.X.iloc[index]\n",
    "            input_features = torch.tensor(self.tokenizer(text), dtype=torch.int64)\n",
    "            labels_tensor = torch.tensor(self.y.iloc[index], dtype=torch.int64)\n",
    "        return {\n",
    "            'inputs': input_features,\n",
    "            'labels': labels_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = CreateDataset(train_data['TITLE'], y_train, sentence2index)\n",
    "X_valid = CreateDataset(valid_data['TITLE'], y_valid, sentence2index)\n",
    "X_test = CreateDataset(test_data['TITLE'], y_test, sentence2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce2ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': tensor([ 145,  153, 6648,   59, 5079,    6, 2950, 1823,    0,  741, 6649,    2,\n",
      "          23,    3]),\n",
      " 'labels': tensor(2)}\n",
      "{'inputs': [tensor([ 145,  153, 6648,   59, 5079,    6, 2950, 1823,    0,  741, 6649,    2,\n",
      "          23,    3]),\n",
      "            tensor([1824,   58, 1825, 1016,  556, 6650,    2, 2535,  117]),\n",
      "            tensor([6651, 6652, 1263,  224, 1637,    5,    7,  324, 4066,  533,   43,  169,\n",
      "         142,    0,    0,    5,   33,    3])],\n",
      " 'labels': [tensor(2), tensor(0), tensor(2)]}\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "pprint(X_train[0])\n",
    "pprint(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3fc1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サーバ上で動かなかった問題解決 -> model.to('cpu') を model に入れていなかった -> 正） model = model.to('cpu')\n",
    "# 今度はhiddenとinputのdeviceが違うというエラーが発生 -> xiをcudaに戻したら解決\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def measure_acc(model, X, y, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_y = []\n",
    "        for xi in X:\n",
    "            xi = xi.to(device)\n",
    "            pred_yi = model(xi[None]).argmax()\n",
    "            pred_yi = pred_yi.to('cpu')\n",
    "            pred_y.append(pred_yi)\n",
    "    return accuracy_score(pred_y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "e01368f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修正版\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader # データローダ使ってみる\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, train_loader, valid_loader, output_path, total_epochs, device, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    # 指定した epoch 数だけ学習\n",
    "    for epoch in range(total_epochs):\n",
    "        train_total_loss = 0.\n",
    "        # train_acc_cnt = 0\n",
    "\n",
    "        # パラメータ更新\n",
    "        model.train()\n",
    "        for data in tqdm(train_loader):\n",
    "            x = data['inputs']\n",
    "            x = x.to(device)\n",
    "            y = data['labels']\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # バッチの中で損失計算\n",
    "            train_loss = 0.\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                loss_i = loss_func(yi_pred, yi)\n",
    "                train_loss += loss_i\n",
    "            \n",
    "            optimizer.zero_grad()  # 勾配の初期化\n",
    "            train_loss.backward()  # 勾配計算\n",
    "            optimizer.step()  # パラメータ修正\n",
    "            train_total_loss += train_loss.item()\n",
    "\n",
    "            # バッチの中で正解率の計算 # ここを修正\n",
    "            # for yi, yi_pred in zip(y, y_pred):\n",
    "            #     if yi.item() == yi_pred.argmax():\n",
    "            #         train_acc_cnt += 1\n",
    "                \n",
    "        # train のロスと正解率の計算\n",
    "        model.eval()\n",
    "        train_acc = measure_acc(model, X_train[:]['inputs'], X_train[:]['labels'], device)\n",
    "\n",
    "\n",
    "        # valid のロスと正解率の計算\n",
    "        model.eval()\n",
    "        valid_acc_cnt = 0\n",
    "        valid_total_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(valid_loader):\n",
    "                x = data['inputs']\n",
    "                x = x.to(device)\n",
    "                y = data['labels']\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                # バッチの中で損失計算\n",
    "                valid_loss = 0.\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    # print(yi)\n",
    "                    # print(yi_pred)\n",
    "                    loss_i = loss_func(yi_pred, yi)\n",
    "                    valid_loss += loss_i\n",
    "\n",
    "                optimizer.zero_grad()  # 勾配の初期化\n",
    "                # valid_loss.backward()  # 勾配計算\n",
    "                # optimizer.step()  # パラメータ修正\n",
    "                valid_total_loss += valid_loss\n",
    "\n",
    "                # バッチの中で正解率の計算  # ここを修正\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    if yi.item() == yi_pred.argmax():\n",
    "                        valid_acc_cnt += 1\n",
    "\n",
    "            # valid のロスと正解率の計算\n",
    "            # valid_acc = measure_acc(model, X_valid[:]['inputs'], X_valid[:]['labels'], device)\n",
    "\n",
    "        # 表示\n",
    "        train_ave_loss = train_total_loss / len(X_train)\n",
    "        # train_acc = train_acc_cnt / len(X_train)\n",
    "        valid_ave_loss = valid_total_loss / len(X_valid)\n",
    "        valid_acc = valid_acc_cnt / len(X_valid)\n",
    "        print(f\"epoch{epoch}: train_loss = {train_ave_loss}, train_acc = {train_acc}, valid_loss = {valid_ave_loss}, valid_acc = {valid_acc}\")\n",
    "\n",
    "    # パラメータを保存\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56611e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 3 to 6 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m valid_loader \u001b[39m=\u001b[39m DataLoader(X_valid, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcollate_fn)\n\u001b[1;32m     29\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(X_test, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcollate_fn)\n\u001b[0;32m---> 31\u001b[0m model \u001b[39m=\u001b[39m RNN(vocab_size, padding_idx, emb_size, hidden_size, n_labels, batch_size, device)\n\u001b[1;32m     32\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./trained_param.npz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m total_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes from 3 to 6 positional arguments but 8 were given"
     ]
    }
   ],
   "source": [
    "# 訓練データの最後で何故かエラーが起きる -> あまりの分がおかしくなっていた！\n",
    "# ロスが下がっていない．．．なぜだ．．．\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "#バッチサイズ\n",
    "batch_size = 32\n",
    "PADDING_IDX = vocab_size = len(vocab)\n",
    "\n",
    "#ミニバッチを取り出して長さを揃える関数\n",
    "def collate_fn(batch):\n",
    "    sorted_batch = sorted(batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n",
    "    sequences = [x['inputs'] for x in sorted_batch]\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PADDING_IDX)\n",
    "    labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n",
    "    return {'inputs': sequences_padded, 'labels': labels}\n",
    "\n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = RNN(vocab_size, padding_idx, emb_size, hidden_size, n_labels, batch_size, device)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e022fb",
   "metadata": {},
   "source": [
    "<h2 id=\"84-単語ベクトルの導入\">84. 単語ベクトルの導入</h2>\n",
    "<p>事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での<a href=\"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\">学習済み単語ベクトル</a>）で単語埋め込み\\(\\mathrm{emb}(x)\\)を初期化し，学習せよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "c318b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm # 進捗表示\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 70からもってきた\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('./../chapter08/data/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "vocab_size = len(vocab) + 1\n",
    "emb_size = 300\n",
    "padding_idx = len(vocab)\n",
    "\n",
    "rnn_emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('./../chapter08/data/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "embedding_weight_matrix = rnn_emb(torch.tensor([0])).detach().numpy().copy()\n",
    "\n",
    "# ID持ちの単語が学習済み単語ベクトルを持っていればそれを行方向に足していき，なければnn.Embeddingのベクトルを行方向に足していく\n",
    "for key, value in vocab.items():\n",
    "    try:\n",
    "        embedding_weight_matrix = np.vstack((embedding_weight_matrix, word2vec_model[key]))\n",
    "    except KeyError:\n",
    "        embedding_weight_matrix = np.vstack((embedding_weight_matrix, rnn_emb(torch.tensor([value])).detach().numpy().copy()))\n",
    "    \n",
    "embedding_weight_matrix = np.vstack((embedding_weight_matrix, np.zeros(emb_size, dtype=np.float32)))  # paddingの分\n",
    "embedding_weight_matrix = torch.from_numpy(embedding_weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "9ffa989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/334 [00:01<05:47,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     7,  1710,     2,    90,    23,  1018,     0,    95,\n",
      "            94,     0,  6396,    90,  8910,    17,     3],\n",
      "        [ 1767,    17,   927,   166,  2042,    50,    24,  1099,  3192,    19,\n",
      "             0,  2484,  4319,  2201,   885,    18,     3],\n",
      "        [ 3115,  1690,    46,   100,  4067,   389,  5787,     5,     2,    90,\n",
      "            17,  5788,    13,    17,    34,   147,   707],\n",
      "        [  133,     0,  4664,  4490,     2,  3612,     5,     9,  4753,  8946,\n",
      "          1215,  2502,  1804,    22,    17,     3, 19690],\n",
      "        [  173,   475,    24,   935,  5537,  3225,  1338,   141,  7454,     4,\n",
      "           980,    49,   633,    64,   173,    32, 19690],\n",
      "        [ 1860,     5,   795,  3653,     5,     2,  1034,     9,   196,    64,\n",
      "            48,  1330,   162,    25,    48,    27, 19690],\n",
      "        [ 4753,  3344,     2,  1608,   546,     0,   284,  2052,     2,   389,\n",
      "             0,     7,   968,   621,  2779,     3, 19690],\n",
      "        [   86,    73,    11,    86,    73,  2496,  2708,  2064,    44,  8233,\n",
      "           194,   216,     4,  1946,     3, 19690, 19690],\n",
      "        [ 3333,   529,  3268,    59,  3213,     6,     0,   617,   932,     9,\n",
      "            70,  3710,    17,   270,   566, 19690, 19690],\n",
      "        [  303,   419,   186,    92,    51,     0,  1823,  2352,    23,   336,\n",
      "           276,   142,  1257,     3, 19690, 19690, 19690],\n",
      "        [ 2066,  7099,   246,   792,     4,     0,    28,   165,  7100,     1,\n",
      "             0,     0,  4206,     3, 19690, 19690, 19690],\n",
      "        [ 8706,  8707,   722,     0,    91,     2,     0,  2067,     1,  2343,\n",
      "             0,    25,    48,    27, 19690, 19690, 19690],\n",
      "        [   10,   259,    38,   168,  6388,  1418,  1220,    41,    22,  2512,\n",
      "           768,     8,  1354,  2521, 19690, 19690, 19690],\n",
      "        [  382,  2026,   404,     0,     8,   892,  9017,     1,   366,   131,\n",
      "            25,    48,    27, 19690, 19690, 19690, 19690],\n",
      "        [   10,  3992,   155,   769,     6,   229,     1,   210,   623,  1275,\n",
      "          7448,  4174, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,  1424,  2140,  2727,     2,   978,  1235,    29,  5097,\n",
      "          3646,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  234,   845,     2,  1789,   127,     2,  4139,    36,   504,     1,\n",
      "           205,  2446, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  173,   475,  2429,     0,  3725,  3200,  2782,    14,    97,  7347,\n",
      "           192,    32, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  6021,   185,  2900,     0,     5,     9,   704,     9,   229,\n",
      "          2931, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3417,   372,   143,     4,     0,    17,     0,    13,   245,  9145,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,  3263,     1,  3603,   878,    33,  1651,  3831,   133,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 5012,  2092,   139,   891,   109,   672,   224,    53,  3470,  9217,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 4107,   380,   317,   226,  1512,    13,  3466,   556,    12,  4108,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  719,  1045,     4,    51,  3524,  4627,   761,   280,     0, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3016,  4137,  5741,    14,  5155,    33,  1190,   497,  1687, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  395,    18,  5701,  5702,     2,  1377,     8,   296,  1234, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  315,     0,   105,  4012,   450,  2556,  2557,    24,  3754, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,    46,  9021,  9022,  2366,     2,  9023,  1734, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,   993,  7897,  3886,    29,  4716,  1242,  3887, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1975,  3375,    11,  1975,  3375,   373,  1343,  2640, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   14,  1291,     2,     0,   154,  3868,  1111, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,     5,     0,     0,     0,     0, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/334 [00:01<05:05,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   16,  3968,    23,     0,     7,  2696,   411,     0,     5,  2313,\n",
      "            18,  9254,   731,  1461,     4,  4485,     2,     3],\n",
      "        [   24,  2740,  2078,     7,    14,   336,  1030,  7756,  1410,  2476,\n",
      "          6256,    43,    59,   139,  2740,  3966,    12,     3],\n",
      "        [  324,   454,     6,  8230,  2920,    18,  3954,     6,  1539,    13,\n",
      "             0,  2167,   214,  6096,     5,  6097,     9,     3],\n",
      "        [ 1483,  1120,   640,  1158,  3280,    16,   190,    34,  2335,    20,\n",
      "           165,   245,  6699,     1,     0,     3, 19690, 19690],\n",
      "        [ 1330,   457,   518,  5417,     7,    38,  5418,    57,    41,  2543,\n",
      "             1,    38,  4347,    41,  5419, 19690, 19690, 19690],\n",
      "        [   39,    35,     1,    86,    73,   312,    16,  2742,     0,   107,\n",
      "           316,    25,  9244,    27, 19690, 19690, 19690, 19690],\n",
      "        [    0,  4827,    50,  2996,  2997,   171,     0,  4011,     6,  9432,\n",
      "          4576,     9,    70,     3, 19690, 19690, 19690, 19690],\n",
      "        [ 1329,   844,  2572,     2,   642,    22,  5410,   193,   156,    33,\n",
      "           467,   227,   523, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  525,     4,   535,   329,   131,   289,    44,   215,  4658,   791,\n",
      "            25,    57,    27, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  5879,   767,   348,  1752,   262,   195,  2868,   435,  3228,\n",
      "             4,  7845, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  401,   368,     7,   318,  1401,     2,    15,   520,     8,   811,\n",
      "           188,  1044, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  7399,     2,   157,  3221,  1595,  1340,  2746,    12,    19,\n",
      "           219,   232, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 4320,     2,  1448,    69,    19,   254,    91,    40,  6907,  1233,\n",
      "             6,  7065, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  119,   112,   182,     2,  5557,   233,     9,   504,   372,     8,\n",
      "          1096,   628, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  9053,     4,   113,     0,     6,   358,    12,  9054,    15,\n",
      "           126, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  107,    28,  2556,  2557,     5,  2574,     0,    82,    20,   176,\n",
      "           803, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,   295,   364,   123,   683,  1365,   740,   227,  5910,  1565,\n",
      "             0, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  994,     7,     0,     9,  8939,     5,     6,   483,   805,   672,\n",
      "          1421, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3061,     4,  1690,    58,   375,  4100,     0,    13,  2612,     5,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   56,   797,    12,   832,     2,  1648,     0,     0,     0,   372,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  3840,  1095,  4399,    80,  9356,    13,    63,     6,  3162,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,  4430,   185,     0,  1055,   738,     8,  5270,  6051, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  187,     7,  1446,  8822,    20,  5757,  4638,  8823,   693, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,   767,     6,  1179,     2,  2391,  3489,   806, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  4384,   607,   394,   251,     8,    51,    67,   548, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  631,     4,  1118,  4376,    12,    17,  1230,  2127, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1161,  2499,    11,  1161,  4944,    24,    34,  4028, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,   517,    23,   776,   457,    12,     0,     0, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 5692,    46,   178,  2456,  7530,  5693,     6,   234, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  163,   998,   736,   605,    18,    75,    87,     0, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   34,  2171,  4851,     0,     1,   460,   913, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3442,     4,     0,  1139,    17,   394, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/334 [00:02<04:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  108,    62,   850,   505,    16,  1312,   130,    75,   973,  1291,\n",
      "            69,   144,    14,  6285,  9081,     3],\n",
      "        [ 2295,   863,   109,   161,   315,   104,     1,   607,   561,     8,\n",
      "          1851,   341,     1,   258,   111, 19690],\n",
      "        [    0,     5,   697,   698,   634,   106,     6,     0,  3460,   284,\n",
      "            18,  4874,   914,  1228,  2163, 19690],\n",
      "        [ 1419,  2396,  2438,     9,  3633,  1412,  4420,  1521,  1522,     6,\n",
      "          1585,  4385,     0,     3, 19690, 19690],\n",
      "        [   10,     0,  5748,     1,   249,     2,   449,  3740,  3976,   195,\n",
      "          2523,  1091,     6,     3, 19690, 19690],\n",
      "        [ 1821,   937,  3327,  1946,     1,  5464,   248,   265,   165,  2400,\n",
      "            14,  1272,  3924,     3, 19690, 19690],\n",
      "        [ 2478,  3364,  2948,   497,    22,    17,   509,   462,    29,    19,\n",
      "             0,  1647,  2319, 19690, 19690, 19690],\n",
      "        [   19,  8634,    31,    24,     0,  1742,    32,  1578,  1097,  1528,\n",
      "          1335,  2972,     3, 19690, 19690, 19690],\n",
      "        [  938,  2504,    22,    17,  7867,     4,     0,     8,  2284,    64,\n",
      "           803,    40,   194, 19690, 19690, 19690],\n",
      "        [  140,   230,   295,   364,   562,    43,     9,  1275,     8,    56,\n",
      "          6303,  1169,     0, 19690, 19690, 19690],\n",
      "        [   60,    61,    42,    14,  3634,  3128,     0,   167,   838,     0,\n",
      "             4,  1753,     3, 19690, 19690, 19690],\n",
      "        [   15,  1708,  2500,  2001,  1751,    71,   340,  3214,  3064,    18,\n",
      "           752,     3, 19690, 19690, 19690, 19690],\n",
      "        [   85,    88,  3256,  2650,     0,    42,  2889,     0,    20,   379,\n",
      "          3637,   167, 19690, 19690, 19690, 19690],\n",
      "        [   56,   695,     8,  4030,   385,    89,  1101,     0,    12,   430,\n",
      "          6182, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3628,  2805,  7444,  2624,    67,  7445,  1662,    20,     0,  7446,\n",
      "           338, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  380,    13,    47,     4,  1964,  3937,  1688,   104,     6,   310,\n",
      "           548, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1326,  2561,  1812,   986,     0,   350,     4,  1506,   118,  3943,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  6311,   860,     0,  1578,     4,   332,    12,     0,  3126,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1046,   220,    23,  2247,   110,     8,  1512,   689,   372,  8808,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  527,     4,  5027,    46,   166,  4047,   520,   740,   342,  5028,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6306,     0,  7321,   956,  2278,     0,    79,  3123,  3424,   117,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,   259,     2,   636,     0,  6083,    53,  1624,  1401,  3015,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1374,  1507,    11,  1374,  1507,   166,    90,  1319,   876, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 8463,    83,   316,     0,  2827,   106,     8,   394,   251, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3890,  1590,    54,  1743,  3313,     9,    15,  9406,  2584, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  328,   730,  4269,  7638,    40,   483,  3902,  1256,  2849, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1253,   368,    11,   447,  7635,     0,  1253,   368,  4604, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  159,   152,    11,   159,   152,  2496,  3522,  2871,  2064, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  314,  4088,  6010,    54,  4870,   945,   641,  1321, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1499,     0,  5495,  1043,  1846,  3676,  3164,   337, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  758,  1250,     0,     0,   243,  2978,   729,   365, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9342,     0,    11,  9342,  8685,    53,  3334, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/334 [00:03<04:05,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  107,   304,    22,  1541,     1,  1741,  4040,  1176,   329,  1423,\n",
      "             1,  4817,    93,  1190,  3227,    13,    17,  3998,    64],\n",
      "        [ 1607,    55,  8757,    81,   921,    32,   998,  2237,  8758,    93,\n",
      "            23,  8759,    13,    17,   827,     7,  8760,     3, 19690],\n",
      "        [  986,   318,  1538,    93,     6,  2277,  1364,     1,   281,    18,\n",
      "            68,  1516,    93,     6,   299,     2,  1172,     3, 19690],\n",
      "        [   14,  1787,   619,    99,   253,  4683,   740,  7242,   596,   166,\n",
      "          1012,     6,  3680,   323,   209,     3, 19690, 19690, 19690],\n",
      "        [    0,  1814,    45,    17,  1068,     0,     0,     6,    17,   544,\n",
      "             5,     7,  1122,  1853,     3, 19690, 19690, 19690, 19690],\n",
      "        [    0,  3085,  3875,    40,  1579,   323,   171,   492,     2,     0,\n",
      "           322,   427,    13,    17,     3, 19690, 19690, 19690, 19690],\n",
      "        [  211,  5442,     7,  5398,    82,    69,    17,  4598,    34,     0,\n",
      "             0,    18,     0,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  173,    28,    55,    14,  4118,  2287,   644,  2288,   389,  1034,\n",
      "            18,  6728,   612,    32, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9122,  8984,     7,  2050,    18,     0,   236,    14,  3614,     1,\n",
      "          1719,    13,   148,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   81,  2569,  5123,  1284,   608,     7,  1098,  1918,  1000,   621,\n",
      "           981,  1587,  4099,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   14,   272,  5405,  1897,  2123,     5,   277,    57,  3727,   160,\n",
      "          1165,  1356,     3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   14,   401,   368,   515,    20,  6915,     0,    31,   409,     0,\n",
      "            37,   309,   292, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,   767,  8480,    42,     0,     4,    19,  1314,    80,\n",
      "           801,   864,  1844, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  994,  4862,   420,  2211,     9,  1646,     0,  9173,  4556,    25,\n",
      "            48,    27, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  330,   331,  4073,     7,  3703,  2721,    71,     0,   120,     6,\n",
      "          4438,  7291, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  669,    16,     0,  5105,  1837,     1,     5,  2275,  1662,    45,\n",
      "            74,  2276, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  512,   190,     0,    66,   360,     0,  2592,  1383,     9,  3681,\n",
      "           722,  1386, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   74,    24,  2751,   455,    30,   326,   145,   153,  1050,    37,\n",
      "           737,     0, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2675,  3569,   270,   566,   980,    97,    28,  3790,    30,     0,\n",
      "          3208, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3420,   281,    11,  3420,   281,  2496,  1434,  4020,    49,  2031,\n",
      "          2049, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  324,   454,    11,   324,   454,    18,  2124,  1739,  2868,  3336,\n",
      "          1018, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2838,  2839,  6387,  1558,   584,  2790,  8877,   151,    36,   487,\n",
      "           966, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  137,   426,   123,  3624,   128,   203,    13,   122,     4,   548,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2420,  2421,     7,   286,    28,   177,   410,     4,  7932,    32,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 5816,    30,  6793,   195,   246,    31,    14,     0,   277, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  437,  2294,  4413,     0,    13,  2332,     6,  9674,  3769, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,  4561,   135,  4562,    40,   497,  3797,  1575, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  530,   631,  1649,    16,     0,    26,     0,   145,   153, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  278,   920,    11,   278,   920,    45,  3323,   500, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,  3496,    31,  1698,   291,     4,  2606,  2046, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,  2756,    12,  3751,  2398,  2399,  2151, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,  9180,    53,  3972,  8354,    33,     0, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 5/334 [00:03<03:32,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7811,  5163,   171,     0,  2515,   597,     1,  1701,   349,    33,\n",
      "             0,  9390,     5,     0,  1715,    59,     6,     3],\n",
      "        [    0,     1,     0,    25,   471,    27,   472,   763,  1600,     0,\n",
      "           158,    55,     6,    17,  8560,  6329,     3, 19690],\n",
      "        [ 1192,   313,     0,     5,    22,    39,    18,    86,     4,     0,\n",
      "             2,   646,   208,  3283,  1741,     3, 19690, 19690],\n",
      "        [   14,  3363,  3265,  8148,  2589,  2292,  2291,   491,    22,  1968,\n",
      "          3563,    33,  5194,    59,  1343,     3, 19690, 19690],\n",
      "        [ 3115,  1690,  2626,   130,    69,    34,   147,   707,  5991,     1,\n",
      "            58,   375,   559,    55,     3, 19690, 19690, 19690],\n",
      "        [   28,  2231,  2232,     4,    34,   283,   167,    24,  4007,   392,\n",
      "           283,    32,   553,   167,   603, 19690, 19690, 19690],\n",
      "        [  681,  8429,    11,   681,  8429,  1482,   130,    16,   355,   344,\n",
      "            44,  1261,   217,     4,  1691, 19690, 19690, 19690],\n",
      "        [   56,  7672,    26,  1142,     6,  1405,     2,  3848,  7802,   711,\n",
      "           734,    25,    48,    27, 19690, 19690, 19690, 19690],\n",
      "        [   48,    20,   250,    15,  5487,    87,   399,  7223,   564,   764,\n",
      "           101,  2668,    31,  7224, 19690, 19690, 19690, 19690],\n",
      "        [   96,  3921,     1,     5,  2493,  3427,  1629,     5,   480,  5582,\n",
      "          4019,    22,  5018,     0, 19690, 19690, 19690, 19690],\n",
      "        [ 9747,  4125,  2291,   767,   755,    26,    14,   371,    29,  3382,\n",
      "          6739,  4124,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1136,   274,  2369,   156,     8,   516,   613,     1,  6075,   769,\n",
      "             8,   102,  1220, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   75,  1597,  5350,   154,  1272,   110,    37,   107,  8962,     4,\n",
      "          1956,    81,     0, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  921,     0,     7,   144,    81,     4,     0,  3541,  8503,    54,\n",
      "           648,    16,  5131, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3686,  5513,  1048,  5514,  1908,    31,  3175,    37,    26,    17,\n",
      "           190,   455, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     2,   157,  3050,   188,   416,  9610,    12,    19,\n",
      "          2841,    80, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  102,  2702,   855,    57,    29,   855,   184,     1,  1322,  5354,\n",
      "          1120,  7014, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,  6573,  3375,     9,  5985,   227,    93,   537,  9197,  5040,\n",
      "           969, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,  2755,   322,   225,     2,   513,   198,   203,    13,   357,\n",
      "           111, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  486,  2495,    11,   486,   702,  5753,  7841,  7842,    33,  2452,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1171,   204,     2,   199,     0,  4012,   205,     0,     0,   797,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  260,   628,   665,     2,    77,   986,     0,  5920,     2,   546,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1252,  1477,    11,  1252,  1477,  2075,  2076,     2,  1264,  5871,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,  2249,   115,   440,   776,     2,  2143,     6,     0,     0,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1238,  2825,     2,  8320,   381,     6,   931,  1153,   778, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  137,   225,   262,    89,     9,   206,  2860,    22,   735, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,   335,  1251,     0,   322,  1509,  3821,  4804,   494, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3595,  3087,  2109,  3596,  2355,     8,  4277,  1141,   292, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   39,    35,    11,    39,    35,  4161,    43,   264, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,    98,  1750,   139,     0,   295,   862, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3406,  2239,     7,   108,    62,  1724, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1596,     1,   329,   315,     1,    97, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/334 [00:04<03:13,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3195,    93,   168,    33,  1819,     1,  4290,    93,    33,     0,\n",
      "             5,     7,  4678,     4,  3378,  8231,  2358,   106,     2,     3],\n",
      "        [  165,   551,   732,  1757,  1798,  7827,   491,    22,    17,  1968,\n",
      "            13,  5866,     3,  2602,  2401,   142,     3, 19690, 19690, 19690],\n",
      "        [  194,   216,    11,   194,   216,    30,   138,  6558,    37,  3939,\n",
      "            16,    76,   379,     0,   455,  8226,     3, 19690, 19690, 19690],\n",
      "        [  900,   291,     5,   277,   315,  3098,     7,  1066,  1976,     0,\n",
      "            37,    14,  4229,     4,  3273,     3, 19690, 19690, 19690, 19690],\n",
      "        [   61,    18,  3634,  3128,     5,  2403,     0,  1373,  2966,  3657,\n",
      "             0,    20,    14,  7410,    26,     3, 19690, 19690, 19690, 19690],\n",
      "        [  144,     2,     0,    23,  7976,   176,    26,  1127,     4,  7977,\n",
      "             2,   604,     2,    17,  2043, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6177,     0,     5,  2881,     0,   510,   241,  5662,   105,    24,\n",
      "             0,   265,  6580,   237,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  234,   634,     6,     2,  2890,  1957,     4,   325,  1732,     6,\n",
      "             2,  8376,    29,     0,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  418,   217,     4,  3241,   453,     6,   879,   203,    13,  2475,\n",
      "             0,    40,   355,   344, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  272,  2259,    11,   272,  2259,  1627,    55,  2042,  3310,  7856,\n",
      "             6,  3109,    13,     0, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1793,  2189,  5077,     9,  2010,     0,    33,  2207,    23,  2037,\n",
      "            13,     0,     2,     3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3996,     4,   338,  9255,   754,    28,  9421,    26,  1893,     1,\n",
      "          3077,    16,   913, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  173,    14,  1062,  6195,  1897,  1625,     5,   268,   212,   165,\n",
      "          8450,     4,  6196, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,   361,   662,    83,   387,   198,     6,  2847,  2089,    36,\n",
      "          1404,   183,  1453, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  395,   329,  1022,   289,  6610,     9,    34,   674,     0,    25,\n",
      "            48,    27, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     4,   388,   659,   444,   549,  1197,    13,  5594,\n",
      "           716,  7379, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  347,   352,   743,     4,  6257,    22,     5,  9153,     5,  1315,\n",
      "          2298,     0, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,   361,  2988,    42,   124,   219,   387,   895,  2579,    12,\n",
      "           139,   358, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  144,    17,    15,  4554,    98,  5840,     8,  1912,     5,   268,\n",
      "          2781, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1087,  1088,    11,  1087,  1088,     4,  1506,  6476,    31,  1132,\n",
      "          1596, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   52,     4,   177,  1240,  4375,     2,   295,     0,   276,   210,\n",
      "           623, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  140,     4,  5747,  4135,  7334,     9,    56,  6489,   592,     7,\n",
      "          3281, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 5095,   863,     0,   597,  5916,   236,   724,     1,   508,  8121,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1140,   422,   823,   737,    18,    63,  8622,  6271,  1313,  2251,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3726,    18,   617,  9427,     0,  6396,  2391,   731,   684,     5,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1567,   380,  1073,  2411,    44,   591,   689,     6,   346,   654,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 5352,  6325,  4595,  4020,     1,     0,     0,   187,  3054, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6637,  6638,    11,  6637,  6638,    63,     0,   208,     0, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  504,  2068,    98,   628,   819,     0,     5,   574,  2112, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  970,     2,  2343,  4234,  4235,  4236,     6,   483,  2566, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  832,  1347,  1493,   612,     7,    98,    13,   141, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  250,  7198,    12,   350,     4,   110,     0, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/334 [00:04<03:32,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    5,    94,   928,   532,  1735,    12,   318,  2859,     5,     7,\n",
      "          1416,  2085,  4163,    12,  1313,     8,  5023,  2536,     3],\n",
      "        [   14,   340,  5332,    16,    15,  2427,     5,  8122,  1454,  4783,\n",
      "           144,    75,   967,  2886, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9762,    13,   965,    50,  1074,  1485,    18,  1905,  1906,     0,\n",
      "          8222,     0,     2,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  149,   150,   138,  1515,   150,  1108,    30,   194,   216,    16,\n",
      "           163,  3400,   455,    32, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2683,   146,   550,    67,  2824,   367,     1,   326,     0,  2563,\n",
      "          1362,    21,  1317, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  170,   386,     4,   876,     0,   142,   688,     4,  5276,    45,\n",
      "           389,  6662,     5, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6992,  6993,     5,   463,     0,     9,  4210,  1422,  1895,  1737,\n",
      "            13,  8289,  8494, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  5721,  7887,     5,  2384,  1970,     2,  1210,   374,  7888,\n",
      "           106,    13,  4712, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   97,   214,  6034,   192,     7,     0,    30,  7605,  3003,     0,\n",
      "          8642,     0,     3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  687,   725,  2961,  1314,    37,   958,   455,    44,   418,   217,\n",
      "             4,   268, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  970,    18,  4639,  6236,    19,  5792,    80,  1752,    12,  4689,\n",
      "          5793,  4596, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  383,  4668,    28,  7765,  4669,     6,   320,     4,     0,    25,\n",
      "           129,    27, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  594,   624,    11,   594,   624,    18,  1798,  6213,   209,     0,\n",
      "          2789,     5, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,   541,   318,  3472,  3738,     1,    89,   230,  7382,\n",
      "            95,  3739, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9378,     7,    14,  7347,   192,  9208,    17,     0,    13,     0,\n",
      "            18,  9218, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  1067,   329,  5997,  2905,  3047,  1346,  3108,   104,     8,\n",
      "           304, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  102,  1649,    16,   739,  1963,   195,   117,    26,   743,     7,\n",
      "          1064, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,   966,    33,   227,    65,     1,    89,  3458,    13,    56,\n",
      "           985, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  173,   202,  1605,   400,   956,    82,    16,    19,  3233,    16,\n",
      "          1683, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  2051,    52,  3182,  4261,     0,    53,  1871,     1,   184,\n",
      "          2485, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  2063,   621,   573,     5,     0,    12,   109,   175,     0,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   85,    88,    11,  1434,  3153,     2,    85,    88,     4,   405,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  435,   917,   548,  1635,     2,   802,   633,  9108,     5,  2472,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1046,   592,   379,   110,   275,  8244,   297,   107,   316, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9707,     2,   806,    51,     0,     8,  8418,  5361,   332, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  103,     0,   381,   601,    15,   183,    36,   965,   197, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2510,  1995,    11,  2510,  1995,  9097,   118,  1868,  3350, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  7647,  2412,  2216,   322,     0,     1,  2241,   414, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 5075,    11,  5075,  9571,   415,   194,   216,  4937,  2064, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1827,  1828,    11,  1827,  1828,    12,  1443,    32, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3402,  2431,  9402,     2,     0,  9403,   981, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  399,  2909,    26,  9570,   252,    50, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/334 [00:05<03:16,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,   638,  9552,    50,     5,     7,  1412,  1155,    45,     2,\n",
      "          1172,    60,    61,     8,    59,  3234,   816,     6,    17,    15,\n",
      "             2,     3],\n",
      "        [ 2660,   465,   433,    18,  1677,     0,    13,     0,    18,     0,\n",
      "            23,   336,   341,  6209,   618,    40,    23,  1265, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  145,   153,   441,    34,  5743,  2822,     1,   240,   108,   476,\n",
      "            62,  3046,    32,   553,  2715,   603, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  345,    21,   148,   277,   184,     0,  1094,  2212,    11,   397,\n",
      "             4,   254,  1730,   173,   248,     3, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 1600,  4004,     1,     0,  2401,  1797,     4,     0,    57,   639,\n",
      "          3163,     1,   491,  1614,  3449, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 3928,  4781,    18,  1160,  3363,  1996,    19,     0,   879,     0,\n",
      "             5,  4782,    33,  1101,     3, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   85,    88,  1464,  1160,  3191,  3397,   179,     8,    17,  3382,\n",
      "             0,     2,     0,     3, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  627,  1227,    11,   627,  1227,    42,  4558,  9789,  1714,   847,\n",
      "          1131,    20,    34,   167, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 3245,  2813,     1,  1481,  1799,  2175,  1062,  3392,    21,   159,\n",
      "           152,     4,     0,     3, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  994,  2006,   215,   220,     8,   258,  1316,     1,  8879,   563,\n",
      "          2857,    25,    48,    27, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  293,   302,  1355,   384,    28,     0,    31,  9366,  1993,  1945,\n",
      "             5,  3383,   162, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   47,  1057,  1338,  2552,     9,     0,     1,  8299,  2975,   215,\n",
      "            25,    57,    27, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   75,  1332,  1739,    13,   249,     1,    95,   144,   163,    75,\n",
      "           933,    62,    32, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   10,  3151,  8849,    46,    15,  8850,  2443,  4382,   166,    90,\n",
      "            23,  2377, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 1243,    23,   754,     1,  5928,    62,  6156,     7,   551,  2777,\n",
      "          6486,   754, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 1571,  1451,  1452,  1572,     7,   272,    18,  1658,  4543,     0,\n",
      "            17,  5844, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  176,  1345,    11,   176,  1345,   371,  9444,  3400,     5,    20,\n",
      "          1483,   167, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   10,     0,  1937,  3264,  8388,     2,   157,  8389,    12,    19,\n",
      "             0,    80, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  705,   313,  1093,  8622,     6,     0,     0,   932,    22,   270,\n",
      "           566,    97, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  355,   344,   846,     0,   831,    16,   418,   217,  1315,   121,\n",
      "          6026,   268, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   52,     4,   177,    46,   253,   168,   752,  1587,   408,   839,\n",
      "           120, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 1412,  1155,    11,    60,    61,  1239,  1412,  1155,    16,   565,\n",
      "           455, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   10,     0,   814,  1444,  1619,    12,   466,   881,  1557,  3687,\n",
      "          5973, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   39,    35,    18,    86,    73,  2460,   726,   264,     4,   139,\n",
      "           898, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 6606,     5,   363,     7,    24,     0,     0,    54,    14,  1805,\n",
      "           264, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 8501,     0,    18,     0,  2900,    51,  2947,   181,  1163,  9484,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  867,   975,    11,   867,   975,  8535,    40,  1070,     4,  4685,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   52,     4,  1815,   301,     0,     5,  3328,   210,   623, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  122,     7,   196,   780,     2,   292,   866,    79,  8189, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 4647,     5,   277,   640,    16,   512,    37,   682, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  890,  1589,  4516,   247,   426,   672,  8594,  1178, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  133,  1322,     0,  5817,    12,     0,     0, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 9/334 [00:05<03:01,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1467,  3500,   136,  4661,  4662,     7,     5,    94,  6667,   551,\n",
      "          3371,   343,    21,   108,  1546,  2830,    37,     5],\n",
      "        [ 4284,  1075,     1,  1688,     1,   158,     0,    22,  1074,  1485,\n",
      "          3783,     6,  3109,    13,  4735,    13,   761,     3],\n",
      "        [   38,  1200,    41,  6808,    38,  1826,     1,    41,    38,  2618,\n",
      "            41,     2,   972,  2319,   509,   462, 19690, 19690],\n",
      "        [ 8364,   162,    28,  3955,    24,  1474,   326,     1,    30,   399,\n",
      "            28,    62,     4,    24,  3895,   223, 19690, 19690],\n",
      "        [  800,  1075,  2124,  1739,  9630,     8,   127,    13,  1735,     6,\n",
      "          3253,  3954,    17,  4328,    33,     3, 19690, 19690],\n",
      "        [  249,   673,   246,   300,   282,   393,  2220,   292,     1,  8261,\n",
      "            58,    25,    48,    27, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     4,  4425,  7496,    43,   135,     2,   157,  4426,\n",
      "            64,    24,    64,    98, 19690, 19690, 19690, 19690],\n",
      "        [ 5774,    13,  4283,  6604,     0,  2911,     8,  9608,     0,     0,\n",
      "            53,    17,     0, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6465,   765,  2856,   949,     6,   534,  8995,    33,   438,     0,\n",
      "          9668,  2542,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  452,   238,   181,     6,   570,  1094,     8,   239,    12,    52,\n",
      "           910,     0, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  571,     4,  7679,    22,  5771,  5772,    23,  2704,     0,     5,\n",
      "            22,   633, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  546,   628,  1718,   289,   488,     9,   459,   404,  4887,    25,\n",
      "            57,    27, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  140,  3995,     2,   172,     2,    19,     0,     2,     0,    56,\n",
      "             7,     0, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  145,   153,    11,   145,   153,     4,  2215,   196,  1938,   121,\n",
      "          5988, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  685,   459,  1139,    57,   218,    12,    74,   115,     6,   346,\n",
      "           256, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1915,   197,    12,  1839,   336,     1,  5569,   123,  3414,   969,\n",
      "           128, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  228,     0,  5334,  1034,     6,  1733,     2,  1148,  2115,  6984,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1783,  6485,    72,  1214,   521,  2524,  1825,    20,  3175,  1321,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   96,   536,    15,     5,     7,   346,  8194,  3862,   298,  1071,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2623,     2,   334,  5207,   443,  3209,     1,  1040,  2610,   365,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     2,  2130,   402,  5485,     6,   635,    18,   685,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  328,   846,   379,  3154,   215,   220,     8,  1810,   590, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9712,   708,     2,   983,     0,  2770,     1,     0,   913, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  925,  4502,   235,   161,     8,   151,   583,    53,    47, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1686,  1027,    11,  1686,  1027,     0,    51,  6760,  2041, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   14,   284,  1464,    22,    17,   264,   146,   509,   462, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  170,   386,     4,  1433,   722,  7211,    21,  7212, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  968,  6376,  9157,  2875,     6,   534,  9158,   778, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,   376,  3047,    29,   122,     8,  3279, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2518,  5628,  1532,  1384,  2926,     0,  1022, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  777,    35,     4,  6583,     0,     5, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 8065,   109,  6124,    40,     0,   111, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/334 [00:06<03:22,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1623,    54,  3318,     5,   160,    24,     0,     1,    30,   308,\n",
      "          1007,   121,    16,    76,    37,  3883,     3],\n",
      "        [ 1122,  1853,  1682,    64,  1123,  1683,    14,   674,    44,  2591,\n",
      "            26,    38,  1382,    41,  5149,     3, 19690],\n",
      "        [ 6275,    96,  9394,     5,  9584,  2782,     0,     1,  1818,  3398,\n",
      "            66,    76,   312,    31,    24,     3, 19690],\n",
      "        [  355,   344,  9093,    40,   688,  8153,    66,    18,  5000,  6114,\n",
      "          1368,    12,  1702,     4,     3, 19690, 19690],\n",
      "        [  938,  2504,   369,    34,   162,     1,  8701,  1374,  1507,     4,\n",
      "             0,     0,     1,    31,     3, 19690, 19690],\n",
      "        [   44,  2227,  1301,     1,  1010,  1312,  4476,   105,  5678,  8950,\n",
      "            30,     0,  6417,    82,     3, 19690, 19690],\n",
      "        [   86,    73,   369,    69,   547,  6443,    37,   118,    39,    35,\n",
      "             1,    30,     0,   990,     3, 19690, 19690],\n",
      "        [ 1486,   494,    12,  2603,     0,    99,     0,     0,     7,  1488,\n",
      "          1164,   127,     2,  2533,     3, 19690, 19690],\n",
      "        [ 1007,   417,  1705,     4,  4609,  1677,     0,    32,   107,     0,\n",
      "             0,    12,  5121,     3, 19690, 19690, 19690],\n",
      "        [  875,   604,     2,     0,     8,   223,     4,  1304,  1646,     1,\n",
      "          7588,  1383,  5542, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     2,   157,  3190,    12,    19,     0,    80,     1,\n",
      "             0,     6,  2633, 19690, 19690, 19690, 19690],\n",
      "        [  351,  7872,    15,   776,   393,     6,  1071,    26,   129,     0,\n",
      "            25,    57,    27, 19690, 19690, 19690, 19690],\n",
      "        [  834,  1255,   444,    30,   314,     0,  6067,  2883,   273,     0,\n",
      "            16,   270,   566, 19690, 19690, 19690, 19690],\n",
      "        [ 1797,     4,   273,   366,  2191,     0,    22,   191,    42,   124,\n",
      "            44,   239,  4947, 19690, 19690, 19690, 19690],\n",
      "        [ 1786,    58,  1057,  5610,   317,   226,   289,  1766,  4611,    25,\n",
      "           129,    27, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2326,     5,  1861,   280,   509,   462,  8952,    29,    19,   391,\n",
      "           385,  1647, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1466,  2202,  3273,    16,  2556,  2557,    26,     0,     0,    30,\n",
      "           346,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  456,    35,  6290,    29,  8672,  6291,   395,  1166,    18,   116,\n",
      "          1365,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1326,  3725,  7477,    33,   116,     0,     5,  5970,    43,  1260,\n",
      "          4211,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  401,   368,  2028,   309,   297,  3465,    16,     0,   141,     4,\n",
      "             3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  366,   807,    67,    46,    38,  2053,     0,    41,     2,   875,\n",
      "           430, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3883,  2879,    24,   115,     5,  1193,   114,  1805,     0,     2,\n",
      "             0, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9645,     6,   704,   855,  1344,     0,  4905,     6,  1944,     7,\n",
      "           663, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 4981,     2,   211,    22,  2595,    19,     0,   132,     6,  1084,\n",
      "          1544, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,   134,   251,   127,     2,   785,    22,   429,     8,   113,\n",
      "           557, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1756,  2390,     0,  1395,   354,  8475,   952,     0,     6,     3,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1195,    21,  6449,  1855,  2697,   118,  6449,   441,     0,   758,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9237,  9238,    30,     0,   668,   450,     0,  2074,  1111, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  351,   941,  1308,   118,    19,    48,    31,    34,  2261, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9555,     0,  1338,  2099,     1,  1977,    98,  2309,  1534, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  582,    42,   489,  6704,   894,  3462,     7,  6705, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1381,   112,   592,  3709,   110,     8,    52,   239, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 11/334 [00:07<03:07,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    5,    94,  1332,  1081,    17,   927,   358,     5,     7,  1694,\n",
      "          2091,  7128,     0,    13,   537,    23,  5426,     5,   276,     3],\n",
      "        [ 4754,    21,    14,  1363,    21,    14,  2897,     5,     7,  2356,\n",
      "          1525,  9487,    62,     4,   873,  9421,     3, 19690, 19690, 19690],\n",
      "        [  159,   152,     4,  2159,    31,    14,  4525,   286,  1483,   121,\n",
      "          2607,   754,     5,    25,   713,    27, 19690, 19690, 19690, 19690],\n",
      "        [ 2996,  2997,   171,  1598,   294,     4,     0,    45,  5013,   772,\n",
      "          3796,     5,     3,     9,   116,     3, 19690, 19690, 19690, 19690],\n",
      "        [ 1304,  2948,    13,     0,     0,     6,    17,   720,   496,     0,\n",
      "            29,  2181,  2255,     3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  668,  5569,   123,  7353,   128,   465,   895,    19,  5570,    36,\n",
      "           929,   175,   601,   649, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  191,    42,   124,   219,  1491,   327,     2,  4693,  1904,   317,\n",
      "             9,    52,  2160,  1086, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  485,  2290,   512,  2448,     0,     1,  2079,   951,  2681,    21,\n",
      "          2858,    31,   180,     3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9246,   973,  5519,    14,  2155,     5,    21,     0,    30,     0,\n",
      "             1,  1010,     3, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  635,     7,  1620,   971,  1198,  1301,  3784,     0,  1369,    16,\n",
      "            34,     0,     3, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,     0,     2,   414,   106,   544,   193,     0,     2,     0,\n",
      "             0,     6,     3, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  163,  2844,  1505,  4256,  1683,    32,    62,  7693,     8,     0,\n",
      "            75,  2962, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  979,   355,   344,    11,   355,   344,     0,    16,   450,   214,\n",
      "           244,  1321, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  8544,  1052,     0,   839,    90,  6491,    12,   228,    11,\n",
      "          1591,  4567, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  370,   784,    26,   452,     8,    77,  4202,     9,   140,   238,\n",
      "             8,   206, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,   282,     4,     0,   430,  5083,     2,    47,  2952,\n",
      "          1639,   557, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,  9169,  3724,     0,   496,  6505,     9,  4547,     1,  6204,\n",
      "             0, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  119,   810,   941,  4399,    91,  2580,     6,  1302,    13,  3162,\n",
      "           380, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1136,   403,     2,  8089,   128,     8,   467,   613,     1,    47,\n",
      "            65, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     5,  9077,  2907,    29,  4631,    19,  3067,   232,\n",
      "           548, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     4,  2678,   849,  7396,  3746,  1303,    12,  3565,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  397,    87,  4856,  2157,  1371,   798,  1493,   243,   211,  6703,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  278,  1194,  1286,    30,   819,  2642,  3499,     0,   130, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 8727,    14,  5726,    30,     0,    21,    14,    97,  3463, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 8318,  8345,     9,   215,  1154,   255,     8,  2200,  1795, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   60,    61,    11,    60,    61,   822,   714,  2843, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,     7,   995,    13,   651,  1766,   162,   363, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,  1417,  1740,  3750,    15,   289,  1713, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  586,  1164,     0,   434,   168,     6,  2902, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6475,  5049,  3039,  9372,     1,  1150,  9373, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3353,     0,  5989,  4762,    78,  3168,  8060, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 8726,  5072,   821,   450,   700,  1407, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 12/334 [00:08<03:42,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  682,    50,  1460,   346,    34,     0,    54,     0,    18,     0,\n",
      "             1,     5,     0,     5,  2785],\n",
      "        [   14,  2947,   643,   990,     0,  3880,    78,    14,  3929,   371,\n",
      "            20,    96,  2947,   181,     3],\n",
      "        [  328,  2307,    46,  1265,   891,  5170,     1,  3021,  4152,     2,\n",
      "           120,   250,   413,   644,     3],\n",
      "        [    0,     0,    50,     0,     0,  3817,     6,  3310,     0,  3912,\n",
      "           932,    22,    17,  8040,     3],\n",
      "        [   14,     0,    13,    15,  6313,  3397,     0,    93,    22,    17,\n",
      "          1670,   438,  1332,  1548,     3],\n",
      "        [ 1663,  3781,     2,  5047,    19,  8634,  1308,    13,  2972,  1508,\n",
      "           538,    25,    57,    27, 19690],\n",
      "        [ 6632,   185,  2840,   618,    13,  2077,   596,     7,  2912,  4788,\n",
      "          3556,    93,   114,     3, 19690],\n",
      "        [  350,  1715,    29,     0,  5467,     6,     0,    18,   318,     0,\n",
      "            33,     0,  2522,     3, 19690],\n",
      "        [  979,  1495,  5857,    11,  1495,  5857,  1048,  6391,   392,  8772,\n",
      "            21,  4277,     4,     3, 19690],\n",
      "        [    0,     0,     0,    24,   916,    26,   214,   244,  7911,     1,\n",
      "            72,   384,     3, 19690, 19690],\n",
      "        [  351,  1460,  2652,  3056,     1,  1412,  3550,  5733,     1,  7386,\n",
      "            30,  1367,     5, 19690, 19690],\n",
      "        [  170,   386,    11,   170,   386,    16,    76,     0,    49,  5165,\n",
      "           162,   192, 19690, 19690, 19690],\n",
      "        [ 2838,  2839,     0,  2790,  3903,     9,  1529,  3037,    36,  6235,\n",
      "          4324, 19690, 19690, 19690, 19690],\n",
      "        [  412,  3768,     4,   536,    37,     0,    28,   904,    25,   713,\n",
      "            27, 19690, 19690, 19690, 19690],\n",
      "        [ 5113,  2977,  1985,    16,  7357,  5114,     5,  1295,     0,  1881,\n",
      "             5, 19690, 19690, 19690, 19690],\n",
      "        [  666,  2157,   263,   287,     4,  2187,    72,    76,  6650,    16,\n",
      "          7467, 19690, 19690, 19690, 19690],\n",
      "        [    0,  1455,  3641,     0,    29,     0,  3382,  1604,     6,   922,\n",
      "             0, 19690, 19690, 19690, 19690],\n",
      "        [  122,    58,    52,     2,   199,   319,  4209,  5248,    12,   400,\n",
      "           115, 19690, 19690, 19690, 19690],\n",
      "        [   77,    46,  3428,   402,     6,  1777,     2,  1781,  3995,  4003,\n",
      "          6403, 19690, 19690, 19690, 19690],\n",
      "        [   14,  7890,    18,  8294,  4856,   388,  2672,  3623,   491,  1614,\n",
      "          4857, 19690, 19690, 19690, 19690],\n",
      "        [  571,     4,  2230,   442,    23,  4848,   449,    12,   602,     0,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,     0,  3723,    12,    15,   598,   312,    12,  2933,  2036,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2691,  7958,  1836,  4101,    15,     9,     0,  4077,     0,   469,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [  200,   129,  8225,   199,  2963,    49,  2921,    37,  3806,   926,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [  606,    11,   345,    21,   148,   463,  1269,     0,  3465, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,     0,     7,  5134,     0,   551,     0,   297,  4080, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [   28,   107,    14,  1806,   141,  2449,  7823,  1934,    32, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,     2,   663,  5296,  1256,  4417,  3065,  7531,     0, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6346,  9291,  2013,  4230,  3554,  6554,    20,   483,   805, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [   81,  6143,   400,  5964,  2355,     8,  1709,  1141,  1841, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [  164,   259,  2147,   398,  1716,     2,  5042,  1574,   310, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690],\n",
      "        [ 5180,   501,   763,  2672,  3623,   356,  2551,  4857, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 13/334 [00:08<03:31,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  852,   848,     0,   178,   934,  5250,     0,     8,   139,  4835,\n",
      "            13,    94,  5955,  5956,  2742,   848],\n",
      "        [  979,   355,   344,    11,  1261,   217,     4,  1506,    16,    76,\n",
      "          4169,    20,  1082,  1083,    11,   187],\n",
      "        [  384,  2027,    90,    71,    17,  3145,    50,  1388,  1455,   495,\n",
      "            19,     0,    40,     0,     3, 19690],\n",
      "        [   20,     0,    13,    17,  6283,  2750,    13,   180,   294,     4,\n",
      "           268,     1,   397,     3, 19690, 19690],\n",
      "        [ 2414,  2430,   913,   184,    84,  2301,    30,  4018,  4851,    54,\n",
      "           466,     1,  9084,     3, 19690, 19690],\n",
      "        [   34,  1686,  1027,  1691,  4700,    30,   268,  1969,  3748,  1237,\n",
      "            78,  1368,    21,     3, 19690, 19690],\n",
      "        [  103,  4834,    83,   373,     0,  4139,    36,   247,    18,    89,\n",
      "             0,   644,   613, 19690, 19690, 19690],\n",
      "        [   62,     4,   101,   200,    75,     1,  2403,  4395,    50,   853,\n",
      "          2765,   268,     0, 19690, 19690, 19690],\n",
      "        [ 6099,     4,  6460,    58,     0,  9029,  2220,     0,     7,  1511,\n",
      "            13,    17,   110, 19690, 19690, 19690],\n",
      "        [  892,   131,    12,   125,     1,   231,   273,  4714,     9,  1216,\n",
      "          8179,  2404,  4812, 19690, 19690, 19690],\n",
      "        [  729,  6382,    99,   175,  3415,    17,  8866,     7,  8867,  1415,\n",
      "            99,   166,     3, 19690, 19690, 19690],\n",
      "        [ 1261,   217,    11,  1261,   217,     4,  3300,   708,  3964,  8491,\n",
      "           149,    16,  4911, 19690, 19690, 19690],\n",
      "        [ 1010,  2343,   407,  8375,   105,  1239,     0,     2,  5974,     0,\n",
      "            25,    48,    27, 19690, 19690, 19690],\n",
      "        [ 1079,  3938,  1142,     0,  9621,     9,  9563,     0,     8,  3714,\n",
      "            25,    48,    27, 19690, 19690, 19690],\n",
      "        [   10,     0,   466,    65,  1399,   186,   924,  1178,     6,  2948,\n",
      "            13,  4940, 19690, 19690, 19690, 19690],\n",
      "        [ 8055,   525,     4,     0,     5,    34,  3057,    28,  3154,   130,\n",
      "            21,     0, 19690, 19690, 19690, 19690],\n",
      "        [ 6687,     0,     2,    17,  4974,    50,   627,  1227,  5249,     0,\n",
      "             0,     3, 19690, 19690, 19690, 19690],\n",
      "        [   10,  7825,    46,   466,   881,    13,  4363,   276,     0,     1,\n",
      "          2473,     0, 19690, 19690, 19690, 19690],\n",
      "        [   15,   376,   785,  5186,     1,    95,   891,   127,     2,   513,\n",
      "          1619, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  409,  7464,  3776,  4525,    28,     0,   797,   165,  1471,  2444,\n",
      "          4526, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  594,   624,    11,   594,   624,    24,  5078,    21,  9640,  2210,\n",
      "          9641, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1576,    13,    17,   730,   175,    90,  1174,     4,   927,   799,\n",
      "          1364, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6250,     0,     0,     0,     0,     6,   574,   677,    25,    57,\n",
      "            27, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  173,    62,     4,  2197,     2,  9262,    14,   940,     4,   531,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  610,     0,   897,   540,   588,  2442,    63,  7744,     6,     3,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  103,  1274,  1339,     8,  1603,    15,    65,    36,    89,   197,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2032,     0,     5,   907,  7229,    26,  1022,  5494, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  173,   249,  1903,  1007,     2,  5709,     6,  1556, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   85,    88,  1214,  7397,    49,  4082, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  102,     0,     0,     6,  3876,  1676, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,     0,  1451,     0,     0, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9280,  4122,   422, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 14/334 [00:09<03:11,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3698,     4,  5527,     2,   209,    23,  1564,    12,   349,     5,\n",
      "             7,     0,  1692,    23,   726,  1944,    45,     0,     3],\n",
      "        [ 6202,    24,  6203,  6204,     7,   527,  4901,  8457,     1,  1149,\n",
      "          1603,  1437,   672,   258,     3, 19690, 19690, 19690, 19690],\n",
      "        [ 1438,  3601,   442,    19,  1372,   385,     6,   109,    33,  1856,\n",
      "           879,  4782,    12,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,    26,    14,   371,     5,   622,  1158,   190,   165,    14,\n",
      "             0,    30,     0, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  938,  2195,  1038,   107,   762,    24,  1966,    37,  2284,     4,\n",
      "           900,   291,     5, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1208,    35,  3344,  8623,     2,  1761,   395,  1166,     9,    70,\n",
      "           411,    51,     3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1941,  5574,     4,    34,     0,  1607,    55,  6231,   131,    13,\n",
      "             0,     0,  4914, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     0,    29,    19,     0,   232,  1112,    71,   965,\n",
      "           827,  6559, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  7837,     4,  2802,   178,  5755,   596,     1,     2,  2559,\n",
      "          1415,  7838, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  181,  1191,  4545,     8,  3792,   175,    90,  2265,     7,  2743,\n",
      "           912, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2109,  2101,  1543,  4259,   442,   966,   548,     6,  5308,   126,\n",
      "           310, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  326,    17,  2140,  3227,     7,   290,    67,  7851,    51,  2788,\n",
      "          3618, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1076,  1232,    11,  1076,  1232,   738,   106,    51,   848,  1358,\n",
      "          1735, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,   348,  1675,  4336,    64,  2353,   963,  1570,     2,    66,\n",
      "           842, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,    16,   836,  7201,   105,  4862,  2528,  1085,   407,    30,\n",
      "           268, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  206,    46,    56,  1967,     2,  4047,   520,   740,   155,   403,\n",
      "          1561, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1880,   252,   110,    11,  1243,  7489,   589,   718,    12,   510,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  733,  1475,    11,   733,  1475,     4,  3310,  7856,     6,  1552,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,   361,   662,    83,   533,   359,    29,    65,     8,  3279,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,   228,   881,  2630,    53,     0,  1733,     2,   984,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  435,   917,    11,   435,   917,  7409,     2,     0,     0,  4665,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3118,   120,    53,  3031,   128,     1,   516,   583,  5622,   225,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  103,  9113,  9114,    63,     6,  3739,   806,   276,    52,  1399,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  141,  7454,    45,     0,  7455,    22,  4518,   554,  2276,   192,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  355,   344,    11,   355,   344,  5531,     0,     5,  6543,   418,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  174,  5724,  2695,  1512,    79,     0,  7599,  2275,     3, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  122,   381,   601,    15,   183,    36,   965,   197, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1371,  5552,   872,     0,  7326,     6,     0,   612, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  107,  3119,     0,   243,  7868,    14,  6976,  1022, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  137,   667,  1498,     8,  1641,    77,  2546, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 7170,  1750,     0,     0,     9,     0,     0, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 4680,  1769,     0,  1487,  5688,     1,  3804, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 15/334 [00:09<03:26,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1124,     4,    23,  3372,     2,    17,  1575,     1,     5,    46,\n",
      "          2263,  4753,     9,  1435,  2466,     5,  2374,     2,  4322,   106,\n",
      "            13,     3],\n",
      "        [  107,   304,    22,  1541,     1,  1741,  4040,  1176,   329,  1423,\n",
      "             1,  4817,    93,  1190,  3227,    13,    17,  3998,    64, 19690,\n",
      "         19690, 19690],\n",
      "        [  507,   231,     1,  5007,  1815,    18,   819,  9075,    87,   165,\n",
      "            20,   569,     1,   240,    28,    62,    31,   136,     3, 19690,\n",
      "         19690, 19690],\n",
      "        [  231,   695,     2,  5366,  9606,     9,   100,   186,    92,  6629,\n",
      "          6293,  6625,     8,   284,    13,     3, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  180,   294,     4,  2791,  3404,  8490,     2,  3606,  1264,   388,\n",
      "             4,   279,  2152,  1981,     3, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 2514,  1013,   186,    92,    59,  8139,  6043,  1876,     6,  8140,\n",
      "          8029,    22,  1325,     3, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  691,   352,   184,  6903,   598,    22,     5,  2292,     5,    36,\n",
      "          2666,   239,    13,     0, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  200,  3349,    50,   960,  1854,   442,    23,  4707,  3028,     9,\n",
      "            70,  4133,  9612,     3, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 1326,  1746,  1747,  1262,    17,  2312,    13,  9425,  7528,    12,\n",
      "           485,   167,   283,     3, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  108,     2,  2886,    54,    81,     0,   107,   316,     7,  3564,\n",
      "          3258,     0,     2,     3, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  610,     0,     4,  2888,  2116,     2,   157,  1824,     6,    19,\n",
      "          8898,    80,   135, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 3593,   900,   291,     5,   622,   803,   246,    62,     4,    69,\n",
      "            16,   211,     0, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  107,   900,   291,     5,  3119,  8100,    72,  1502,    75,     0,\n",
      "            31,  1140,  2046, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 6384,   353,  2945,   365,  4014,  4183,  1668,    64,    14,  1320,\n",
      "          8778,  1058,     3, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   81,  6233,  1284,   608,   169,  9338,     0,    18,     0,   323,\n",
      "          8115,     9,     3, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 4548,   281,  5666,  7603,    30,  7604,     5,    31,   861,  1867,\n",
      "           553,   167,   603, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 1554,  1896,   441,    14,  6477,     0,    21,  2036,    31,  2463,\n",
      "             4,  1096, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  167,   186,  5016,  3282,     4,  2906,  5017,     0,  6428,     0,\n",
      "             3, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  180,   294,     0,   224,    29,   398,   100,   158,  2337,  1557,\n",
      "          9137, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   14,     0,    37,  5222,     1,    14,     0,   869,    75,   428,\n",
      "             3, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  258,   229,  2285,  1114,  1032,   186,   139,  2990,     6,   129,\n",
      "          1674, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 2480,    38,  3856,    41,    22,   383,     4,  7724,    13,   320,\n",
      "           337, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   10,   259,  2386,   561,  1135,   198,     0,     9,   849,  7114,\n",
      "           172, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [   10,  9062,  1319,    29,   647,     2,  4648,    12,   539,  1153,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 7405,   117,    23,  7406,  3752,    12,    47,     4,   665,  5611,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 1252,  1477,     4,  5162,  1173,   470,    20,   892,  2551,  3127,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [    0,     0,  1999,    36,  4272,  2107,     2,   123,  6074,   128,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 2131,   746,  1050,    37,  9032,    20,  1450,   525,     4, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 1447,  1078,    11,  1447,  1078,  1322,   193,  8462, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  931,     2,   127,   106,  8368,  4800,   262,  1238, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [ 2984,    12,  5703,  2171,  4567,  7555,  3805, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690],\n",
      "        [  420,  1138,  4313,  1170, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 16/334 [00:10<03:07,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1058,    57,  1362,    28,    17,    34,   184,  1362,     1,  1540,\n",
      "           967,    17,   586,   604,    54,  1503,     3],\n",
      "        [ 3866,  3867,    11,  3866,  3867,    16,   521,  1805,     4,  1987,\n",
      "          1623,    20,  2800,  2879,    24,   115, 19690],\n",
      "        [   14,  2464,  8146,  7133,     7,    14,  7471,  8147,  3276,   178,\n",
      "          6045,   114,   236,    19,     0,     3, 19690],\n",
      "        [ 9583,  1317,  1429,   660,   218,    13,   742,  1103,     4,  1541,\n",
      "           269,   661,    12,    19,     0,   132, 19690],\n",
      "        [ 1452,     0,  7411,     7,   420,  2027,  7412,  1408,   100,   541,\n",
      "            53,  5613,    29,    17,    15, 19690, 19690],\n",
      "        [   85,    88,   782,  2218,   696,    13,  1263,     0,    29,   145,\n",
      "           153,     2,   447,     3, 19690, 19690, 19690],\n",
      "        [   26,   319,     0,   818,     1,   382,   360,    28,    55,  5928,\n",
      "           245,     0,   578, 19690, 19690, 19690, 19690],\n",
      "        [   39,    35,    18,    18,    86,    73,     4,  8006,  1201,   208,\n",
      "          5968,    93,     3, 19690, 19690, 19690, 19690],\n",
      "        [  278,  1194,  1286,  8990,     0,    11,   278,  1194,  1286,  5035,\n",
      "          5865,     8,     3, 19690, 19690, 19690, 19690],\n",
      "        [ 1261,   217,    11,   418,   217,     0,    13,   989,     4,     0,\n",
      "           684,     5, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1885,  4592,  5420,    20,  8224,     1,    30,  3009,   265,  5047,\n",
      "           173, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  558,   207,   377,   637,     5,  1584,  2562,     1,  1375,     0,\n",
      "           622, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 9555,   112,   317,   181,     6,   223,     8,    74,  2068,    98,\n",
      "          2067, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 6456,  2002,  2451,   495,    19,  4861,   385,     6,   126,     7,\n",
      "          3061, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  482,   885,    35,  3268,    59,  5836,  4408,     6,  9033,     0,\n",
      "             3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3251,  1636,     9,    67,    58,   384,     4,  1173,  9014,  7874,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   85,    88,  5381,  5121,  6706,  3200,     0,  3169,    37,   133,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   81,  1584,  7998,  1301,  2026,    49,  6695,    21,   400,   308,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,   134,    83,   359,    33,  1878,   939,     1,  9290,   511,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3845,     7,  1488,   651,  6174,     5,  2727,     2,   963, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  119,  2302,     0,   320,    18,   383,    71,  1153,   135, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1947,   204,  3914,     2,   739,  3835,    26,  3190,   117, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  647,   941,  1809,   779,    12,   539,    26,    84,  1486, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1361,  3818,  1808,    16,   830,  3124,    37,  7941,    98, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1136,     0,  2083,  5355,   156,     8,   467,    15,    65, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  321,     8,  3511,    33,    56,  1378,  4045,   833,  4046, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  669,  4710,  2309,  4558,  4559,     2,     0,     6,  2194, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  395,    18,  5701,  5702,     2,  1377,     8,   229,  3257, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,   346,     0,    66,  2803,  1753,  4477, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1473,  3217,  1347,   707,    11,   296,  3218, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1071,    13,    17,  3266,  2662,    18,  3267, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   68,   244,     4,     0,   976,   942,  1666, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 17/334 [00:10<02:57,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  420,    45,     8,  1624,  1401,   645,  2521,     1,   460,  1259,\n",
      "            46,     7,  6549,  1424,     0,   618,     3],\n",
      "        [ 3330,   925,     1,  2892,   323,   494,    92,   142,  2081,  5932,\n",
      "           166,   168,   209,   100,     0, 19690, 19690],\n",
      "        [ 9076,  6474,     0,     0,     1,     0,   253,    90,  1390,    12,\n",
      "           473,     7,     0,  1965,     3, 19690, 19690],\n",
      "        [ 1410,  2476,  1697,   173,   375,  4657,    82,   121,    74,  3843,\n",
      "          1198,  2845,    16,    15,  1723, 19690, 19690],\n",
      "        [  531,  1245,     0,    39,    35,     9,   116,     0,    17,   544,\n",
      "           116,  3952,     0,    29,     3, 19690, 19690],\n",
      "        [  354,   526,  9653,     0,     5,   676,    49,     0,  4304,   269,\n",
      "           661,   553,  1948,   603, 19690, 19690, 19690],\n",
      "        [  184,  2157,     2,   199,    24,  2128,   110,    25,    30,   407,\n",
      "            27,     6,   154,  3964, 19690, 19690, 19690],\n",
      "        [  174,   333,  3226,    16,   604,  9592,    26,  1307,     0,   468,\n",
      "            37,    38,  2799,     3, 19690, 19690, 19690],\n",
      "        [   39,    35,     0,    59,     0,   280,  1602,   313,     4,  2492,\n",
      "            12,  2950,     3, 19690, 19690, 19690, 19690],\n",
      "        [ 4695,   338,     8,   345,    21,   148,  5732,  4653,  4696,    33,\n",
      "             0,   588,     3, 19690, 19690, 19690, 19690],\n",
      "        [  650,    96,   578,     5,  1660,  2145,     0,  5447,   118,  1758,\n",
      "          1581,     4,     3, 19690, 19690, 19690, 19690],\n",
      "        [  577,  4918,    63,  1698,   291,  6600,     8,    14,  2118,   190,\n",
      "          9492,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1840,     4,     0,  8072,  8739,     9,  3492,   393,  8195,    25,\n",
      "            48,    27, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  201,   845,     2,  1789,   453,   426,    22,   429,     1,   205,\n",
      "             6,   870, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  560,  4488,   958,     6,  7404,   340,     0,    12,  1248,    25,\n",
      "            48,    27, 19690, 19690, 19690, 19690, 19690],\n",
      "        [    0,     0,  3710,    97,  1257,   283,   192,    29,  8795,   388,\n",
      "          2677,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,     0,  2244,    66,  1413,     0,   218,    22,  1302,     1,\n",
      "          2658,    58, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3846,   944,    24,  3790,   648,    74,  2425,     1,  2020,    16,\n",
      "          7699,  1869, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1571,  1451,  1452,  1572,     7,  8628,     0,   373,   389,  8087,\n",
      "             0, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   14,   176,  1474,   705,   313,   559,    55,  1294,    26,   121,\n",
      "          7807, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  606,    11,   531,  6355,     4,   753,   482,    26,   528,  1442,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   39,    35,  1169,  6416,  9713,  3629,     6,  6371,  4756,     0,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  234,   892,   131,   182,    12,     0,  1061,     8,  1417,   372,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1261,   217,    11,   418,   217,     4,  2502,  1804,     0,  5687,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   85,    88,    11,    85,    88,  2496,  2064,    21,    34,  2871,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  456,    35,    63,  2600,    13,   264,     0,    59,  6092,  9261,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1371,   711,  6115,   423,     0,    15,  1460,     1,  1115,    58,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   39,    35,   186,    92,  3615,  1560,     6,    35,     0,  3020,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,   112,  1406,     9,  1863,   380,   182,     8,   102,  2026,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  365,  1693,     0,   379,   466,   612,    20,  2846,    64, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 4750,   722,  3327,     2,   539,  2388,     2,   677,  1302, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  140,  5851,     9,   201,  1977,    98,  1134,   408, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 18/334 [00:11<03:13,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   30,  9052,  1503,   973,   570,    11,   286,  1056,  5945,   343,\n",
      "            38,   146,   961,    41,   107,   316,    32],\n",
      "        [   34,  5795,    32,  1720,  2341,  6031,  6383,  1923,  1625,   388,\n",
      "          1054,  2665,     5,     3,  1701,     3, 19690],\n",
      "        [   28,   328,   169,     2,  1333,    23,  5578,  1734,  7362,    32,\n",
      "           913,  2784,    17,  7363,     3, 19690, 19690],\n",
      "        [ 3698,  2682,  1637,    70,  4257,     2,  2804,     0,  1166,     5,\n",
      "             7,  4063,  4064,   171,     3, 19690, 19690],\n",
      "        [ 1410,  8970,    18,     0,  8181,   285,    92,  4717,     0,     5,\n",
      "          1604,    22,     0,  5922, 19690, 19690, 19690],\n",
      "        [ 8635,     0,     7,   409,  1034,  2922,     2,  7584,     4,  8636,\n",
      "            69,  3696,  1218, 19690, 19690, 19690, 19690],\n",
      "        [  320,  2167,  5737,  6197,     4,  8452,    13,   119,  4688,     2,\n",
      "           646,   383,   135, 19690, 19690, 19690, 19690],\n",
      "        [   44,  8505,     1,    14,   687,   725,   312,    31,     0,     5,\n",
      "          1633,     6,  4199, 19690, 19690, 19690, 19690],\n",
      "        [  258,     4,   575,    13,   459,    26,   237,   289,   941,  5261,\n",
      "            12,  6574, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  9288,   659,   444,  8510,     0,  1075,    12,   716,     1,\n",
      "           348,  2682, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  125,  2799,   693,    13,  1536,   404,     0,  2178,  5619,    25,\n",
      "           184,    27, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1070,  2706,  2242,    16,  4937,    37,   867,   975,    37,  1897,\n",
      "           867,     5, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,     0,     0,     1,     0,   388,    13,   756,     1,   491,\n",
      "            22,  5351, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  987,  5343,     7,  4342,  1554,  1896,   667,  2472,    12,     0,\n",
      "             0,     3, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2079,  6893,     0,  4216,   265,  2080,     0,   443,     1,   406,\n",
      "          1534, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  545,   595,    11,   545,   595,   517,  2158,  4402,    20,   569,\n",
      "           521, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1503,    28,   196,     0,     0,  1085,  4028,   703,    18,     0,\n",
      "             3, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   10,  7236,    63,     0,    51,   683,     9,  1610,     0,  7759,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  103,  3700,     0,  1235,  1215,     0,     8,   234,    18,    47,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1084,   204,     2,  8650,    15,  4338,     9,  1376,  1003,  4013,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   14,    34,   147,   707,  8890,    34,  2442,     0,    37,  1057,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1957,     4,  1265,   826,     0,    46,   318,     0,     8,  5913,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1473,  3352,   481,  2284,   765,   674,    11,    66,  1175, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 1288,  4563,    46,   141,  7536,   657,   185,  7537,  2821, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   39,    35,  1093,     0,     6,  2251,   276,  6404,     0, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  137,   225,     8,  1603,    15,  2382,  1299,   227,    65, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 5842,    11,  5842,   615,    82,    69,     0,     0, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [   15,    51,   405,   111,   172,     2,  7173,   128, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  354,  1492,   542,    34,  1121,     5,   439, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [  960,  1854,    11,   960,  1854,  1005,  1228, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 2362,     4,   723,  5332,    16,  1321,     0, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690],\n",
      "        [ 3820,     0,  1627,    55,  1608,  1200, 19690, 19690, 19690, 19690,\n",
      "         19690, 19690, 19690, 19690, 19690, 19690, 19690]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 18/334 [00:11<03:29,  1.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[800], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./trained_param.npz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m total_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m---> 52\u001b[0m train(model, train_loader, valid_loader, output_path, total_epochs, device)\n",
      "Cell \u001b[0;32mIn[770], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, output_path, total_epochs, device, lr)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# パラメータ更新\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     25\u001b[0m     x \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[661], line 28\u001b[0m, in \u001b[0;36mCreateDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX\u001b[39m.\u001b[39miloc[index]\n\u001b[0;32m---> 28\u001b[0m     input_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(text), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m     29\u001b[0m     labels_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my\u001b[39m.\u001b[39miloc[index], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     31\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m: input_features,\n\u001b[1;32m     32\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: labels_tensor\n\u001b[1;32m     33\u001b[0m }\n",
      "Cell \u001b[0;32mIn[650], line 8\u001b[0m, in \u001b[0;36msentence2index\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      6\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(sentence)\n\u001b[1;32m      7\u001b[0m \u001b[39m# 単語のみのリストに分割する\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m vocab_order, cnt_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mvocab\u001b[39m.\u001b[39;49mmost_common())\n\u001b[1;32m      9\u001b[0m index_output \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m     11\u001b[0m     \u001b[39m# 語彙にないときは 0\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/collections/__init__.py:610\u001b[0m, in \u001b[0;36mCounter.most_common\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[39m# Emulate Bag.sortedByCount from Smalltalk\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems(), key\u001b[39m=\u001b[39;49m_itemgetter(\u001b[39m1\u001b[39;49m), reverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    611\u001b[0m \u001b[39mreturn\u001b[39;00m _heapq\u001b[39m.\u001b[39mnlargest(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m_itemgetter(\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# やっぱりロスが下がらない．．．\n",
    "# 埋め込み単語ベクトルを変更可能にする\n",
    "# emb_weight に初期値を入れることで，モード変更をする\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4, batch_size=64, device='cpu', emb_weight=None) -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        # 追加\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        \n",
    "        # batch_first とは？ -> batch_size と emb の２次元目のサイズが異なるときに合わせている？？\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        self.func = nn.Linear(hidden_size, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        # バッチサイズを固定すると，一番最後の余りの分がおかしくなるので，動的に毎回決める！\n",
    "        self.batch_size = x.size()[0]\n",
    "        h0 = torch.zeros(1, self.batch_size, self.hidden_size, device=self.device) # ここを変更した\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "        out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す # ここを変更した\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        # out = self.func(h_last) #現在のhだけ取り出す\n",
    "        return out\n",
    "    \n",
    "    \n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = RNN(vocab_size, padding_idx, emb_size, hidden_size, n_labels, batch_size, device, embedding_weight_matrix)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e264a2d",
   "metadata": {},
   "source": [
    "<h2 id=\"85-双方向rnn多層化\">85. 双方向RNN・多層化</h2>\n",
    "<p>順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．</p>\n",
    "\n",
    "<p>\\[\\overleftarrow{h}_{T+1} = 0, \\\\\n",
    "\\overleftarrow{h}_t = {\\rm \\overleftarrow{RNN}}(\\mathrm{emb}(x_t), \\overleftarrow{h}_{t+1}), \\\\\n",
    "y = {\\rm softmax}(W^{(yh)} [\\overrightarrow{h}_T; \\overleftarrow{h}_1] + b^{(y)}))\\]</p>\n",
    "\n",
    "<p>ただし，\\(\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}, \\overleftarrow{h}_t \\in \\mathbb{R}^{d_h}\\)はそれぞれ，順方向および逆方向のRNNで求めた時刻\\(t\\)の隠れ状態ベクトル，\\({\\rm \\overleftarrow{RNN}}(x,h)\\)は入力\\(x\\)と次時刻の隠れ状態\\(h\\)から前状態を計算するRNNユニット，\\(W^{(yh)} \\in \\mathbb{R}^{L \\times 2d_h}\\)は隠れ状態ベクトルからカテゴリを予測するための行列，\\(b^{(y)} \\in \\mathbb{R}^{L}\\)はバイアス項である．また，\\([a; b]\\)はベクトル\\(a\\)と\\(b\\)の連結を表す。</p>\n",
    "<p>さらに，双方向RNNを多層化して実験せよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "592b1e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:43<00:00,  1.49it/s]\n",
      "100%|██████████| 42/42 [00:28<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 1.0233958778824086, train_acc = 0.6118494945713217, valid_loss = 0.9286081194877625, valid_acc = 0.5883233532934131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:26<00:00,  1.62it/s]\n",
      "100%|██████████| 42/42 [00:24<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: train_loss = 0.878185788686246, train_acc = 0.649475851740921, valid_loss = 0.8094227313995361, valid_acc = 0.6190119760479041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:17<00:00,  1.69it/s]\n",
      "100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: train_loss = 0.808253781262608, train_acc = 0.6994571321602396, valid_loss = 0.805242657661438, valid_acc = 0.6601796407185628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:11<00:00,  1.75it/s]\n",
      "100%|██████████| 42/42 [00:24<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3: train_loss = 0.7563907426200703, train_acc = 0.7318420067390491, valid_loss = 0.8188757300376892, valid_acc = 0.6699101796407185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:21<00:00,  1.66it/s]\n",
      "100%|██████████| 42/42 [00:25<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4: train_loss = 0.7158668217075224, train_acc = 0.7485960314488955, valid_loss = 0.8426365852355957, valid_acc = 0.6676646706586826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:40<00:00,  1.51it/s]\n",
      "100%|██████████| 42/42 [00:30<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5: train_loss = 0.7093842308068713, train_acc = 0.6381505054286783, valid_loss = 0.8171107172966003, valid_acc = 0.5965568862275449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:30<00:00,  1.59it/s]\n",
      "100%|██████████| 42/42 [00:24<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6: train_loss = 0.6515607636147159, train_acc = 0.713964807188319, valid_loss = 0.7950844168663025, valid_acc = 0.624251497005988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:33<00:00,  1.56it/s]\n",
      "100%|██████████| 42/42 [00:29<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7: train_loss = 0.6121267506978968, train_acc = 0.7616997379258704, valid_loss = 0.8064600825309753, valid_acc = 0.6803892215568862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:45<00:00,  1.48it/s]\n",
      "100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8: train_loss = 0.5837842783469112, train_acc = 0.780700112317484, valid_loss = 0.8587974309921265, valid_acc = 0.6699101796407185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [03:11<00:00,  1.75it/s]\n",
      "100%|██████████| 42/42 [00:21<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9: train_loss = 0.5564715132111693, train_acc = 0.752527143391988, valid_loss = 0.782258927822113, valid_acc = 0.6616766467065869\n"
     ]
    }
   ],
   "source": [
    "# 今度はロスがちょっと下がった！\n",
    "# 双方向と多層化に拡張\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4, batch_size=64, device='cpu', emb_weight=None, bidirectional=False, layers=1) -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # 双方向と多層化に拡張\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layers = layers\n",
    "        self.directions = bidirectional + 1 # 単方向：１， 双方向：2\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        \n",
    "        # batch_first とは？ -> batch_size と emb の２次元目のサイズが異なるときに合わせている？？\n",
    "        # 双方向と多層化に対応        \n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, self.layers, nonlinearity='tanh', batch_first=True, bidirectional=bidirectional)\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        # 双方向に対応， 隠れ層に self.directions を掛ける\n",
    "        self.func = nn.Linear(hidden_size * self.directions, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # バッチサイズを固定すると，一番最後の余りの分がおかしくなるので，動的に毎回決める！\n",
    "        self.batch_size = x.size()[0]\n",
    "        # 多層と双方向に対応， １次元目を 1 -> self.layers * self.directions\n",
    "        h0 = torch.zeros(self.layers * self.directions, self.batch_size, self.hidden_size, device=self.device) # ここを変更した\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "        # 双方向に対応\n",
    "        if self.bidirectional:\n",
    "            out = self.func(torch.cat([h_last[-2], h_last[-1]], dim=1))\n",
    "        else:\n",
    "            out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す # ここを変更した\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        # out = self.func(h_last) #現在のhだけ取り出す\n",
    "        return out\n",
    "    \n",
    "    \n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = RNN(vocab_size, padding_idx, emb_size, hidden_size, n_labels, batch_size, device, embedding_weight_matrix, bidirectional=True, layers=3)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227106d8",
   "metadata": {},
   "source": [
    "<h2 id=\"86-畳み込みニューラルネットワーク-cnn\">86. 畳み込みニューラルネットワーク (CNN)</h2>\n",
    "<p>ID番号で表現された単語列\\(\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)\\)がある．ただし，\\(T\\)は単語列の長さ，\\(x_t \\in \\mathbb{R}^{V}\\)は単語のID番号のone-hot表記である（\\(V\\)は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列\\(\\boldsymbol{x}\\)からカテゴリ\\(y\\)を予測するモデルを実装せよ．</p>\n",
    "<p>ただし，畳み込みニューラルネットワークの構成は以下の通りとする．</p>\n",
    "<ul>\n",
    "<li>単語埋め込みの次元数: \\(d_w\\)</li>\n",
    "<li>畳み込みのフィルターのサイズ: 3 トークン</li>\n",
    "<li>畳み込みのストライド: 1 トークン</li>\n",
    "<li>畳み込みのパディング: あり</li>\n",
    "<li>畳み込み演算後の各時刻のベクトルの次元数: \\(d_h\\)</li>\n",
    "<li>畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を\\(d_h\\)次元の隠れベクトルで表現</li>\n",
    "</ul>\n",
    "<p>すなわち，時刻\\(t\\)の特徴ベクトル\\(p_t \\in \\mathbb{R}^{d_h}\\)は次式で表される．</p>\n",
    "\n",
    "<p>\\[p_t = g(W^{(px)} [\\mathrm{emb}(x_{t-1}); \\mathrm{emb}(x_t); \\mathrm{emb}(x_{t+1})] + b^{(p)}))\\]</p>\n",
    "\n",
    "<p>ただし，\\(W^{(px)} \\in \\mathbb{R}^{d_h \\times 3d_w}, b^{(p)} \\in \\mathbb{R}^{d_h}\\)はCNNのパラメータ，\\(g\\)は活性化関数（例えば\\(\\tanh\\)やReLUなど），\\([a; b; c]\\)はベクトル\\(a, b, c\\)の連結である．なお，行列\\(W^{(px)}\\)の列数が\\(3d_w\\)になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．</p>\n",
    "<p>最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル\\(c \\in \\mathbb{R}^{d_h}\\)を求める．\\(c[i]\\)でベクトル\\(c\\)の\\(i\\)番目の次元の値を表すことにすると，最大値プーリングは次式で表される．</p>\n",
    "\n",
    "<p>\\[c[i] = \\max_{1 \\leq t \\leq T} p_t[i]]\\]</p>\n",
    "\n",
    "<p>最後に，入力文書の特徴ベクトル\\(c\\)に行列\\(W^{(yc)} \\in \\mathbb{R}^{L \\times d_h}\\)とバイアス項\\(b^{(y)} \\in \\mathbb{R}^{L}\\)による線形変換とソフトマックス関数を適用し，カテゴリ\\(y\\)を予測する．</p>\n",
    "\n",
    "<p>\\[y = {\\rm softmax}(W^{(yc)} c + b^{(y)}))\\]</p>\n",
    "\n",
    "<p>なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で\\(y\\)を計算するだけでよい．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "f949ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, out_channels,  emb_size=300, kernel_heights=3, stride=1, n_labels=4, device=\"cpu\", emb_weight=None) -> None:\n",
    "        \"\"\"\n",
    "        stride: 動かす単位（小さいほど細かい）\n",
    "        kenel_height: 窓の大きさ\n",
    "        out_channels: \n",
    "        conv2d: convolution 層（次元を維持しつつ畳み込み）\n",
    "        max_pool1d: pooling層（最大値を取り，ダウンサンプリングする）\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding_idx, 0))\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.func = nn.Linear(out_channels, n_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x).unsqueeze(1)\n",
    "        conv = self.conv(emb)  # 畳み込み層\n",
    "        act = F.relu(conv.squeeze(3))  # 活性化関数\n",
    "        max_pool = F.max_pool1d(act, act.size()[2])  # pooling 層\n",
    "        out = self.func(self.drop(max_pool.squeeze(2)))  # 全結合層？\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "904435e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "予測値：tensor([[ 0.2797, -0.3276, -0.5477, -0.3352]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：0\n",
      "正解ラベル：2\n",
      "予測値：tensor([[ 0.5948, -0.0891, -0.8027,  0.3534]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：0\n",
      "正解ラベル：0\n",
      "予測値：tensor([[ 0.9635, -0.1160, -0.5544,  0.9841]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：3\n",
      "正解ラベル：2\n",
      "予測値：tensor([[ 0.6579,  0.1459, -0.9274,  0.9395]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：3\n",
      "正解ラベル：2\n",
      "予測値：tensor([[ 0.8712,  0.4642, -0.8398,  0.2624]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：0\n",
      "正解ラベル：2\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "out_channels = 50 # ハイパラ？\n",
    "emb_size = 300  # ハイパラ\n",
    "kernel_height = 3\n",
    "stride = 1\n",
    "n_labels = 4  # ラベル数\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model = CNN(vocab_size, padding_idx, out_channels, emb_size, kernel_height, stride, n_labels, device)\n",
    "\n",
    "for i in range(5):\n",
    "    xi = X_train[i]['inputs']\n",
    "    yi = X_train[i]['labels']\n",
    "    pred_probs = model(xi.unsqueeze(0))\n",
    "    print(f\"予測値：{pred_probs}\")\n",
    "    print(f\"予測ラベル：{pred_probs.argmax()}\")\n",
    "    print(f\"正解ラベル：{yi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad7f34",
   "metadata": {},
   "source": [
    "<h2 id=\"87-確率的勾配降下法によるcnnの学習\">87. 確率的勾配降下法によるCNNの学習</h2>\n",
    "<p>確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "385b835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [34:31<00:00,  6.20s/it]\n",
      "100%|██████████| 42/42 [02:08<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 11.279748118267396, train_acc = 0.582366154998128, valid_loss = 5.261847972869873, valid_acc = 0.5546407185628742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [35:28<00:00,  6.37s/it]\n",
      "100%|██████████| 42/42 [02:09<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: train_loss = 5.102930523312882, train_acc = 0.7336203669037814, valid_loss = 1.967748761177063, valid_acc = 0.6706586826347305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [35:17<00:00,  6.34s/it]\n",
      "100%|██████████| 42/42 [02:08<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: train_loss = 1.6193601711998715, train_acc = 0.7936166229876451, valid_loss = 1.1482020616531372, valid_acc = 0.6976047904191617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [35:02<00:00,  6.30s/it]\n",
      "100%|██████████| 42/42 [02:05<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3: train_loss = 0.8907247123000596, train_acc = 0.8742980157244478, valid_loss = 1.0152617692947388, valid_acc = 0.7410179640718563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [35:03<00:00,  6.30s/it]\n",
      "100%|██████████| 42/42 [02:06<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4: train_loss = 0.6636096884007313, train_acc = 0.8873081242980158, valid_loss = 0.9940643310546875, valid_acc = 0.7477544910179641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [34:56<00:00,  6.28s/it]\n",
      "100%|██████████| 42/42 [02:06<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5: train_loss = 0.4966894327947059, train_acc = 0.9120179707974542, valid_loss = 1.0128010511398315, valid_acc = 0.7604790419161677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [35:06<00:00,  6.31s/it]\n",
      "100%|██████████| 42/42 [02:06<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6: train_loss = 0.42624359978178117, train_acc = 0.9087420441782104, valid_loss = 1.1438573598861694, valid_acc = 0.7612275449101796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [35:08<00:00,  6.31s/it]\n",
      "100%|██████████| 42/42 [02:09<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7: train_loss = 0.3629707233350583, train_acc = 0.9338262822912766, valid_loss = 1.040395736694336, valid_acc = 0.7597305389221557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [35:27<00:00,  6.37s/it]\n",
      "100%|██████████| 42/42 [02:11<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8: train_loss = 0.33030957762934393, train_acc = 0.9378509921377761, valid_loss = 1.100417137145996, valid_acc = 0.7604790419161677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [35:26<00:00,  6.37s/it]\n",
      "100%|██████████| 42/42 [02:11<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9: train_loss = 0.28721515234881956, train_acc = 0.9463684013478099, valid_loss = 1.1618684530258179, valid_acc = 0.7791916167664671\n"
     ]
    }
   ],
   "source": [
    "# ちゃんとロスが下がった！！\n",
    "# cpuで回すと10時間かかったらしい\n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = CNN(vocab_size, padding_idx, out_channels, emb_size, kernel_height, stride, n_labels, device)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b734e94",
   "metadata": {},
   "source": [
    "<h2 id=\"88-パラメータチューニング\">88. パラメータチューニング</h2>\n",
    "<p>問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da3e21fc",
   "metadata": {},
   "source": [
    "一番性能が良かったCNNを採用する\n",
    "\n",
    "optuna でパラメータを自動最適化する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "529b1ea9",
   "metadata": {},
   "source": [
    "あまりにも時間がかかるので，src/q88.py で実行した\n",
    "ここに関しては，GPUのほうがCPUよりも3倍くらい早かった気がする\n",
    "running.log に実行履歴が残っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2802d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータチューニング用に引数を変更\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, out_channels,  emb_size=300, kernel_heights=3, stride=1, n_labels=4, device=\"cpu\", emb_weight=None, active_func='relu', dropout=0.3) -> None:\n",
    "        \"\"\"\n",
    "        stride: 動かす単位（小さいほど細かい）\n",
    "        kenel_height: 窓の大きさ\n",
    "        out_channels: \n",
    "        conv2d: convolution 層（次元を維持しつつ畳み込み）\n",
    "        max_pool1d: pooling層（最大値を取り，ダウンサンプリングする）\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding_idx, 0))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.func = nn.Linear(out_channels, n_labels)\n",
    "        self.active_func = active_func # 活性化関数をパラメータにする\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x).unsqueeze(1)\n",
    "        conv = self.conv(emb)  # 畳み込み層\n",
    "\n",
    "        # 活性化関数の最適化を行う\n",
    "        if self.active_func == 'relu':\n",
    "            act = F.relu(conv.squeeze(3))\n",
    "        elif self.active_func == 'tanh':\n",
    "            act = torch.tanh(conv.squeeze(3))\n",
    "        elif self.active_func == 'mish':\n",
    "            act = F.mish(conv.squeeze(3))\n",
    "        else:\n",
    "            act = F.relu(conv.squeeze(3))\n",
    "\n",
    "        max_pool = F.max_pool1d(act, act.size()[2])  # pooling 層\n",
    "        out = self.func(self.drop(max_pool.squeeze(2)))  # 全結合層？\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "488e510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stoppingを差し込む\n",
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, path='checkpoint_model.pth'):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        self.path = path             #ベストモデル格納path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epoch目の処理\n",
    "            self.best_score = score   #1Epoch目はそのままベストスコアとして記録する\n",
    "            self.checkpoint(val_loss, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score < self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する \n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.best_score = score  #ベストスコアを上書き\n",
    "            self.checkpoint(val_loss, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.val_loss_min = val_loss  #その時のlossを記録する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46d0026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータチューニング用に引数と返り値を変更\n",
    "# early stopping の機構を追加\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader # データローダ使ってみる\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 学習率を引数に追加\n",
    "def train(model, train_loader, valid_loader, output_path, total_epochs, device, lr=0.01, op='sgd'):\n",
    "    earlystopping = EarlyStopping(patience=3, verbose=True)\n",
    "    \n",
    "    # 最適化手法を変更\n",
    "    if op == 'sgd':  \n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    elif op == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif op == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    # 指定した epoch 数だけ学習\n",
    "    for epoch in range(total_epochs):\n",
    "        train_total_loss = 0.\n",
    "        train_acc_cnt = 0\n",
    "\n",
    "        # パラメータ更新\n",
    "        model.train()\n",
    "        for data in tqdm(train_loader):\n",
    "            x = data['inputs']\n",
    "            x = x.to(device)\n",
    "            y = data['labels']\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # バッチの中で損失計算\n",
    "            train_loss = 0.\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                loss_i = loss_func(yi_pred, yi)\n",
    "                train_loss += loss_i\n",
    "            \n",
    "            optimizer.zero_grad()  # 勾配の初期化\n",
    "            train_loss.backward()  # 勾配計算\n",
    "            optimizer.step()  # パラメータ修正\n",
    "            train_total_loss += train_loss.item()\n",
    "\n",
    "            # バッチの中で正解率の計算\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                if yi.item() == yi_pred.argmax():\n",
    "                    train_acc_cnt += 1\n",
    "        \n",
    "        #★毎エポックearlystoppingの判定をさせる★\n",
    "        train_ave_loss = train_total_loss / len(X_train)\n",
    "        \n",
    "        earlystopping(train_ave_loss, model) #callメソッド呼び出し\n",
    "        if earlystopping.early_stop: #ストップフラグがTrueの場合、breakでforループを抜ける\n",
    "            print(f\"epoch{epoch}: train_loss = {train_ave_loss}\")\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "                \n",
    "        # train のロスと正解率の計算\n",
    "        model.eval()\n",
    "        train_acc = measure_acc(model, X_train[:]['inputs'], X_train[:]['labels'], device)\n",
    "\n",
    "\n",
    "        # valid のロスと正解率の計算\n",
    "        model.eval()\n",
    "        valid_acc_cnt = 0\n",
    "        valid_total_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(valid_loader):\n",
    "                x = data['inputs']\n",
    "                x = x.to(device)\n",
    "                y = data['labels']\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                # バッチの中で損失計算\n",
    "                valid_loss = 0.\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    # print(yi)\n",
    "                    # print(yi_pred)\n",
    "                    loss_i = loss_func(yi_pred, yi)\n",
    "                    valid_loss += loss_i\n",
    "\n",
    "                optimizer.zero_grad()  # 勾配の初期化\n",
    "                # valid_loss.backward()  # 勾配計算\n",
    "                # optimizer.step()  # パラメータ修正\n",
    "                valid_total_loss += valid_loss\n",
    "\n",
    "                # バッチの中で正解率の計算\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    if yi.item() == yi_pred.argmax():\n",
    "                        valid_acc_cnt += 1\n",
    "\n",
    "            # valid のロスと正解率の計算\n",
    "            valid_acc = measure_acc(model, X_valid[:]['inputs'], X_valid[:]['labels'], device)\n",
    "\n",
    "        # 表示\n",
    "        train_ave_loss = train_total_loss / len(X_train)\n",
    "        # train_acc = train_acc_cnt / len(X_train)\n",
    "        valid_ave_loss = valid_total_loss / len(X_valid)\n",
    "        # valid_acc = valid_acc_cnt / len(X_valid)\n",
    "        print(f\"epoch{epoch}: train_loss = {train_ave_loss}, train_acc = {train_acc}, valid_loss = {valid_ave_loss}, valid_acc = {valid_acc}\")\n",
    "\n",
    "    # パラメータを保存\n",
    "    torch.save(model.state_dict(), output_path)\n",
    "    \n",
    "    # valid loss を返り値とする\n",
    "    return valid_ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bb3aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna でパラメータの自動最適化\n",
    "from typing import Any\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # 固定のもの\n",
    "    vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "    padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "    n_labels = 4  # ラベル数\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(X_valid, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    output_path = \"./trained_param.npz\"\n",
    "    total_epochs = 10\n",
    "\n",
    "    # ハイパラを変更させる\n",
    "    out_channels = trial.suggest_categorical('out_channels', [16, 32, 64, 128])  # これだけよくわかっていない\n",
    "    emb_size = trial.suggest_categorical('emb_size', [50, 100, 200, 300])  # 特徴ベクトルの次元数\n",
    "    kernel_height = trial.suggest_int('kernel_height', 1, 5, step=1)  # 窓の大きさ\n",
    "    stride = trial.suggest_int('stride', 1, 2, step=1)  # 窓を動かす単位\n",
    "    active_func = trial.suggest_categorical('active_func', ['relu', 'tanh', 'mish'])  # 活性化関数\n",
    "    lr = trial.suggest_float('lr', 1e-3, 1e-2, log=True)  # 学習率\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)  # ドロップアウト\n",
    "    op = trial.suggest_categorical('optimizer', ['rmsprop', 'adam', 'sgd'])  # 最適化手法 \n",
    "\n",
    "    print(f\"device: {device}\")\n",
    "    model = CNN(vocab_size, padding_idx, out_channels, emb_size, kernel_height, stride, n_labels, device, active_func=active_func, dropout=dropout)\n",
    "    valid_loss = train(model, train_loader, valid_loader, output_path, total_epochs, device, lr, op)\n",
    "\n",
    "    # 訓練の最後で得られた valid_loss でパラメータチューニングを行う\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6f6f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 21:26:36,072] A new study created in memory with name: no-name-640055c7-92dc-42e9-930c-ccce990283a0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/334 [00:09<27:00,  4.88s/it]\n",
      "[W 2023-11-08 21:26:45,886] Trial 0 failed with parameters: {'out_channels': 16, 'emb_size': 200, 'kernel_height': 5, 'stride': 2, 'active_func': 'relu', 'lr': 0.004388631453821411, 'dropout': 0.4163492420037371, 'optimizer': 'sgd'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nyuton/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/bw/n738lb8d773382cjs4qszbbh0000gn/T/ipykernel_2658/4258090099.py\", line 29, in objective\n",
      "    valid_loss = train(model, train_loader, valid_loader, output_path, total_epochs, device, lr, op)\n",
      "  File \"/var/folders/bw/n738lb8d773382cjs4qszbbh0000gn/T/ipykernel_2658/4177580950.py\", line 50, in train\n",
      "    train_loss.backward()  # 勾配計算\n",
      "  File \"/Users/nyuton/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/Users/nyuton/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-08 21:26:45,890] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 流石に時間がかかりすぎるのでサーバで実行 (100時間くらいかかる？)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[68], line 29\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdevice: \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m model \u001b[39m=\u001b[39m CNN(vocab_size, padding_idx, out_channels, emb_size, kernel_height, stride, n_labels, device, active_func\u001b[39m=\u001b[39mactive_func, dropout\u001b[39m=\u001b[39mdropout)\n\u001b[0;32m---> 29\u001b[0m valid_loss \u001b[39m=\u001b[39m train(model, train_loader, valid_loader, output_path, total_epochs, device, lr, op)\n\u001b[1;32m     31\u001b[0m \u001b[39m# 訓練の最後で得られた valid_loss でパラメータチューニングを行う\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m valid_loss\n",
      "Cell \u001b[0;32mIn[67], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, output_path, total_epochs, device, lr, op)\u001b[0m\n\u001b[1;32m     47\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_i\n\u001b[1;32m     49\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# 勾配の初期化\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# 勾配計算\u001b[39;00m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# パラメータ修正\u001b[39;00m\n\u001b[1;32m     52\u001b[0m train_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 流石に時間がかかりすぎるのでサーバで実行 (100時間くらいかかる？)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d2c2ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m最高精度のACC：\u001b[39m\u001b[39m{\u001b[39;00mstudy\u001b[39m.\u001b[39mbest_value\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m最高精度のパラメータ\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m pprint(study\u001b[39m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/study.py:128\u001b[0m, in \u001b[0;36mStudy.best_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbest_value\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    118\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the best objective value in the study.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     best_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_trial\u001b[39m.\u001b[39mvalue\n\u001b[1;32m    129\u001b[0m     \u001b[39massert\u001b[39;00m best_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m best_value\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/study.py:157\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_multi_objective():\n\u001b[1;32m    152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_storage\u001b[39m.\u001b[39;49mget_best_trial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_study_id))\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/storages/_in_memory.py:234\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[0;34m(self, study_id)\u001b[0m\n\u001b[1;32m    231\u001b[0m best_trial_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_studies[study_id]\u001b[39m.\u001b[39mbest_trial_id\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m best_trial_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo trials are completed yet.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_studies[study_id]\u001b[39m.\u001b[39mdirections) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "print(f\"最高精度のACC：{study.best_value}\")\n",
    "print('最高精度のパラメータ')\n",
    "pprint(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83601e3",
   "metadata": {},
   "source": [
    "<h2 id=\"89-事前学習済み言語モデルからの転移学習\">89. 事前学習済み言語モデルからの転移学習</h2>\n",
    "<p>事前学習済み言語モデル（例えば<a href=\"https://github.com/google-research/bert\">BERT</a>など）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a94ea9f4",
   "metadata": {},
   "source": [
    "こちらもかなり時間がかかるので，src/q89.pyを用いてサーバ上で回した\n",
    "\n",
    "出力ログを q89.running.log ファイルに残した"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db01e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class BERTmodel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "\n",
    "    def forward(self, encoding):\n",
    "        outputs = self.bert_sc(**encoding)\n",
    "        return outputs\n",
    "    \n",
    "class CreateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'inputs': self.X[index],\n",
    "            'labels': self.y[index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "771d0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT用\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader # データローダ使ってみる\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def bert_train(model, train_loader, valid_loader, output_path, total_epochs, device, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # BERTモデルのエンコード用\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "    # 指定した epoch 数だけ学習\n",
    "    for epoch in range(total_epochs):\n",
    "        train_total_loss = 0.\n",
    "        train_acc_cnt = 0\n",
    "\n",
    "        # パラメータ更新\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader):\n",
    "            x_texts = batch['inputs']\n",
    "            x_encordings = tokenizer(\n",
    "                list(x_texts), \n",
    "                max_length=128, \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                return_tensors='pt', \n",
    "                return_attention_mask=True, \n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "            x_encordings = x_encordings.to(device)\n",
    "            y = batch['labels']\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x_encordings).logits\n",
    "\n",
    "            # バッチの中で損失計算\n",
    "            train_loss = loss_func(y_pred, y)\n",
    "\n",
    "            # train_loss = 0.\n",
    "            # for yi, yi_pred in zip(y, y_pred):\n",
    "            #     loss_i = loss_func(yi_pred, yi)\n",
    "            #     train_loss += loss_i\n",
    "            \n",
    "            optimizer.zero_grad() # 勾配の初期化\n",
    "            train_loss.backward()  # 勾配計算\n",
    "            optimizer.step()  # パラメータ修正\n",
    "            train_total_loss += train_loss.item()\n",
    "\n",
    "            # バッチの中で正解率の計算 # ここを修正\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                if yi.item() == yi_pred.argmax():\n",
    "                    train_acc_cnt += 1\n",
    "                \n",
    "        # train のロスと正解率の計算\n",
    "        model.eval()\n",
    "        # train_acc = measure_acc(model, X_train[:]['inputs'], X_train[:]['labels'], device)\n",
    "\n",
    "\n",
    "        # valid のロスと正解率の計算\n",
    "        model.eval()\n",
    "        valid_acc_cnt = 0\n",
    "        valid_total_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_loader):\n",
    "                x_texts = batch['inputs']\n",
    "                x_encordings = tokenizer(\n",
    "                    list(x_texts), \n",
    "                    max_length=128, \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    return_tensors='pt', \n",
    "                    return_attention_mask=True, \n",
    "                    return_token_type_ids=True\n",
    "                )\n",
    "                x_encordings = x_encordings.to(device)\n",
    "                y = batch['labels']\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x_encordings).logits\n",
    "\n",
    "                # バッチの中で損失計算\n",
    "                valid_loss = loss_func(y_pred, y)\n",
    "                # valid_loss = 0.\n",
    "                # for yi, yi_pred in zip(y, y_pred):\n",
    "                #     # print(yi)\n",
    "                #     # print(yi_pred)\n",
    "                #     loss_i = loss_func(yi_pred, yi)\n",
    "                #     valid_loss += loss_i\n",
    "\n",
    "                optimizer.zero_grad()  # 勾配の初期化\n",
    "                # valid_loss.backward()  # 勾配計算\n",
    "                # optimizer.step()  # パラメータ修正\n",
    "                valid_total_loss += valid_loss\n",
    "\n",
    "                # バッチの中で正解率の計算  # ここを修正\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    if yi.item() == yi_pred.argmax():\n",
    "                        valid_acc_cnt += 1\n",
    "\n",
    "            # valid のロスと正解率の計算\n",
    "            # valid_acc = measure_acc(model, X_valid[:]['inputs'], X_valid[:]['labels'], device)\n",
    "\n",
    "        # 表示\n",
    "        train_ave_loss = train_total_loss / len(X_train)\n",
    "        train_acc = train_acc_cnt / len(X_train)\n",
    "        valid_ave_loss = valid_total_loss / len(X_valid)\n",
    "        valid_acc = valid_acc_cnt / len(X_valid)\n",
    "        print(f\"epoch{epoch}: train_loss = {train_ave_loss}, train_acc = {train_acc}, valid_loss = {valid_ave_loss}, valid_acc = {valid_acc}\")\n",
    "\n",
    "    # パラメータを保存\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5dc2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTモデルに入れるためのデータセットの作成\n",
    "category_dict = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "batch_size = 32\n",
    "\n",
    "y_train = torch.tensor(train_data['CATEGORY'].map(category_dict).values, dtype=torch.int64)\n",
    "y_valid = torch.tensor(valid_data['CATEGORY'].map(category_dict).values, dtype=torch.int64)\n",
    "y_test = torch.tensor(test_data['CATEGORY'].map(category_dict).values, dtype=torch.int64)\n",
    "\n",
    "train_set = CreateDataset(train_data['TITLE'].to_list(), y_train)\n",
    "valid_set = CreateDataset(valid_data['TITLE'].to_list(), y_valid)\n",
    "test_set = CreateDataset(test_data['TITLE'].to_list(), y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b853a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 42/42 [04:31<00:00,  6.46s/it]\n",
      "100%|██████████| 42/42 [01:27<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 0.0036510499323275775, train_acc = 0.08152377386746537, valid_loss = 0.025179985910654068, valid_acc = 0.7155688622754491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [04:49<00:00,  6.89s/it]\n",
      "100%|██████████| 42/42 [01:30<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: train_loss = 0.0021149115124386108, train_acc = 0.1012729314863347, valid_loss = 0.011974153108894825, valid_acc = 0.8720059880239521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [05:07<00:00,  7.33s/it]\n",
      "100%|██████████| 42/42 [01:38<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: train_loss = 0.0016038271140701892, train_acc = 0.10922875327592661, valid_loss = 0.011902498081326485, valid_acc = 0.8787425149700598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/42 [00:22<07:37, 11.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[1;32m      4\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m bert_train(model, test_loader, valid_loader, \u001b[39m'\u001b[39;49m\u001b[39mbert_param.npz\u001b[39;49m\u001b[39m'\u001b[39;49m, total_epochs, device, lr)\n",
      "Cell \u001b[0;32mIn[47], line 52\u001b[0m, in \u001b[0;36mbert_train\u001b[0;34m(model, train_loader, valid_loader, output_path, total_epochs, device, lr)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# train_loss = 0.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# for yi, yi_pred in zip(y, y_pred):\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m#     loss_i = loss_func(yi_pred, yi)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m#     train_loss += loss_i\u001b[39;00m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# 勾配の初期化\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# 勾配計算\u001b[39;00m\n\u001b[1;32m     53\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# パラメータ修正\u001b[39;00m\n\u001b[1;32m     54\u001b[0m train_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BERTmodel()\n",
    "total_epochs = 10\n",
    "lr = 0.01\n",
    "device = 'cpu'\n",
    "\n",
    "bert_train(model, train_loader, valid_loader, 'bert_param.npz', total_epochs, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de69cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger('testtest')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info('できてる？')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
