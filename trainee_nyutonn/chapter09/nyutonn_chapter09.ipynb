{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1f70bd",
   "metadata": {},
   "source": [
    "<h2 id=\"80-id番号への変換\">80. ID番号への変換</h2>\n",
    "<p>問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に<code class=\"language-plaintext highlighter-rouge\">1</code>，2番目に頻出する単語に<code class=\"language-plaintext highlighter-rouge\">2</code>，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて<code class=\"language-plaintext highlighter-rouge\">0</code>とせよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdf237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10684 1336 1336\n",
      "10684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>409106</th>\n",
       "      <td>Selena Gomez exposes her derriere in VERY shor...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290867</th>\n",
       "      <td>Hillshire Says Tyson Foods Bid Superior to Pin...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36532</th>\n",
       "      <td>'Friends saw him hit me': Johnny Weir opens up...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358830</th>\n",
       "      <td>'As funny as a liver transplant!' Melissa McCa...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67622</th>\n",
       "      <td>Piers Morgan Delivers One Final Blow To Gun Vi...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    TITLE CATEGORY\n",
       "409106  Selena Gomez exposes her derriere in VERY shor...        e\n",
       "290867  Hillshire Says Tyson Foods Bid Superior to Pin...        b\n",
       "36532   'Friends saw him hit me': Johnny Weir opens up...        e\n",
       "358830  'As funny as a liver transplant!' Melissa McCa...        e\n",
       "67622   Piers Morgan Delivers One Final Blow To Gun Vi...        e"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ分割\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# csvファイルを読み込む\n",
    "path = \"/Users/nyuton/Documents/100knock-2023/trainee_nyutonn/chapter08/data/newsCorpora.csv\"\n",
    "df = pd.read_table(path, header=None, sep='\\\\t', engine='python')\n",
    "df.columns = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "\n",
    "# PUBLISHERが特定の行のみを取り出す\n",
    "publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n",
    "daily_mails = df[df['PUBLISHER'].isin(publishers)]\n",
    "\n",
    "# 訓練データ、検証データ、テストデータに分ける\n",
    "train_data, non_train, train_target, non_train_target = train_test_split(daily_mails[['TITLE', 'CATEGORY']], daily_mails['CATEGORY'], train_size=0.8, random_state=10, stratify=daily_mails['CATEGORY'])\n",
    "valid_data, test_data, valid_target, test_target = train_test_split(non_train, non_train_target, train_size=0.5, random_state=10,  stratify=non_train_target)\n",
    "print(len(train_data), len(valid_data), len(test_data))\n",
    "\n",
    "# テキストファイルに書き込む\n",
    "train_data.to_csv('work/train.txt', header=None, index=None, sep='\\t')\n",
    "valid_data.to_csv('work/valid.txt', header=None, index=None, sep='\\t')\n",
    "test_data.to_csv('work/test.txt', header=None, index=None, sep='\\t')\n",
    "print(len(train_data))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1574a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の回数を数え上げ\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def make_vocab(train_data):\n",
    "    vocab = defaultdict(int)\n",
    "    for id, (title, category) in train_data.iterrows():\n",
    "        # words = title.split()\n",
    "        words = word_tokenize(title)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    vocab = Counter(vocab)\n",
    "    return vocab\n",
    "# vocab.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ee98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kim Kardashian Takes The Plunge In A Simple Black Tee\n",
      "[39, 35, 581, 14, 4855, 20, 24, 8956, 794, 0]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "vocab = make_vocab(train_data)\n",
    "\n",
    "# 単語列から出現頻度インデックスを返す関数\n",
    "def sentence2index(sentence):\n",
    "    # 文を単語列に分割\n",
    "    words = word_tokenize(sentence)\n",
    "    # 単語のみのリストに分割する\n",
    "    vocab_order, cnt_list = zip(*vocab.most_common())\n",
    "    index_output = []\n",
    "    for word in words:\n",
    "        # 語彙にないときは 0\n",
    "        if word not in vocab:\n",
    "            index = 0\n",
    "        # 回数が1のときも0\n",
    "        elif cnt_list[vocab_order.index(word)] == 1:\n",
    "            index = 0\n",
    "        # 語彙にあるとき，0 インデックスなので +1 する\n",
    "        else:  \n",
    "            index = vocab_order.index(word) + 1\n",
    "\n",
    "        index_output.append(index)\n",
    "    return index_output\n",
    "\n",
    "\n",
    "sentence = \"Kim Kardashian Takes The Plunge In A Simple Black Tee\"\n",
    "print(sentence)\n",
    "print(sentence2index(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb47809c",
   "metadata": {},
   "source": [
    "<h2 id=\"81-rnnによる予測\">81. RNNによる予測</h2>\n",
    "<p>ID番号で表現された単語列\\(\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)\\)がある．ただし，\\(T\\)は単語列の長さ，\\(x_t \\in \\mathbb{R}^{V}\\)は単語のID番号のone-hot表記である（\\(V\\)は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列\\(\\boldsymbol{x}\\)からカテゴリ\\(y\\)を予測するモデルとして，次式を実装せよ．</p>\n",
    "\n",
    "<p>\\[\\overrightarrow{h}_0 = 0, \\\\\n",
    "\\overrightarrow{h}_t = {\\rm \\overrightarrow{RNN}}(\\mathrm{emb}(x_t), \\overrightarrow{h}_{t-1}), \\\\\n",
    "y = {\\rm softmax}(W^{(yh)} \\overrightarrow{h}_T + b^{(y)}))\\]</p>\n",
    "\n",
    "<p>ただし，\\(\\mathrm{emb}(x) \\in \\mathbb{R}^{d_w}\\)は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），\\(\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}\\)は時刻\\(t\\)の隠れ状態ベクトル，\\({\\rm \\overrightarrow{RNN}}(x,h)\\)は入力\\(x\\)と前時刻の隠れ状態\\(h\\)から次状態を計算するRNNユニット，\\(W^{(yh)} \\in \\mathbb{R}^{L \\times d_h}\\)は隠れ状態ベクトルからカテゴリを予測するための行列，\\(b^{(y)} \\in \\mathbb{R}^{L}\\)はバイアス項である（\\(d_w, d_h, L\\)はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニット\\({\\rm \\overrightarrow{RNN}}(x,h)\\)には様々な構成が考えられるが，典型例として次式が挙げられる．</p>\n",
    "\n",
    "<p>\\[{\\rm \\overrightarrow{RNN}}(x,h) = g(W^{(hx)} x + W^{(hh)}h + b^{(h)}))\\]</p>\n",
    "\n",
    "<p>ただし，\\(W^{(hx)} \\in \\mathbb{R}^{d_h \\times d_w}，W^{(hh)} \\in \\mathbb{R}^{d_h \\times d_h}, b^{(h)} \\in \\mathbb{R}^{d_h}\\)はRNNユニットのパラメータ，\\(g\\)は活性化関数（例えば\\(\\tanh\\)やReLUなど）である．</p>\n",
    "<p>なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータで\\(y\\)を計算するだけでよい．次元数などのハイパーパラメータは，\\(d_w = 300, d_h=50\\)など，適当な値に設定せよ（以降の問題でも同様である）．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814548e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4) -> None:\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        # Embedding で次元が１つ増える\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        # batch_first とは？ -> バッチサイズの次元を前に持ってくるかどうか\n",
    "        # RNN は変な順番になっているので，batch_first をするといい感じの順番になる\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        self.func = nn.Linear(hidden_size, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.batch_size = x.size()[0]\n",
    "        # h0 = self.init_hidden(x.device)  # h0 の初期化 ゼロベクトル\n",
    "        h0 = torch.zeros(1, self.hidden_size)\n",
    "        # h0 = torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        # RNN は出力が２つ返ってくる\n",
    "        # 1つめは全部の出力，2つめは最後の出力\n",
    "        # 2つめの出力だと，そのまま渡せる\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "\n",
    "        # print(h_last.shape)\n",
    "        # print(h_last[:, -1].shape)\n",
    "        # print(h_last[:, -1])\n",
    "        # 一般の３つの軸：バッチサイズ，系列長，語彙の次元数\n",
    "\n",
    "        # これを実行すると次元数のちがいでエラーになる\n",
    "        # -> 入力した x がバッチサイズを含んでいないから\n",
    "        # out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す\n",
    "        # これもエラーになる\n",
    "        # -> 軸の順番が 系列長，語彙の次元数，だったから\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        # out = self.func(x_rnn[-1, :]) # これだとうまくいく！！\n",
    "        out = self.func(h_last) # 現在のhだけ取り出す\n",
    "        return out\n",
    "\n",
    "    # 隠れ層の初期化\n",
    "    # def init_hidden(self, device):\n",
    "    #     hidden = torch.zeros(1, self.batch_size, self.hidden_size, device=device)\n",
    "    #     return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1787e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # len(Dataset) で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    # Dataset[index] で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        text = self.X.iloc[index]\n",
    "        input_features = self.tokenizer(text)\n",
    "        label = self.y.iloc[index]\n",
    "        return {\n",
    "            'inputs': torch.tensor(input_features, dtype=torch.int64),\n",
    "            'labels': torch.tensor(label, dtype=torch.int64)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b7ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10684\n",
      "{'inputs': tensor([1824,   58, 1825, 1016,  556, 6650,    2, 2535,  117]),\n",
      " 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# ラベル\n",
    "category_dict = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "y_train = train_data['CATEGORY'].map(category_dict)\n",
    "y_valid = valid_data['CATEGORY'].map(category_dict)\n",
    "y_test = test_data['CATEGORY'].map(category_dict)\n",
    "\n",
    "# 特徴量データセット\n",
    "X_train = CreateDataset(train_data['TITLE'], y_train, sentence2index)\n",
    "X_valid = CreateDataset(valid_data['TITLE'], y_valid, sentence2index)\n",
    "X_test = CreateDataset(test_data['TITLE'], y_test, sentence2index)\n",
    "\n",
    "# 使い方の例\n",
    "from pprint import pprint\n",
    "print(len(X_train))\n",
    "pprint(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea873b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0番目\n",
      "入力ベクトル：{'inputs': tensor([ 145,  153, 6648,   59, 5079,    6, 2950, 1823,    0,  741, 6649,    2,\n",
      "          23,    3]), 'labels': tensor(2)}\n",
      "出力ベクトル：tensor([[ 0.1662,  0.1981,  0.8108, -0.2331]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル　：2\n",
      "正解ラベル　：2\n",
      "1番目\n",
      "入力ベクトル：{'inputs': tensor([1824,   58, 1825, 1016,  556, 6650,    2, 2535,  117]), 'labels': tensor(0)}\n",
      "出力ベクトル：tensor([[ 0.1360, -0.3244,  0.2765, -0.3773]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル　：2\n",
      "正解ラベル　：0\n",
      "2番目\n",
      "入力ベクトル：{'inputs': tensor([6651, 6652, 1263,  224, 1637,    5,    7,  324, 4066,  533,   43,  169,\n",
      "         142,    0,    0,    5,   33,    3]), 'labels': tensor(2)}\n",
      "出力ベクトル：tensor([[ 0.1289, -0.0115,  0.7583, -0.3192]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル　：2\n",
      "正解ラベル　：2\n"
     ]
    }
   ],
   "source": [
    "vocab = make_vocab(train_data)\n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "\n",
    "model = RNNmodel(vocab_size, padding_idx, emb_size, hidden_size, n_labels)\n",
    "# model.eval()\n",
    "\n",
    "# 先頭3件の入出力を表示\n",
    "for i in range(3):\n",
    "    print(f\"{i}番目\")\n",
    "    print(f\"入力ベクトル：{X_train[i]}\")\n",
    "    print(f\"出力ベクトル：{model(X_train[i]['inputs'])}\")\n",
    "    print(f\"予測ラベル　：{model(X_train[i]['inputs']).argmax()}\")\n",
    "    print(f\"正解ラベル　：{X_train[i]['labels'].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c80b578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14])\n",
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# 確認 入力サイズがちがう．．．\n",
    "print(X_train[0]['inputs'].shape)\n",
    "print(X_train[1]['inputs'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa6347",
   "metadata": {},
   "source": [
    "<h2 id=\"82-確率的勾配降下法による学習\">82. 確率的勾配降下法による学習</h2>\n",
    "<p>確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cd1c985",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./trained_param.npz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m total_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m---> 79\u001b[0m train(model, output_path, total_epochs)\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, output_path, total_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m y \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m loss \u001b[39m=\u001b[39m loss_func(y_pred, y)  \u001b[39m# 損失計算\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# 勾配の初期化\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/envs/ISHate/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/envs/ISHate/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mRNNmodel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m# ここを変更した\u001b[39;00m\n\u001b[1;32m     22\u001b[0m emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(x)  \u001b[39m# 入力サイズが異なるので統一する\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m x_rnn, h_last \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(emb, h0)  \u001b[39m# RNN\u001b[39;00m\n\u001b[1;32m     24\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(x_rnn[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]) \u001b[39m# 最後の層だけ取り出す # ここを変更した\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# out = self.func(h_last) #現在のhだけ取り出す\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/envs/ISHate/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/envs/ISHate/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/envs/ISHate/lib/python3.11/site-packages/torch/nn/modules/rnn.py:529\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m         \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 529\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    530\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[39m{\u001b[39;00mhx\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    531\u001b[0m         hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(model, output_path, total_epochs):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 指定した epoch 数だけ学習\n",
    "    for epoch in range(total_epochs):\n",
    "        train_total_loss = 0.\n",
    "        train_acc_cnt = 0\n",
    "\n",
    "        # パラメータ更新\n",
    "        model.train()\n",
    "        for data in X_train:\n",
    "            # print(data['inputs'])\n",
    "            # print(model(data['inputs']))\n",
    "            # print(data['labels'])\n",
    "            x = data['inputs']\n",
    "            y = data['labels']\n",
    "            y_pred = model(x)[0]\n",
    "            loss = loss_func(y_pred, y)  # 損失計算\n",
    "            optimizer.zero_grad()  # 勾配の初期化\n",
    "            loss.backward()  # 勾配計算\n",
    "            optimizer.step()  # パラメータ修正\n",
    "            train_total_loss += loss.item()\n",
    "\n",
    "            # 正解率の計算  # ここで計算するのはまずいかも，学習エポックが終わってからやったほうがよさそう\n",
    "            # 次の問題からは修正\n",
    "            if y.item() == y_pred.argmax():\n",
    "                train_acc_cnt += 1\n",
    "\n",
    "        # valid のロスと正解率の計算\n",
    "        model.eval()\n",
    "        valid_acc_cnt = 0\n",
    "        valid_total_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for data in X_valid:\n",
    "                x = data['inputs']\n",
    "                y = data['labels']\n",
    "                y_pred = model(x)[0]\n",
    "                loss = loss_func(y_pred, y)  # 損失計算\n",
    "                optimizer.zero_grad()  # 勾配の初期化\n",
    "                # loss.backward()  # 勾配計算\n",
    "                # optimizer.step()  # パラメータ修正\n",
    "                valid_total_loss += loss.item()\n",
    "\n",
    "                # 正解率の計算\n",
    "                if y.item() == y_pred.argmax():\n",
    "                    valid_acc_cnt += 1\n",
    "\n",
    "        # 表示\n",
    "        train_ave_loss = train_total_loss / len(X_train)\n",
    "        train_acc = train_acc_cnt / len(X_train)\n",
    "        valid_ave_loss = valid_total_loss / len(X_valid)\n",
    "        valid_acc = valid_acc_cnt / len(X_valid)\n",
    "        print(f\"epoch{epoch}: train_loss = {train_ave_loss}, train_acc = {train_acc}, valid_loss = {valid_ave_loss}, valid_acc = {valid_acc}\")\n",
    "\n",
    "    # パラメータを保存\n",
    "    torch.save(model.state_dict(), output_path)\n",
    "\n",
    "vocab = make_vocab(train_data)\n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "\n",
    "model = RNNmodel(vocab_size, padding_idx, emb_size, hidden_size, n_labels)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, output_path, total_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68116e3",
   "metadata": {},
   "source": [
    "<h2 id=\"83-ミニバッチ化gpu上での学習\">83. ミニバッチ化・GPU上での学習</h2>\n",
    "<p>問題82のコードを改変し，\\(B\\)事例ごとに損失・勾配を計算して学習を行えるようにせよ（\\(B\\)の値は適当に選べ）．また，GPU上で学習を実行せよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "304325e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUにしても早くなってないような気がする．．．\n",
    "# 走らせながら正答率を計測する valid acc と 最後にまとめて計算する valid acc2 の結果が異なるのがかなり気になる．．．\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4, batch_size=64, device='cpu') -> None:\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        # batch_first とは？ -> batch_size と emb の２次元目のサイズが異なるときに合わせている？？\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        self.func = nn.Linear(hidden_size, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # バッチサイズを固定すると，一番最後の余りの分がおかしくなるので，動的に毎回決める！\n",
    "        self.batch_size = x.size()[0]\n",
    "        h0 = torch.zeros(1, self.batch_size, self.hidden_size, device=self.device) # ここを変更した\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "        out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す # ここを変更した\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        # out = self.func(h_last) #現在のhだけ取り出す\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762221e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # len(Dataset) で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    # Dataset[index] で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        titles = self.X.iloc[index]\n",
    "        # スライス記法のとき\n",
    "        if type(index) == slice:\n",
    "            labels = self.y.iloc[index]\n",
    "            input_features = []\n",
    "            labels_tensor = []\n",
    "            for title, label in zip(titles, labels):\n",
    "                input_feature = torch.tensor(self.tokenizer(title))\n",
    "                input_features.append(input_feature)\n",
    "                labels_tensor.append(torch.tensor(label, dtype=torch.int64))\n",
    "        else:\n",
    "            text = self.X.iloc[index]\n",
    "            input_features = torch.tensor(self.tokenizer(text), dtype=torch.int64)\n",
    "            labels_tensor = torch.tensor(self.y.iloc[index], dtype=torch.int64)\n",
    "        return {\n",
    "            'inputs': input_features,\n",
    "            'labels': labels_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88be080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CreateDataset2(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    # len(Dataset) で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    # Dataset[index] で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        titles = self.X[index]\n",
    "        # print(titles)\n",
    "        # スライス記法のとき\n",
    "        if type(index) == slice:\n",
    "            labels = self.y[index]\n",
    "            input_features = []\n",
    "            labels_tensor = []\n",
    "            for title, label in zip(titles, labels):\n",
    "                print(title, label)\n",
    "                input_feature = torch.tensor(title)\n",
    "                input_features.append(input_feature)\n",
    "                labels_tensor.append(torch.tensor(label, dtype=torch.int64))\n",
    "        else:\n",
    "            text = self.X[index]\n",
    "            input_features = torch.tensor(text, dtype=torch.int64)\n",
    "            labels_tensor = torch.tensor(self.y.iloc[index], dtype=torch.int64)\n",
    "        return {\n",
    "            'inputs': input_features,\n",
    "            'labels': labels_tensor\n",
    "        }\n",
    "    \n",
    "# 単語列から出現頻度インデックスを返す関数\n",
    "def words2index(words, vocab):\n",
    "    # 単語のみのリストに分割する\n",
    "    vocab_order, cnt_list = zip(*vocab.most_common())\n",
    "    index_output = []\n",
    "    for word in words:\n",
    "        # 語彙にないときは 0\n",
    "        if word not in vocab:\n",
    "            index = 0\n",
    "        # 回数が1のときも0\n",
    "        elif cnt_list[vocab_order.index(word)] == 1:\n",
    "            index = 0\n",
    "        # 語彙にあるとき，0 インデックスなので +1 する\n",
    "        else:  \n",
    "            index = vocab_order.index(word) + 1\n",
    "\n",
    "        index_output.append(index)\n",
    "    return index_output\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    tokenized_dataset = []\n",
    "    for data in dataset:\n",
    "        tokenized_dataset.append(word_tokenize(data))\n",
    "    return tokenized_dataset\n",
    "\n",
    "# データ取得\n",
    "train_data = pd.read_table('./work/train.txt', names=['TITLE', 'CATEGORY'])\n",
    "valid_data = pd.read_table('./work/valid.txt', names=['TITLE', 'CATEGORY'])\n",
    "test_data = pd.read_table('./work/test.txt', names=['TITLE', 'CATEGORY'])\n",
    "\n",
    "# nltk で tokenized する\n",
    "train_tokenized = tokenize_dataset(train_data['TITLE'])\n",
    "valid_tokenized = tokenize_dataset(train_data['TITLE'])\n",
    "test_tokenized = tokenize_dataset(train_data['TITLE'])\n",
    "\n",
    "# id に変換する\n",
    "train_tokenized_id = [words2index(words, vocab) for words in train_tokenized]\n",
    "valid_tokenized_id = [words2index(words, vocab) for words in valid_tokenized]\n",
    "test_tokenized_id = [words2index(words, vocab) for words in test_tokenized]\n",
    "\n",
    "# rnn に実際に渡す形にする\n",
    "X_train_tokenized = CreateDataset2(train_tokenized_id, y_train)\n",
    "X_valid_tokenized = CreateDataset2(valid_tokenized_id, y_valid)\n",
    "X_test_tokenized = CreateDataset2(test_tokenized_id, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b6ab5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': tensor([ 145,  153, 6648,   59, 5079,    6, 2950, 1823,    0,  741, 6649,    2,\n",
       "           23,    3]),\n",
       " 'labels': tensor(2)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5710cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('./work/train_tokenized.txt', 'wb')\n",
    "pickle.dump(train_tokenized, f)\n",
    "f = open('./work/valid_tokenized.txt', 'wb')\n",
    "pickle.dump(valid_tokenized, f)\n",
    "f = open('./work/test_tokenized.txt', 'wb')\n",
    "pickle.dump(test_tokenized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19f2724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text をロード\n",
    "f = open('./work/train_tokenized.txt', 'rb')\n",
    "train_tokenized = pickle.load(f)\n",
    "f = open('./work/valid_tokenized.txt', 'rb')\n",
    "valid_tokenized = pickle.load(f)\n",
    "f = open('./work/test_tokenized.txt', 'rb')\n",
    "test_tokenized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bad7705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = CreateDataset(train_data['TITLE'], y_train, sentence2index)\n",
    "X_valid = CreateDataset(valid_data['TITLE'], y_valid, sentence2index)\n",
    "X_test = CreateDataset(test_data['TITLE'], y_test, sentence2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c3fc1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サーバ上で動かなかった問題解決 -> model.to('cpu') を model に入れていなかった -> 正） model = model.to('cpu')\n",
    "# 今度はhiddenとinputのdeviceが違うというエラーが発生 -> xiをcudaに戻したら解決\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def measure_acc(model, X, y, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_y = []\n",
    "        for xi in X:\n",
    "            xi = xi.to(device)\n",
    "            pred_yi = model(xi[None]).argmax()\n",
    "            pred_yi = pred_yi.to('cpu')\n",
    "            pred_y.append(pred_yi)\n",
    "    return accuracy_score(pred_y, y)\n",
    "\n",
    "# 単語の回数を数え上げ\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# vocabを生成\n",
    "def make_vocab(train_data):\n",
    "    vocab = defaultdict(int)\n",
    "    for id, (title, category) in train_data.iterrows():\n",
    "        # words = title.split()\n",
    "        words = word_tokenize(title)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    vocab = Counter(vocab)\n",
    "    return vocab\n",
    "# vocab.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e01368f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修正版\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader # データローダ使ってみる\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb # 追加\n",
    "# wait and biases\n",
    "import wandb\n",
    "\n",
    "\n",
    "def train(model, train_loader, valid_loader, output_path, total_epochs, device, lr=0.01):\n",
    "    wandb.init(project=\"chapter09_83\")\n",
    "    wandb.run.name = 'modify-tokenized-run'\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    # 指定した epoch 数だけ学習\n",
    "    for epoch in range(total_epochs):\n",
    "        train_total_loss = 0.\n",
    "        train_acc_cnt = 0\n",
    "        train_cnt = 0\n",
    "\n",
    "        # パラメータ更新\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            x = data['inputs']\n",
    "            x = x.to(device)\n",
    "            y = data['labels']\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # バッチの中で損失計算\n",
    "            train_loss = 0.\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                loss_i = loss_func(yi_pred, yi)\n",
    "                train_loss += loss_i\n",
    "            \n",
    "            optimizer.zero_grad()  # 勾配の初期化\n",
    "            train_loss.backward()  # 勾配計算\n",
    "            optimizer.step()  # パラメータ修正\n",
    "            train_total_loss += train_loss.item()\n",
    "\n",
    "            # バッチの中で正解率の計算 # ここを修正\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                if yi.item() == yi_pred.argmax():\n",
    "                    train_acc_cnt += 1\n",
    "                train_cnt += 1\n",
    "\n",
    "            # wandb に10ステップごとにログを書く\n",
    "            if i % 10 == 0:\n",
    "                train_running_loss = train_total_loss / train_cnt\n",
    "                train_running_acc = train_acc_cnt / train_cnt\n",
    "                wandb.log({'train_loss': train_running_loss, 'train_acc': train_running_acc})\n",
    "                \n",
    "        # train のロスと正解率の計算\n",
    "        # model.eval()\n",
    "        # train_acc2 = measure_acc(model, X_train[:]['inputs'], X_train[:]['labels'], device)\n",
    "\n",
    "\n",
    "        # valid のロスと正解率の計算\n",
    "        model.eval()\n",
    "        valid_acc_cnt = 0\n",
    "        valid_total_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(valid_loader)):\n",
    "                x = data['inputs']\n",
    "                x = x.to(device)\n",
    "                y = data['labels']\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                # バッチの中で損失計算\n",
    "                valid_loss = 0.\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    # print(yi)\n",
    "                    # print(yi_pred)\n",
    "                    loss_i = loss_func(yi_pred, yi)\n",
    "                    valid_loss += loss_i\n",
    "\n",
    "                optimizer.zero_grad()  # 勾配の初期化\n",
    "                # valid_loss.backward()  # 勾配計算\n",
    "                # optimizer.step()  # パラメータ修正\n",
    "                valid_total_loss += valid_loss\n",
    "\n",
    "                # バッチの中で正解率の計算  # ここを修正\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    if yi.item() == yi_pred.argmax():\n",
    "                        valid_acc_cnt += 1\n",
    "\n",
    "\n",
    "            # valid のロスと正解率の計算\n",
    "            # valid_acc2 = measure_acc(model, X_valid[:]['inputs'], X_valid[:]['labels'], device)\n",
    "\n",
    "        # 表示\n",
    "        train_ave_loss = train_total_loss / len(X_train_tokenized)\n",
    "        train_acc = train_acc_cnt / len(X_train_tokenized)\n",
    "        valid_ave_loss = valid_total_loss / len(X_valid_tokenized)\n",
    "        valid_acc = valid_acc_cnt / len(X_valid_tokenized)\n",
    "        print(f\"epoch{epoch}: train_loss = {train_ave_loss}, train_acc = {train_acc}, valid_loss = {valid_ave_loss}, valid_acc = {valid_acc}\")\n",
    "        # print(f'train_acc2: {train_acc2}, valid_acc2: {valid_acc2}')\n",
    "\n",
    "    # パラメータを保存\n",
    "    torch.save(model.state_dict(), output_path)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "56611e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nyuton/Documents/100knock-2023/trainee_nyutonn/chapter09/wandb/run-20231109_181549-mibd6gjp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nakano_test/chapter09_83/runs/mibd6gjp' target=\"_blank\">rich-leaf-6</a></strong> to <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nakano_test/chapter09_83/runs/mibd6gjp' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/mibd6gjp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 158.07it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 701.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 1.1195700514053235, train_acc = 0.541463871209285, valid_loss = 1.2387775182724, valid_acc = 0.41467065868263475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 166.90it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 709.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: train_loss = 1.1781565783310366, train_acc = 0.4976600524148259, valid_loss = 1.2570102214813232, valid_acc = 0.4176646706586826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 168.46it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 636.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: train_loss = 1.1999710910287256, train_acc = 0.47791089479595655, valid_loss = 1.4195775985717773, valid_acc = 0.39895209580838326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 159.02it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 674.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3: train_loss = 1.1283182767570263, train_acc = 0.5428678397603893, valid_loss = 1.341603398323059, valid_acc = 0.4161676646706587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 165.10it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 655.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4: train_loss = 1.1225296284366038, train_acc = 0.5623362036690378, valid_loss = 1.2817877531051636, valid_acc = 0.4176646706586826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 152.97it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 560.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5: train_loss = 1.2834279893355975, train_acc = 0.4635904155746911, valid_loss = 1.363749384880066, valid_acc = 0.405688622754491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 149.76it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 628.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6: train_loss = 1.3568002125244827, train_acc = 0.461344065892924, valid_loss = 1.2740203142166138, valid_acc = 0.4086826347305389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 165.85it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 813.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7: train_loss = 1.686722041872368, train_acc = 0.4219393485585923, valid_loss = 1.4258145093917847, valid_acc = 0.2776946107784431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 167.82it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 779.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8: train_loss = 1.9663927106096788, train_acc = 0.4134219393485586, valid_loss = 2.5287065505981445, valid_acc = 0.4244011976047904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 172.36it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 646.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9: train_loss = 1.7828145623430247, train_acc = 0.422688131785848, valid_loss = 1.567039966583252, valid_acc = 0.41467065868263475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▃▆▇▇▆▆▆▅▆▅▄▄▆▇▇▇█▇██▅▆▄▄▃▂▃▃▄▂▂▂▁▁▂▁▁▁▁▂</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▂▁▂▂▃▃▃▃▂▃▄▅▇█▇▇█▇▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.42249</td></tr><tr><td>train_loss</td><td>1.77866</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-leaf-6</strong> at: <a href='https://wandb.ai/nakano_test/chapter09_83/runs/mibd6gjp' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/mibd6gjp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231109_181549-mibd6gjp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 訓練データの最後で何故かエラーが起きる -> あまりの分がおかしくなっていた！\n",
    "# ロスが下がっていない．．．なぜだ．．．\n",
    "# あまりにも時間がかかる -> 最初に nltk_tokenized してからデータを渡したらめちゃめちゃ早くなった！\n",
    "# 40分 -> 40秒\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "#バッチサイズ\n",
    "batch_size = 32\n",
    "vocab = make_vocab(train_data)\n",
    "PADDING_IDX = len(vocab)\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "#ミニバッチを取り出して長さを揃える関数\n",
    "def collate_fn(batch):\n",
    "    sorted_batch = sorted(batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n",
    "    sequences = [x['inputs'] for x in sorted_batch]\n",
    "    # padding 処理\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PADDING_IDX)\n",
    "    labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n",
    "    return {'inputs': sequences_padded, 'labels': labels}\n",
    "\n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(X_train_tokenized, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid_tokenized, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test_tokenized, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = RNNmodel(vocab_size, padding_idx, emb_size, hidden_size, n_labels, batch_size, device)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e022fb",
   "metadata": {},
   "source": [
    "<h2 id=\"84-単語ベクトルの導入\">84. 単語ベクトルの導入</h2>\n",
    "<p>事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での<a href=\"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\">学習済み単語ベクトル</a>）で単語埋め込み\\(\\mathrm{emb}(x)\\)を初期化し，学習せよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c318b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm # 進捗表示\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 70からもってきた\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('./../chapter08/data/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "vocab_size = len(vocab) + 1\n",
    "emb_size = 300\n",
    "padding_idx = len(vocab)\n",
    "\n",
    "rnn_emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('./../chapter08/data/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "embedding_weight_matrix = rnn_emb(torch.tensor([0])).detach().numpy().copy()\n",
    "\n",
    "# ID持ちの単語が学習済み単語ベクトルを持っていればそれを行方向に足していき，なければnn.Embeddingのベクトルを行方向に足していく\n",
    "for key, value in vocab.items():\n",
    "    try:\n",
    "        embedding_weight_matrix = np.vstack((embedding_weight_matrix, word2vec_model[key]))\n",
    "    except KeyError:\n",
    "        embedding_weight_matrix = np.vstack((embedding_weight_matrix, rnn_emb(torch.tensor([value])).detach().numpy().copy()))\n",
    "    \n",
    "embedding_weight_matrix = np.vstack((embedding_weight_matrix, np.zeros(emb_size, dtype=np.float32)))  # paddingの分\n",
    "embedding_weight_matrix = torch.from_numpy(embedding_weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9ffa989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nyuton/Documents/100knock-2023/trainee_nyutonn/chapter09/wandb/run-20231109_181657-5srxvi90</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nakano_test/chapter09_83/runs/5srxvi90' target=\"_blank\">bumbling-sky-7</a></strong> to <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nakano_test/chapter09_83/runs/5srxvi90' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/5srxvi90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 282.81it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 547.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 1.1600203039225725, train_acc = 0.47079745413702734, valid_loss = 1.1983046531677246, valid_acc = 0.40718562874251496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 306.11it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 499.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: train_loss = 1.1352735627951671, train_acc = 0.4990640209659304, valid_loss = 1.2707889080047607, valid_acc = 0.41392215568862273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 315.23it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 512.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: train_loss = 1.1646572150170067, train_acc = 0.47285660801198054, valid_loss = 1.2272007465362549, valid_acc = 0.4184131736526946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 290.58it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 518.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3: train_loss = 1.1323917480677266, train_acc = 0.5125421190565331, valid_loss = 1.2472282648086548, valid_acc = 0.4116766467065868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 319.46it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 572.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4: train_loss = 1.127316132505839, train_acc = 0.528453762635717, valid_loss = 1.2504022121429443, valid_acc = 0.4116766467065868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 279.66it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 428.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5: train_loss = 1.1492555729685006, train_acc = 0.5042119056533134, valid_loss = 1.2610063552856445, valid_acc = 0.4154191616766467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 305.71it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 370.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6: train_loss = 1.138463007470991, train_acc = 0.5115125421190565, valid_loss = 1.3136309385299683, valid_acc = 0.41317365269461076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 281.86it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 462.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7: train_loss = 1.1461825918856743, train_acc = 0.5067390490453014, valid_loss = 1.2337405681610107, valid_acc = 0.41467065868263475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 263.02it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 419.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8: train_loss = 1.2122050458150426, train_acc = 0.46686634219393486, valid_loss = 1.2385830879211426, valid_acc = 0.4176646706586826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:01<00:00, 308.60it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 511.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9: train_loss = 1.4167171244048216, train_acc = 0.45011231748408836, valid_loss = 1.282361626625061, valid_acc = 0.4124251497005988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▂▃▄▄▆▆▆▆▄▄▄▆▅▆▇█▇▇▇▇▆▇▆▆▆▇▇▅▆▆▆▆▆▅▄▄▄▄▄</td></tr><tr><td>train_loss</td><td>▄▃▃▂▂▂▂▂▂▃▃▃▃▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▃▂▂▂▃▃▃▃▄▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.45043</td></tr><tr><td>train_loss</td><td>1.41579</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-sky-7</strong> at: <a href='https://wandb.ai/nakano_test/chapter09_83/runs/5srxvi90' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/5srxvi90</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231109_181657-5srxvi90/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# やっぱりロスが下がらない．．．\n",
    "# 埋め込み単語ベクトルを変更可能にする\n",
    "# emb_weight に初期値を入れることで，モード変更をする\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4, batch_size=64, device='cpu', emb_weight=None) -> None:\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        # 追加\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        \n",
    "        # batch_first とは？ -> batch_size と emb の２次元目のサイズが異なるときに合わせている？？\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        self.func = nn.Linear(hidden_size, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        # バッチサイズを固定すると，一番最後の余りの分がおかしくなるので，動的に毎回決める！\n",
    "        self.batch_size = x.size()[0]\n",
    "        h0 = torch.zeros(1, self.batch_size, self.hidden_size, device=self.device) # ここを変更した\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "        out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す # ここを変更した\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        # out = self.func(h_last) #現在のhだけ取り出す\n",
    "        return out\n",
    "    \n",
    "    \n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(X_train_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = RNNmodel(vocab_size, padding_idx, emb_size, hidden_size, n_labels, batch_size, device, embedding_weight_matrix)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43b4d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ekfvzv7a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-thunder-11</strong> at: <a href='https://wandb.ai/nakano_test/chapter09_83/runs/ekfvzv7a' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/ekfvzv7a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231116_113053-ekfvzv7a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ekfvzv7a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nyuton/Documents/100knock-2023/trainee_nyutonn/chapter09/wandb/run-20231116_113134-mnk3pflg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nakano_test/chapter09_83/runs/mnk3pflg' target=\"_blank\">devoted-pyramid-12</a></strong> to <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nakano_test/chapter09_83/runs/mnk3pflg' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/mnk3pflg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:03<00:00, 832.43it/s]\n",
      "100%|██████████| 334/334 [00:00<00:00, 2602.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 1.1205019014174462, train_acc = 0.5296705353800075, valid_loss = 1.2792941331863403, valid_acc = 0.40793413173652693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:02<00:00, 898.58it/s]\n",
      "100%|██████████| 334/334 [00:00<00:00, 2860.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: train_loss = 1.092181903744226, train_acc = 0.5595282665668289, valid_loss = 1.2651035785675049, valid_acc = 0.39820359281437123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:02<00:00, 985.74it/s] \n",
      "100%|██████████| 334/334 [00:00<00:00, 2948.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: train_loss = 1.0642659534780063, train_acc = 0.5895731935604642, valid_loss = 1.2983145713806152, valid_acc = 0.4199101796407186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:02<00:00, 987.49it/s] \n",
      "100%|██████████| 334/334 [00:00<00:00, 2913.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3: train_loss = 1.0875748823317013, train_acc = 0.5656121302882815, valid_loss = 1.4570056200027466, valid_acc = 0.38922155688622756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:02<00:00, 998.32it/s] \n",
      "100%|██████████| 334/334 [00:00<00:00, 2853.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4: train_loss = 1.077451624408593, train_acc = 0.5794646199925122, valid_loss = 1.3294237852096558, valid_acc = 0.4101796407185629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:02<00:00, 945.23it/s] \n",
      "100%|██████████| 334/334 [00:00<00:00, 2665.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5: train_loss = 1.0854484595997151, train_acc = 0.5723511793335829, valid_loss = 1.3304475545883179, valid_acc = 0.4086826347305389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:02<00:00, 951.28it/s] \n",
      "100%|██████████| 334/334 [00:00<00:00, 2849.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6: train_loss = 1.0888683725188886, train_acc = 0.5659865219019093, valid_loss = 1.2992968559265137, valid_acc = 0.405688622754491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:02<00:00, 973.86it/s] \n",
      "100%|██████████| 334/334 [00:00<00:00, 2864.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7: train_loss = 1.099557658687418, train_acc = 0.5601834518906776, valid_loss = 1.2932552099227905, valid_acc = 0.3884730538922156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:02<00:00, 973.71it/s] \n",
      "100%|██████████| 334/334 [00:00<00:00, 2821.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8: train_loss = 1.0804013058693835, train_acc = 0.5830213403219768, valid_loss = 1.2701021432876587, valid_acc = 0.3997005988023952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [00:03<00:00, 871.90it/s]\n",
      "100%|██████████| 334/334 [00:00<00:00, 2689.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9: train_loss = 1.1112056158591817, train_acc = 0.548764507675028, valid_loss = 1.2645072937011719, valid_acc = 0.406437125748503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▁▂▂▅▃▅▄██▇▇▃▄▅▅▆▇▇▇▆▅▆▆▃▆▇▆▃▅▆▅▃▅▆▇█▆▅▄</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▅▄▄▂▁▂▂▅▄▄▃▃▂▂▂▃▃▃▃▆▂▂▃▆▄▃▄▅▄▃▃▁▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.54876</td></tr><tr><td>train_loss</td><td>1.11121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-pyramid-12</strong> at: <a href='https://wandb.ai/nakano_test/chapter09_83/runs/mnk3pflg' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/mnk3pflg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231116_113134-mnk3pflg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# やっぱりロスが下がらない．．．\n",
    "# 埋め込み単語ベクトルを変更可能にする\n",
    "# emb_weight に初期値を入れることで，モード変更をする\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4, batch_size=64, device='cpu', emb_weight=None) -> None:\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        # 追加\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        \n",
    "        # batch_first とは？ -> batch_size と emb の２次元目のサイズが異なるときに合わせている？？\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        self.func = nn.Linear(hidden_size, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        # バッチサイズを固定すると，一番最後の余りの分がおかしくなるので，動的に毎回決める！\n",
    "        self.batch_size = x.size()[0]\n",
    "        h0 = torch.zeros(1, self.batch_size, self.hidden_size, device=self.device) # ここを変更した\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "        # out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す # ここを変更した\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        out = self.func(h_last[-1]) #現在のhだけ取り出す\n",
    "        return out\n",
    "\n",
    "PADDING_IDX = len(vocab)\n",
    "    \n",
    "#ミニバッチを取り出して長さを揃える関数\n",
    "def collate_fn(batch):\n",
    "    sorted_batch = sorted(batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n",
    "    sequences = [x['inputs'] for x in sorted_batch]\n",
    "    # padding 処理\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PADDING_IDX)\n",
    "    labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n",
    "    return {'inputs': sequences_padded, 'labels': labels}\n",
    "\n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(X_train_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = RNNmodel(vocab_size, padding_idx, emb_size, hidden_size, n_labels, batch_size, device, embedding_weight_matrix)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e264a2d",
   "metadata": {},
   "source": [
    "<h2 id=\"85-双方向rnn多層化\">85. 双方向RNN・多層化</h2>\n",
    "<p>順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．</p>\n",
    "\n",
    "<p>\\[\\overleftarrow{h}_{T+1} = 0, \\\\\n",
    "\\overleftarrow{h}_t = {\\rm \\overleftarrow{RNN}}(\\mathrm{emb}(x_t), \\overleftarrow{h}_{t+1}), \\\\\n",
    "y = {\\rm softmax}(W^{(yh)} [\\overrightarrow{h}_T; \\overleftarrow{h}_1] + b^{(y)}))\\]</p>\n",
    "\n",
    "<p>ただし，\\(\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}, \\overleftarrow{h}_t \\in \\mathbb{R}^{d_h}\\)はそれぞれ，順方向および逆方向のRNNで求めた時刻\\(t\\)の隠れ状態ベクトル，\\({\\rm \\overleftarrow{RNN}}(x,h)\\)は入力\\(x\\)と次時刻の隠れ状態\\(h\\)から前状態を計算するRNNユニット，\\(W^{(yh)} \\in \\mathbb{R}^{L \\times 2d_h}\\)は隠れ状態ベクトルからカテゴリを予測するための行列，\\(b^{(y)} \\in \\mathbb{R}^{L}\\)はバイアス項である．また，\\([a; b]\\)はベクトル\\(a\\)と\\(b\\)の連結を表す。</p>\n",
    "<p>さらに，双方向RNNを多層化して実験せよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "592b1e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nyuton/Documents/100knock-2023/trainee_nyutonn/chapter09/wandb/run-20231109_181720-jw3l5tul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nakano_test/chapter09_83/runs/jw3l5tul' target=\"_blank\">misunderstood-deluge-8</a></strong> to <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nakano_test/chapter09_83/runs/jw3l5tul' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/jw3l5tul</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:03<00:00, 110.60it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 296.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 1.033255908963208, train_acc = 0.5848932983901161, valid_loss = 1.528912901878357, valid_acc = 0.38547904191616766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 116.35it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 284.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: train_loss = 0.8740515920474082, train_acc = 0.6771808311493822, valid_loss = 1.831870675086975, valid_acc = 0.37350299401197606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 119.43it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 292.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: train_loss = 0.8174228320091416, train_acc = 0.6977723698989142, valid_loss = 1.669603943824768, valid_acc = 0.39895209580838326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 121.34it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 291.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3: train_loss = 0.7720045291810659, train_acc = 0.7175215275177836, valid_loss = 1.857621431350708, valid_acc = 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 123.16it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 285.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4: train_loss = 0.7209171405315221, train_acc = 0.7299700486709098, valid_loss = 2.036397695541382, valid_acc = 0.36227544910179643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:03<00:00, 107.68it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 187.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5: train_loss = 0.6848365614272288, train_acc = 0.7436353425683264, valid_loss = 2.268230438232422, valid_acc = 0.37649700598802394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:03<00:00, 110.00it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 253.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6: train_loss = 0.6584594965605912, train_acc = 0.755335080494197, valid_loss = 2.163968324661255, valid_acc = 0.38772455089820357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 116.59it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 287.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7: train_loss = 0.6272403965367668, train_acc = 0.7690003743916136, valid_loss = 2.366326093673706, valid_acc = 0.39221556886227543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:03<00:00, 97.09it/s] \n",
      "100%|██████████| 42/42 [00:00<00:00, 229.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8: train_loss = 0.5942053247685292, train_acc = 0.7816360913515538, valid_loss = 2.5697007179260254, valid_acc = 0.3884730538922156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:02<00:00, 117.05it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 221.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9: train_loss = 0.5916570539735132, train_acc = 0.7819168850617746, valid_loss = 2.5829761028289795, valid_acc = 0.38173652694610777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▂▃▃▅▅▅▅▆▆▆▆▇▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▇▇▇▅▅▅▅▄▄▄▄▂▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.78163</td></tr><tr><td>train_loss</td><td>0.59182</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misunderstood-deluge-8</strong> at: <a href='https://wandb.ai/nakano_test/chapter09_83/runs/jw3l5tul' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/jw3l5tul</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231109_181720-jw3l5tul/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 今度はロスがちょっと下がった！\n",
    "# 双方向と多層化に拡張\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, emb_size=300, hidden_size=50, n_labels=4, batch_size=64, device='cpu', emb_weight=None, bidirectional=False, layers=1) -> None:\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # 双方向と多層化に拡張\n",
    "        self.bidirectional = bidirectional\n",
    "        self.layers = layers\n",
    "        self.directions = bidirectional + 1 # 単方向：１， 双方向：2\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        \n",
    "        # batch_first とは？ -> batch_size と emb の２次元目のサイズが異なるときに合わせている？？\n",
    "        # 双方向と多層化に対応        \n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, self.layers, nonlinearity='tanh', batch_first=True, bidirectional=bidirectional)\n",
    "        # self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh')\n",
    "        # 双方向に対応， 隠れ層に self.directions を掛ける\n",
    "        self.func = nn.Linear(hidden_size * self.directions, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # バッチサイズを固定すると，一番最後の余りの分がおかしくなるので，動的に毎回決める！\n",
    "        self.batch_size = x.size()[0]\n",
    "        # 多層と双方向に対応， １次元目を 1 -> self.layers * self.directions\n",
    "        h0 = torch.zeros(self.layers * self.directions, self.batch_size, self.hidden_size, device=self.device) # ここを変更した\n",
    "        emb = self.emb(x)  # 入力サイズが異なるので統一する\n",
    "        x_rnn, h_last = self.rnn(emb, h0)  # RNN\n",
    "        # 双方向に対応\n",
    "        if self.bidirectional:\n",
    "            out = self.func(torch.cat([h_last[-2], h_last[-1]], dim=1))\n",
    "        else:\n",
    "            out = self.func(x_rnn[:, -1, :]) # 最後の層だけ取り出す # ここを変更した\n",
    "        # out = self.func(x_rnn[:, -1]) # 最後の層だけ取り出す\n",
    "        # out = self.func(h_last) #現在のhだけ取り出す\n",
    "        return out\n",
    "    \n",
    "    \n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(X_train_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = RNNmodel(vocab_size, padding_idx, emb_size, hidden_size, n_labels, batch_size, device, embedding_weight_matrix, bidirectional=True, layers=3)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227106d8",
   "metadata": {},
   "source": [
    "<h2 id=\"86-畳み込みニューラルネットワーク-cnn\">86. 畳み込みニューラルネットワーク (CNN)</h2>\n",
    "<p>ID番号で表現された単語列\\(\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)\\)がある．ただし，\\(T\\)は単語列の長さ，\\(x_t \\in \\mathbb{R}^{V}\\)は単語のID番号のone-hot表記である（\\(V\\)は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列\\(\\boldsymbol{x}\\)からカテゴリ\\(y\\)を予測するモデルを実装せよ．</p>\n",
    "<p>ただし，畳み込みニューラルネットワークの構成は以下の通りとする．</p>\n",
    "<ul>\n",
    "<li>単語埋め込みの次元数: \\(d_w\\)</li>\n",
    "<li>畳み込みのフィルターのサイズ: 3 トークン</li>\n",
    "<li>畳み込みのストライド: 1 トークン</li>\n",
    "<li>畳み込みのパディング: あり</li>\n",
    "<li>畳み込み演算後の各時刻のベクトルの次元数: \\(d_h\\)</li>\n",
    "<li>畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を\\(d_h\\)次元の隠れベクトルで表現</li>\n",
    "</ul>\n",
    "<p>すなわち，時刻\\(t\\)の特徴ベクトル\\(p_t \\in \\mathbb{R}^{d_h}\\)は次式で表される．</p>\n",
    "\n",
    "<p>\\[p_t = g(W^{(px)} [\\mathrm{emb}(x_{t-1}); \\mathrm{emb}(x_t); \\mathrm{emb}(x_{t+1})] + b^{(p)}))\\]</p>\n",
    "\n",
    "<p>ただし，\\(W^{(px)} \\in \\mathbb{R}^{d_h \\times 3d_w}, b^{(p)} \\in \\mathbb{R}^{d_h}\\)はCNNのパラメータ，\\(g\\)は活性化関数（例えば\\(\\tanh\\)やReLUなど），\\([a; b; c]\\)はベクトル\\(a, b, c\\)の連結である．なお，行列\\(W^{(px)}\\)の列数が\\(3d_w\\)になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．</p>\n",
    "<p>最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル\\(c \\in \\mathbb{R}^{d_h}\\)を求める．\\(c[i]\\)でベクトル\\(c\\)の\\(i\\)番目の次元の値を表すことにすると，最大値プーリングは次式で表される．</p>\n",
    "\n",
    "<p>\\[c[i] = \\max_{1 \\leq t \\leq T} p_t[i]]\\]</p>\n",
    "\n",
    "<p>最後に，入力文書の特徴ベクトル\\(c\\)に行列\\(W^{(yc)} \\in \\mathbb{R}^{L \\times d_h}\\)とバイアス項\\(b^{(y)} \\in \\mathbb{R}^{L}\\)による線形変換とソフトマックス関数を適用し，カテゴリ\\(y\\)を予測する．</p>\n",
    "\n",
    "<p>\\[y = {\\rm softmax}(W^{(yc)} c + b^{(y)}))\\]</p>\n",
    "\n",
    "<p>なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で\\(y\\)を計算するだけでよい．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f949ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, out_channels,  emb_size=300, kernel_heights=3, stride=1, n_labels=4, device=\"cpu\", emb_weight=None) -> None:\n",
    "        \"\"\"\n",
    "        stride: 動かす単位（小さいほど細かい）\n",
    "        kenel_height: 窓の大きさ\n",
    "        out_channels: \n",
    "        conv2d: convolution 層（次元を維持しつつ畳み込み）\n",
    "        max_pool1d: pooling層（最大値を取り，ダウンサンプリングする）\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding_idx, 0))\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.func = nn.Linear(out_channels, n_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x).unsqueeze(1)\n",
    "        conv = self.conv(emb)  # 畳み込み層\n",
    "        act = F.relu(conv.squeeze(3))  # 活性化関数\n",
    "        max_pool = F.max_pool1d(act, act.size()[2])  # pooling 層\n",
    "        out = self.func(self.drop(max_pool.squeeze(2)))  # 全結合層？\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "904435e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "予測値：tensor([[ 1.6340,  0.9635, -0.7566,  0.2272]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：0\n",
      "正解ラベル：2\n",
      "予測値：tensor([[ 0.8758,  0.1956, -0.2810,  0.8973]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：3\n",
      "正解ラベル：0\n",
      "予測値：tensor([[ 1.1807, -0.1821, -1.1514,  0.2550]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：0\n",
      "正解ラベル：2\n",
      "予測値：tensor([[ 0.6415,  0.3134, -0.9988, -0.2397]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：0\n",
      "正解ラベル：2\n",
      "予測値：tensor([[ 1.3651,  0.0785, -0.9187,  0.1324]], grad_fn=<AddmmBackward0>)\n",
      "予測ラベル：0\n",
      "正解ラベル：2\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "out_channels = 50 # ハイパラ？\n",
    "emb_size = 300  # ハイパラ\n",
    "kernel_height = 3\n",
    "stride = 1\n",
    "n_labels = 4  # ラベル数\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model = CNN(vocab_size, padding_idx, out_channels, emb_size, kernel_height, stride, n_labels, device)\n",
    "\n",
    "for i in range(5):\n",
    "    xi = X_train_tokenized[i]['inputs']\n",
    "    yi = X_train_tokenized[i]['labels']\n",
    "    pred_probs = model(xi.unsqueeze(0))\n",
    "    print(f\"予測値：{pred_probs}\")\n",
    "    print(f\"予測ラベル：{pred_probs.argmax()}\")\n",
    "    print(f\"正解ラベル：{yi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad7f34",
   "metadata": {},
   "source": [
    "<h2 id=\"87-確率的勾配降下法によるcnnの学習\">87. 確率的勾配降下法によるCNNの学習</h2>\n",
    "<p>確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "385b835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nyuton/Documents/100knock-2023/trainee_nyutonn/chapter09/wandb/run-20231109_183110-z077s5o4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nakano_test/chapter09_83/runs/z077s5o4' target=\"_blank\">hopeful-sky-9</a></strong> to <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nakano_test/chapter09_83' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nakano_test/chapter09_83/runs/z077s5o4' target=\"_blank\">https://wandb.ai/nakano_test/chapter09_83/runs/z077s5o4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 170/334 [16:28<15:53,  5.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./trained_param.npz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m total_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m---> 18\u001b[0m train(model, train_loader, valid_loader, output_path, total_epochs, device)\n",
      "Cell \u001b[0;32mIn[135], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, output_path, total_epochs, device, lr)\u001b[0m\n\u001b[1;32m     42\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_i\n\u001b[1;32m     44\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# 勾配の初期化\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# 勾配計算\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# パラメータ修正\u001b[39;00m\n\u001b[1;32m     47\u001b[0m train_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/envs/ISHate/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/envs/ISHate/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ちゃんとロスが下がった！！\n",
    "# cpuで回すと10時間くらい\n",
    "# GPUだと一瞬だった -> 10分弱くらい\n",
    "vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "emb_size = 300  # ハイパラ\n",
    "hidden_size = 50  # ハイパラ\n",
    "n_labels = 4  # ラベル数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(X_train_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(X_valid_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(X_test_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = CNN(vocab_size, padding_idx, out_channels, emb_size, kernel_height, stride, n_labels, device)\n",
    "output_path = \"./trained_param.npz\"\n",
    "total_epochs = 10\n",
    "train(model, train_loader, valid_loader, output_path, total_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "28d8055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch0: train_loss = 25.6953570352695, train_acc = 0.5040247098464994, valid_loss = 14.356131553649902, valid_acc = 0.6137724550898204\n",
      "INFO:root:epoch1: train_loss = 8.639520003883993, train_acc = 0.654530138524897, valid_loss = 3.2860076427459717, valid_acc = 0.6961077844311377\n",
      "INFO:root:epoch2: train_loss = 2.6602175040264693, train_acc = 0.7282852864095845, valid_loss = 2.087599515914917, valid_acc = 0.6729041916167665\n",
      "INFO:root:epoch3: train_loss = 1.2603243913422002, train_acc = 0.7751778360164733, valid_loss = 1.8979467153549194, valid_acc = 0.7245508982035929\n",
      "INFO:root:epoch4: train_loss = 0.8703111402776883, train_acc = 0.8084050917259453, valid_loss = 1.532151222229004, valid_acc = 0.7372754491017964\n",
      "INFO:root:epoch5: train_loss = 0.6320607877768456, train_acc = 0.827311868214152, valid_loss = 1.5626899003982544, valid_acc = 0.7485029940119761\n",
      "INFO:root:epoch6: train_loss = 0.48046755281917525, train_acc = 0.8442530887308124, valid_loss = 1.5024590492248535, valid_acc = 0.750748502994012\n",
      "INFO:root:epoch7: train_loss = 0.42544449786040184, train_acc = 0.852864095844253, valid_loss = 1.5765423774719238, valid_acc = 0.7402694610778443\n",
      "INFO:root:epoch8: train_loss = 0.3851057806127931, train_acc = 0.8616622987645077, valid_loss = 1.6161127090454102, valid_acc = 0.7559880239520959\n",
      "INFO:root:epoch9: train_loss = 0.3654360795224128, train_acc = 0.866154998128042, valid_loss = 1.6529145240783691, valid_acc = 0.7672155688622755\n"
     ]
    }
   ],
   "source": [
    "!cat src/q87.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b734e94",
   "metadata": {},
   "source": [
    "<h2 id=\"88-パラメータチューニング\">88. パラメータチューニング</h2>\n",
    "<p>問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da3e21fc",
   "metadata": {},
   "source": [
    "一番性能が良かったCNNを採用する\n",
    "\n",
    "optuna でパラメータを自動最適化する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "529b1ea9",
   "metadata": {},
   "source": [
    "あまりにも時間がかかるので，src/q88.py で実行した\n",
    "\n",
    "ここに関しては，GPUのほうがCPUよりも3倍くらい早かった気がする\n",
    "\n",
    "running.log に実行履歴が残っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2802d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータチューニング用に引数を変更\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, out_channels,  emb_size=300, kernel_heights=3, stride=1, n_labels=4, device=\"cpu\", emb_weight=None, active_func='relu', dropout=0.3) -> None:\n",
    "        \"\"\"\n",
    "        stride: 動かす単位（小さいほど細かい）\n",
    "        kenel_height: 窓の大きさ\n",
    "        out_channels: \n",
    "        conv2d: convolution 層（次元を維持しつつ畳み込み）\n",
    "        max_pool1d: pooling層（最大値を取り，ダウンサンプリングする）\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        # 入力ベクトルの大きさが異なるので，emb層で形をそろえる\n",
    "        if emb_weight is None:\n",
    "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.emb = nn.Embedding.from_pretrained(emb_weight, padding_idx=padding_idx)\n",
    "        self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding_idx, 0))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.func = nn.Linear(out_channels, n_labels)\n",
    "        self.active_func = active_func # 活性化関数をパラメータにする\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x).unsqueeze(1)\n",
    "        conv = self.conv(emb)  # 畳み込み層\n",
    "\n",
    "        # 活性化関数の最適化を行う\n",
    "        if self.active_func == 'relu':\n",
    "            act = F.relu(conv.squeeze(3))\n",
    "        elif self.active_func == 'tanh':\n",
    "            act = torch.tanh(conv.squeeze(3))\n",
    "        elif self.active_func == 'mish':\n",
    "            act = F.mish(conv.squeeze(3))\n",
    "        else:\n",
    "            act = F.relu(conv.squeeze(3))\n",
    "\n",
    "        max_pool = F.max_pool1d(act, act.size()[2])  # pooling 層\n",
    "        out = self.func(self.drop(max_pool.squeeze(2)))  # 全結合層？\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "488e510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stoppingを差し込む\n",
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, path='checkpoint_model.pth'):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        self.path = path             #ベストモデル格納path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epoch目の処理\n",
    "            self.best_score = score   #1Epoch目はそのままベストスコアとして記録する\n",
    "            self.checkpoint(val_loss, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score < self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する \n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.best_score = score  #ベストスコアを上書き\n",
    "            self.checkpoint(val_loss, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.val_loss_min = val_loss  #その時のlossを記録する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3cc59218",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('work/train-id.txt', 'wb')\n",
    "pickle.dump(train_tokenized_id, f)\n",
    "f = open('work/valid-id.txt', 'wb')\n",
    "pickle.dump(valid_tokenized_id, f)\n",
    "f = open('work/test-id.txt', 'wb')\n",
    "pickle.dump(test_tokenized_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46d0026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータチューニング用に引数と返り値を変更\n",
    "# early stopping の機構を追加\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader # データローダ使ってみる\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 学習率を引数に追加\n",
    "def train(model, train_loader, valid_loader, output_path, total_epochs, device, lr=0.01, op='sgd'):\n",
    "    earlystopping = EarlyStopping(patience=3, verbose=True)\n",
    "    \n",
    "    # 最適化手法を変更\n",
    "    if op == 'sgd':  \n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    elif op == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif op == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    # 指定した epoch 数だけ学習\n",
    "    for epoch in range(total_epochs):\n",
    "        train_total_loss = 0.\n",
    "        train_acc_cnt = 0\n",
    "\n",
    "        # パラメータ更新\n",
    "        model.train()\n",
    "        for data in tqdm(train_loader):\n",
    "            x = data['inputs']\n",
    "            x = x.to(device)\n",
    "            y = data['labels']\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # バッチの中で損失計算\n",
    "            train_loss = 0.\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                loss_i = loss_func(yi_pred, yi)\n",
    "                train_loss += loss_i\n",
    "            \n",
    "            optimizer.zero_grad()  # 勾配の初期化\n",
    "            train_loss.backward()  # 勾配計算\n",
    "            optimizer.step()  # パラメータ修正\n",
    "            train_total_loss += train_loss.item()\n",
    "\n",
    "            # バッチの中で正解率の計算\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                if yi.item() == yi_pred.argmax():\n",
    "                    train_acc_cnt += 1\n",
    "        \n",
    "        #★毎エポックearlystoppingの判定をさせる★\n",
    "        train_ave_loss = train_total_loss / len(X_train_tokenized)\n",
    "        \n",
    "        earlystopping(train_ave_loss, model) #callメソッド呼び出し\n",
    "        if earlystopping.early_stop: #ストップフラグがTrueの場合、breakでforループを抜ける\n",
    "            print(f\"epoch{epoch}: train_loss = {train_ave_loss}\")\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "                \n",
    "        # train のロスと正解率の計算\n",
    "        model.eval()\n",
    "        train_acc = measure_acc(model, X_train_tokenized[:]['inputs'], X_train_tokenized[:]['labels'], device)\n",
    "\n",
    "\n",
    "        # valid のロスと正解率の計算\n",
    "        model.eval()\n",
    "        valid_acc_cnt = 0\n",
    "        valid_total_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(valid_loader):\n",
    "                x = data['inputs']\n",
    "                x = x.to(device)\n",
    "                y = data['labels']\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                # バッチの中で損失計算\n",
    "                valid_loss = 0.\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    # print(yi)\n",
    "                    # print(yi_pred)\n",
    "                    loss_i = loss_func(yi_pred, yi)\n",
    "                    valid_loss += loss_i\n",
    "\n",
    "                optimizer.zero_grad()  # 勾配の初期化\n",
    "                # valid_loss.backward()  # 勾配計算\n",
    "                # optimizer.step()  # パラメータ修正\n",
    "                valid_total_loss += valid_loss\n",
    "\n",
    "                # バッチの中で正解率の計算\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    if yi.item() == yi_pred.argmax():\n",
    "                        valid_acc_cnt += 1\n",
    "\n",
    "            # valid のロスと正解率の計算\n",
    "            valid_acc = measure_acc(model, X_valid[:]['inputs'], X_valid[:]['labels'], device)\n",
    "\n",
    "        # 表示\n",
    "        train_ave_loss = train_total_loss / len(X_train_tokenized)\n",
    "        # train_acc = train_acc_cnt / len(X_train)\n",
    "        valid_ave_loss = valid_total_loss / len(X_valid_tokenized)\n",
    "        # valid_acc = valid_acc_cnt / len(X_valid)\n",
    "        print(f\"epoch{epoch}: train_loss = {train_ave_loss}, train_acc = {train_acc}, valid_loss = {valid_ave_loss}, valid_acc = {valid_acc}\")\n",
    "\n",
    "    # パラメータを保存\n",
    "    torch.save(model.state_dict(), output_path)\n",
    "    \n",
    "    # valid loss を返り値とする\n",
    "    return valid_ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bb3aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna でパラメータの自動最適化\n",
    "from typing import Any\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # 固定のもの\n",
    "    vocab_size = len(vocab) + 1  # padding の分 +1 する\n",
    "    padding_idx = len(vocab)  # 空き単語を埋めるときは最大値を入れる\n",
    "    n_labels = 4  # ラベル数\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader = DataLoader(X_train_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(X_valid_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(X_test_tokenized, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    output_path = \"./trained_param.npz\"\n",
    "    total_epochs = 10\n",
    "\n",
    "    # ハイパラを変更させる\n",
    "    out_channels = trial.suggest_categorical('out_channels', [16, 32, 64, 128])  # これだけよくわかっていない\n",
    "    emb_size = trial.suggest_categorical('emb_size', [50, 100, 200, 300])  # 特徴ベクトルの次元数\n",
    "    kernel_height = trial.suggest_int('kernel_height', 1, 5, step=1)  # 窓の大きさ\n",
    "    stride = trial.suggest_int('stride', 1, 2, step=1)  # 窓を動かす単位\n",
    "    active_func = trial.suggest_categorical('active_func', ['relu', 'tanh', 'mish'])  # 活性化関数\n",
    "    lr = trial.suggest_float('lr', 1e-3, 1e-2, log=True)  # 学習率\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)  # ドロップアウト\n",
    "    op = trial.suggest_categorical('optimizer', ['rmsprop', 'adam', 'sgd'])  # 最適化手法 \n",
    "\n",
    "    print(f\"device: {device}\")\n",
    "    model = CNN(vocab_size, padding_idx, out_channels, emb_size, kernel_height, stride, n_labels, device, active_func=active_func, dropout=dropout)\n",
    "    valid_loss = train(model, train_loader, valid_loader, output_path, total_epochs, device, lr, op)\n",
    "\n",
    "    # 訓練の最後で得られた valid_loss でパラメータチューニングを行う\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6f6f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 21:26:36,072] A new study created in memory with name: no-name-640055c7-92dc-42e9-930c-ccce990283a0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/334 [00:09<27:00,  4.88s/it]\n",
      "[W 2023-11-08 21:26:45,886] Trial 0 failed with parameters: {'out_channels': 16, 'emb_size': 200, 'kernel_height': 5, 'stride': 2, 'active_func': 'relu', 'lr': 0.004388631453821411, 'dropout': 0.4163492420037371, 'optimizer': 'sgd'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nyuton/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/bw/n738lb8d773382cjs4qszbbh0000gn/T/ipykernel_2658/4258090099.py\", line 29, in objective\n",
      "    valid_loss = train(model, train_loader, valid_loader, output_path, total_epochs, device, lr, op)\n",
      "  File \"/var/folders/bw/n738lb8d773382cjs4qszbbh0000gn/T/ipykernel_2658/4177580950.py\", line 50, in train\n",
      "    train_loss.backward()  # 勾配計算\n",
      "  File \"/Users/nyuton/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/Users/nyuton/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-08 21:26:45,890] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 流石に時間がかかりすぎるのでサーバで実行 (100時間くらいかかる？)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[68], line 29\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdevice: \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m model \u001b[39m=\u001b[39m CNN(vocab_size, padding_idx, out_channels, emb_size, kernel_height, stride, n_labels, device, active_func\u001b[39m=\u001b[39mactive_func, dropout\u001b[39m=\u001b[39mdropout)\n\u001b[0;32m---> 29\u001b[0m valid_loss \u001b[39m=\u001b[39m train(model, train_loader, valid_loader, output_path, total_epochs, device, lr, op)\n\u001b[1;32m     31\u001b[0m \u001b[39m# 訓練の最後で得られた valid_loss でパラメータチューニングを行う\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m valid_loss\n",
      "Cell \u001b[0;32mIn[67], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, output_path, total_epochs, device, lr, op)\u001b[0m\n\u001b[1;32m     47\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_i\n\u001b[1;32m     49\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# 勾配の初期化\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# 勾配計算\u001b[39;00m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# パラメータ修正\u001b[39;00m\n\u001b[1;32m     52\u001b[0m train_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 流石に時間がかかりすぎるのでサーバで実行 (100時間くらいかかる？)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d2c2ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m最高精度のACC：\u001b[39m\u001b[39m{\u001b[39;00mstudy\u001b[39m.\u001b[39mbest_value\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m最高精度のパラメータ\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m pprint(study\u001b[39m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/study.py:128\u001b[0m, in \u001b[0;36mStudy.best_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbest_value\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    118\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the best objective value in the study.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     best_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_trial\u001b[39m.\u001b[39mvalue\n\u001b[1;32m    129\u001b[0m     \u001b[39massert\u001b[39;00m best_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m best_value\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/study/study.py:157\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_multi_objective():\n\u001b[1;32m    152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_storage\u001b[39m.\u001b[39;49mget_best_trial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_study_id))\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/optuna/storages/_in_memory.py:234\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[0;34m(self, study_id)\u001b[0m\n\u001b[1;32m    231\u001b[0m best_trial_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_studies[study_id]\u001b[39m.\u001b[39mbest_trial_id\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m best_trial_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo trials are completed yet.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_studies[study_id]\u001b[39m.\u001b[39mdirections) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "print(f\"最高精度のACC：{study.best_value}\")\n",
    "print('最高精度のパラメータ')\n",
    "pprint(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f83601e3",
   "metadata": {},
   "source": [
    "<h2 id=\"89-事前学習済み言語モデルからの転移学習\">89. 事前学習済み言語モデルからの転移学習</h2>\n",
    "<p>事前学習済み言語モデル（例えば<a href=\"https://github.com/google-research/bert\">BERT</a>など）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a94ea9f4",
   "metadata": {},
   "source": [
    "こちらもかなり時間がかかるので，src/q89.pyを用いてサーバ上で回した\n",
    "\n",
    "出力ログを q89.running.log ファイルに残した"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db01e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class BERTmodel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "\n",
    "    def forward(self, encoding):\n",
    "        outputs = self.bert_sc(**encoding)\n",
    "        return outputs\n",
    "    \n",
    "class CreateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'inputs': self.X[index],\n",
    "            'labels': self.y[index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "771d0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT用\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader # データローダ使ってみる\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def bert_train(model, train_loader, valid_loader, output_path, total_epochs, device, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # BERTモデルのエンコード用\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "    # 指定した epoch 数だけ学習\n",
    "    for epoch in range(total_epochs):\n",
    "        train_total_loss = 0.\n",
    "        train_acc_cnt = 0\n",
    "\n",
    "        # パラメータ更新\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader):\n",
    "            x_texts = batch['inputs']\n",
    "            x_encordings = tokenizer(\n",
    "                list(x_texts), \n",
    "                max_length=128, \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                return_tensors='pt', \n",
    "                return_attention_mask=True, \n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "            x_encordings = x_encordings.to(device)\n",
    "            y = batch['labels']\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x_encordings).logits\n",
    "\n",
    "            # バッチの中で損失計算\n",
    "            train_loss = loss_func(y_pred, y)\n",
    "\n",
    "            # train_loss = 0.\n",
    "            # for yi, yi_pred in zip(y, y_pred):\n",
    "            #     loss_i = loss_func(yi_pred, yi)\n",
    "            #     train_loss += loss_i\n",
    "            \n",
    "            optimizer.zero_grad() # 勾配の初期化\n",
    "            train_loss.backward()  # 勾配計算\n",
    "            optimizer.step()  # パラメータ修正\n",
    "            train_total_loss += train_loss.item()\n",
    "\n",
    "            # バッチの中で正解率の計算 # ここを修正\n",
    "            for yi, yi_pred in zip(y, y_pred):\n",
    "                if yi.item() == yi_pred.argmax():\n",
    "                    train_acc_cnt += 1\n",
    "                \n",
    "        # train のロスと正解率の計算\n",
    "        model.eval()\n",
    "        # train_acc = measure_acc(model, X_train[:]['inputs'], X_train[:]['labels'], device)\n",
    "\n",
    "\n",
    "        # valid のロスと正解率の計算\n",
    "        model.eval()\n",
    "        valid_acc_cnt = 0\n",
    "        valid_total_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_loader):\n",
    "                x_texts = batch['inputs']\n",
    "                x_encordings = tokenizer(\n",
    "                    list(x_texts), \n",
    "                    max_length=128, \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    return_tensors='pt', \n",
    "                    return_attention_mask=True, \n",
    "                    return_token_type_ids=True\n",
    "                )\n",
    "                x_encordings = x_encordings.to(device)\n",
    "                y = batch['labels']\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x_encordings).logits\n",
    "\n",
    "                # バッチの中で損失計算\n",
    "                valid_loss = loss_func(y_pred, y)\n",
    "                # valid_loss = 0.\n",
    "                # for yi, yi_pred in zip(y, y_pred):\n",
    "                #     # print(yi)\n",
    "                #     # print(yi_pred)\n",
    "                #     loss_i = loss_func(yi_pred, yi)\n",
    "                #     valid_loss += loss_i\n",
    "\n",
    "                optimizer.zero_grad()  # 勾配の初期化\n",
    "                # valid_loss.backward()  # 勾配計算\n",
    "                # optimizer.step()  # パラメータ修正\n",
    "                valid_total_loss += valid_loss\n",
    "\n",
    "                # バッチの中で正解率の計算  # ここを修正\n",
    "                for yi, yi_pred in zip(y, y_pred):\n",
    "                    if yi.item() == yi_pred.argmax():\n",
    "                        valid_acc_cnt += 1\n",
    "\n",
    "            # valid のロスと正解率の計算\n",
    "            # valid_acc = measure_acc(model, X_valid[:]['inputs'], X_valid[:]['labels'], device)\n",
    "\n",
    "        # 表示\n",
    "        train_ave_loss = train_total_loss / len(X_train_tokenized)\n",
    "        train_acc = train_acc_cnt / len(X_train_tokenized)\n",
    "        valid_ave_loss = valid_total_loss / len(X_valid_tokenized)\n",
    "        valid_acc = valid_acc_cnt / len(X_valid_tokenized)\n",
    "        print(f\"epoch{epoch}: train_loss = {train_ave_loss}, train_acc = {train_acc}, valid_loss = {valid_ave_loss}, valid_acc = {valid_acc}\")\n",
    "\n",
    "    # パラメータを保存\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5dc2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTモデルに入れるためのデータセットの作成\n",
    "category_dict = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "batch_size = 32\n",
    "\n",
    "y_train = torch.tensor(train_data['CATEGORY'].map(category_dict).values, dtype=torch.int64)\n",
    "y_valid = torch.tensor(valid_data['CATEGORY'].map(category_dict).values, dtype=torch.int64)\n",
    "y_test = torch.tensor(test_data['CATEGORY'].map(category_dict).values, dtype=torch.int64)\n",
    "\n",
    "train_set = CreateDataset(train_data['TITLE'].to_list(), y_train)\n",
    "valid_set = CreateDataset(valid_data['TITLE'].to_list(), y_valid)\n",
    "test_set = CreateDataset(test_data['TITLE'].to_list(), y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b853a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 42/42 [04:31<00:00,  6.46s/it]\n",
      "100%|██████████| 42/42 [01:27<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: train_loss = 0.0036510499323275775, train_acc = 0.08152377386746537, valid_loss = 0.025179985910654068, valid_acc = 0.7155688622754491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [04:49<00:00,  6.89s/it]\n",
      "100%|██████████| 42/42 [01:30<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: train_loss = 0.0021149115124386108, train_acc = 0.1012729314863347, valid_loss = 0.011974153108894825, valid_acc = 0.8720059880239521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [05:07<00:00,  7.33s/it]\n",
      "100%|██████████| 42/42 [01:38<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: train_loss = 0.0016038271140701892, train_acc = 0.10922875327592661, valid_loss = 0.011902498081326485, valid_acc = 0.8787425149700598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/42 [00:22<07:37, 11.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[1;32m      4\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m bert_train(model, test_loader, valid_loader, \u001b[39m'\u001b[39;49m\u001b[39mbert_param.npz\u001b[39;49m\u001b[39m'\u001b[39;49m, total_epochs, device, lr)\n",
      "Cell \u001b[0;32mIn[47], line 52\u001b[0m, in \u001b[0;36mbert_train\u001b[0;34m(model, train_loader, valid_loader, output_path, total_epochs, device, lr)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# train_loss = 0.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# for yi, yi_pred in zip(y, y_pred):\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m#     loss_i = loss_func(yi_pred, yi)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m#     train_loss += loss_i\u001b[39;00m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# 勾配の初期化\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# 勾配計算\u001b[39;00m\n\u001b[1;32m     53\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# パラメータ修正\u001b[39;00m\n\u001b[1;32m     54\u001b[0m train_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BERTmodel()\n",
    "total_epochs = 10\n",
    "lr = 0.01\n",
    "device = 'cpu'\n",
    "\n",
    "bert_train(model, train_loader, valid_loader, 'bert_param.npz', total_epochs, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8323ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Start running...\n",
      "INFO:root:Start running...\n",
      "INFO:root:epoch0: train_loss = 0.013288951948220946, train_acc = 0.8549232497192063, valid_loss = 0.007369755767285824, valid_acc = 0.9184131736526946\n",
      "INFO:root:epoch1: train_loss = 0.007372740262478467, train_acc = 0.9213777611381505, valid_loss = 0.006601303815841675, valid_acc = 0.9236526946107785\n",
      "INFO:root:epoch2: train_loss = 0.005779119322473237, train_acc = 0.9395357543991014, valid_loss = 0.0064792754128575325, valid_acc = 0.9206586826347305\n",
      "INFO:root:epoch3: train_loss = 0.004371707276756712, train_acc = 0.953575439910146, valid_loss = 0.005881978198885918, valid_acc = 0.937125748502994\n",
      "INFO:root:epoch4: train_loss = 0.0037143926281943632, train_acc = 0.9605952826656683, valid_loss = 0.007777113933116198, valid_acc = 0.9221556886227545\n",
      "INFO:root:epoch5: train_loss = 0.003243646150737658, train_acc = 0.9663983526769, valid_loss = 0.005479299463331699, valid_acc = 0.9446107784431138\n",
      "INFO:root:epoch6: train_loss = 0.0025273937426145894, train_acc = 0.9734181954324224, valid_loss = 0.006565966177731752, valid_acc = 0.9326347305389222\n",
      "INFO:root:epoch7: train_loss = 0.0021991192821323668, train_acc = 0.9779108947959566, valid_loss = 0.007078968919813633, valid_acc = 0.9393712574850299\n",
      "INFO:root:epoch8: train_loss = 0.0018354987687740206, train_acc = 0.9806252339947585, valid_loss = 0.00703129218891263, valid_acc = 0.9393712574850299\n",
      "INFO:root:epoch9: train_loss = 0.0014023003225718273, train_acc = 0.9864283040059902, valid_loss = 0.006282070651650429, valid_acc = 0.9446107784431138\n"
     ]
    }
   ],
   "source": [
    "# 結果\n",
    "!cat src/q89.running.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
