{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f913d710",
   "metadata": {},
   "source": [
    "<h2 id=\"40-係り受け解析結果の読み込み形態素\">40. 係り受け解析結果の読み込み（形態素）</h2>\n",
    "<p>形態素を表すクラス<code class=\"language-plaintext highlighter-rouge\">Morph</code>を実装せよ．このクラスは表層形（<code class=\"language-plaintext highlighter-rouge\">surface</code>），基本形（<code class=\"language-plaintext highlighter-rouge\">base</code>），品詞（<code class=\"language-plaintext highlighter-rouge\">pos</code>），品詞細分類1（<code class=\"language-plaintext highlighter-rouge\">pos1</code>）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文を<code class=\"language-plaintext highlighter-rouge\">Morph</code>オブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8d5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnZip 6.00 of 20 April 2009, by Info-ZIP.  Maintained by C. Spieler.  Send\n",
      "bug reports using http://www.info-zip.org/zip-bug.html; see README for details.\n",
      "\n",
      "Usage: unzip [-Z] [-opts[modifiers]] file[.zip] [list] [-x xlist] [-d exdir]\n",
      "  Default action is to extract files in list, except those in xlist, to exdir;\n",
      "  file[.zip] may be a wildcard.  -Z => ZipInfo mode (\"unzip -Z\" for usage).\n",
      "\n",
      "  -p  extract files to pipe, no messages     -l  list files (short format)\n",
      "  -f  freshen existing files, create none    -t  test compressed archive data\n",
      "  -u  update files, create if necessary      -z  display archive comment only\n",
      "  -v  list verbosely/show version info       -T  timestamp archive to latest\n",
      "  -x  exclude files that follow (in xlist)   -d  extract files into exdir\n",
      "modifiers:\n",
      "  -n  never overwrite existing files         -q  quiet mode (-qq => quieter)\n",
      "  -o  overwrite files WITHOUT prompting      -a  auto-convert any text files\n",
      "  -j  junk paths (do not make directories)   -aa treat ALL files as text\n",
      "  -C  match filenames case-insensitively     -L  make (some) names lowercase\n",
      "  -X  restore UID/GID info                   -V  retain VMS version numbers\n",
      "  -K  keep setuid/setgid/tacky permissions   -M  pipe through \"more\" pager\n",
      "See \"unzip -hh\" or unzip.txt for more help.  Examples:\n",
      "  unzip data1 -x joe   => extract all files except joe from zipfile data1.zip\n",
      "  unzip -p foo | more  => send contents of foo.zip via pipe into program more\n",
      "  unzip -fo foo ReadMe => quietly replace existing ReadMe if archive file newer\n"
     ]
    }
   ],
   "source": [
    "!unzip -y data/ai.ja.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86bf47b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\n",
      "\n",
      "人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。\n",
      "\n",
      "『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている。\n",
      "人間の知的能力をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステム。応用例は自然言語処理（機械翻訳・かな漢字変換・構文解析等）、専門家の推論・判断を模倣するエキスパートシステム、画像データを解析して特定のパターンを検出・抽出したりする画像認識等がある。1956年にダートマス会議でジョン・マッカーシーにより命名された。現在では、記号処理を用いた知能の記述を主体とする情報処理や研究でのアプローチという意味あいでも使われている。家庭用電気機械器具の制御システムやゲームソフトの思考ルーチンもこう呼ばれることもある。\n",
      "\n",
      "プログラミング言語 による「」というカウンセラーを模倣したプログラム（人工無脳）がしばしば引き合いに出されるが、計算機に人間の専門家の役割をさせようという「エキスパートシステム」と呼ばれる研究・情報処理システムの実現は、人間が暗黙に持つ常識の記述が問題となり、実用への利用が困難視されている。人工的な知能の実現へのアプローチとしては、「ファジィ理論」や「ニューラルネットワーク」などのようなアプローチも知られているが、従来の人工知能である (Good Old Fashioned AI) との差は記述の記号的明示性にある。その後「サポートベクターマシン」が注目を集めた。また、自らの経験を元に学習を行う強化学習という手法もある。「この宇宙において、知性とは最も強力な形質である（レイ・カーツワイル）」という言葉通り、知性を機械的に表現し実装するということは極めて重要な作業である。\n",
      "\n",
      "2006年のディープラーニング（深層学習）の登場と2010年代以降のビッグデータの登場により、一過性の流行を超えて社会に浸透して行った。2016年から2017年にかけて、ディープラーニングを導入したAIが完全情報ゲームである囲碁などのトップ棋士、さらに不完全情報ゲームであるポーカーの世界トップクラスのプレイヤーも破り、麻雀では「Microsoft Suphx (Super Phoenix)」がAIとして初めて十段に到達するなど、時代の最先端技術となった。\n"
     ]
    }
   ],
   "source": [
    "# 最初の10行表示\n",
    "!cat data/ai.ja.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a55d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     155 data/ai.ja.txt\n"
     ]
    }
   ],
   "source": [
    "# 行数\n",
    "!wc -l data/ai.ja.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "194d014a",
   "metadata": {},
   "source": [
    "形態素解析 CaboCha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10cf891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ゼルダの-D          \n",
      "    伝説の-D        \n",
      "      新作が-------D\n",
      "      楽しみで-----D\n",
      "          今日も-D |\n",
      "            明日も-D\n",
      "            眠れない\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "!echo ゼルダの伝説の新作が楽しみで今日も明日も眠れない | cabocha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88081590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 1D 0/1 0.982310\n",
      "ゼルダ\t名詞,一般,*,*,*,*,*\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "* 1 2D 0/1 1.901208\n",
      "伝説\t名詞,一般,*,*,*,*,伝説,デンセツ,デンセツ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "* 2 6D 0/1 -1.480740\n",
      "新作\t名詞,サ変接続,*,*,*,*,新作,シンサク,シンサク\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "* 3 6D 0/1 -1.480740\n",
      "楽しみ\t名詞,一般,*,*,*,*,楽しみ,タノシミ,タノシミ\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "* 4 5D 0/1 0.789261\n",
      "今日\t名詞,副詞可能,*,*,*,*,今日,キョウ,キョー\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "* 5 6D 0/1 -1.480740\n",
      "明日\t名詞,副詞可能,*,*,*,*,明日,アシタ,アシタ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "* 6 -1D 0/1 0.000000\n",
      "眠れ\t動詞,自立,*,*,一段,未然形,眠れる,ネムレ,ネムレ\n",
      "ない\t助動詞,*,*,*,特殊・ナイ,基本形,ない,ナイ,ナイ\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "# f1オプションで処理しやすい形に整えてくれる\n",
    "!echo ゼルダの伝説の新作が楽しみで今日も明日も眠れない | cabocha -f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a51fe662",
   "metadata": {},
   "source": [
    "CaboChaは文ごとに分かれていることが前提となっているので、ai.ja.txtを文毎に分けて、ai.ja.sentence.txtに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd42645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\n",
      "人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語\n",
      "「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる\n",
      "『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている\n",
      "人間の知的能力をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステム\n",
      "応用例は自然言語処理（機械翻訳・かな漢字変換・構文解析等）、専門家の推論・判断を模倣するエキスパートシステム、画像データを解析して特定のパターンを検出・抽出したりする画像認識等がある\n",
      "1956年にダートマス会議でジョン・マッカーシーにより命名された\n",
      "現在では、記号処理を用いた知能の記述を主体とする情報処理や研究でのアプローチという意味あいでも使われている\n",
      "家庭用電気機械器具の制御システムやゲームソフトの思考ルーチンもこう呼ばれることもある\n",
      "プログラミング言語 による「」というカウンセラーを模倣したプログラム（人工無脳）がしばしば引き合いに出されるが、計算機に人間の専門家の役割をさせようという「エキスパートシステム」と呼ばれる研究・情報処理システムの実現は、人間が暗黙に持つ常識の記述が問題となり、実用への利用が困難視されている\n"
     ]
    }
   ],
   "source": [
    "with (open('data/ai.ja.txt') as fr,\n",
    "      open('work/ai.ja.sentence.txt', 'w') as fw):\n",
    "    for line in fr:\n",
    "        if line == '\\n': continue\n",
    "        line = line.rstrip()\n",
    "        # １行に複数文ある場合もあるので句点で分ける\n",
    "        lines = line.split('。')\n",
    "        # 文末に改行をくっつける\n",
    "        add_sentence = '\\n'.join(lines)\n",
    "        fw.write(add_sentence)\n",
    "        # 読点で終わっていない文にも改行を付ける\n",
    "        if add_sentence[-1] != '\\n':\n",
    "            fw.write('\\n')\n",
    "\n",
    "!cat work/ai.ja.sentence.txt | head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f36591",
   "metadata": {},
   "source": [
    "文毎に分けたai.ja.sencence.txtから、CaboChaで形態素解析をしてai.ja.txt.cabochaに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6e07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat work/ai.ja.sentence.txt | cabocha -f1 > work/ai.ja.txt.cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abffc48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MorphDeco(surface='人工', base='人工', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='知能', base='知能', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='（', base='（', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='じん', base='じん', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='こうち', base='こうち', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='のう', base='のう', pos='助詞', pos1='終助詞'),\n",
      " MorphDeco(surface='、', base='、', pos='記号', pos1='読点'),\n",
      " MorphDeco(surface='、', base='、', pos='記号', pos1='読点'),\n",
      " MorphDeco(surface='AI', base='*', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='〈', base='〈', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='エーアイ', base='*', pos='名詞', pos1='固有名詞'),\n",
      " MorphDeco(surface='〉', base='〉', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='）', base='）', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='と', base='と', pos='助詞', pos1='格助詞'),\n",
      " MorphDeco(surface='は', base='は', pos='助詞', pos1='係助詞'),\n",
      " MorphDeco(surface='、', base='、', pos='記号', pos1='読点'),\n",
      " MorphDeco(surface='「', base='「', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='『', base='『', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='計算', base='計算', pos='名詞', pos1='サ変接続'),\n",
      " MorphDeco(surface='（', base='（', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='）', base='）', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='』', base='』', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='という', base='という', pos='助詞', pos1='格助詞'),\n",
      " MorphDeco(surface='概念', base='概念', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='と', base='と', pos='助詞', pos1='並立助詞'),\n",
      " MorphDeco(surface='『', base='『', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='コンピュータ', base='コンピュータ', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='（', base='（', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='）', base='）', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='』', base='』', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='という', base='という', pos='助詞', pos1='格助詞'),\n",
      " MorphDeco(surface='道具', base='道具', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='を', base='を', pos='助詞', pos1='格助詞'),\n",
      " MorphDeco(surface='用い', base='用いる', pos='動詞', pos1='自立'),\n",
      " MorphDeco(surface='て', base='て', pos='助詞', pos1='接続助詞'),\n",
      " MorphDeco(surface='『', base='『', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='知能', base='知能', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='』', base='』', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='を', base='を', pos='助詞', pos1='格助詞'),\n",
      " MorphDeco(surface='研究', base='研究', pos='名詞', pos1='サ変接続'),\n",
      " MorphDeco(surface='する', base='する', pos='動詞', pos1='自立'),\n",
      " MorphDeco(surface='計算', base='計算', pos='名詞', pos1='サ変接続'),\n",
      " MorphDeco(surface='機', base='機', pos='名詞', pos1='接尾'),\n",
      " MorphDeco(surface='科学', base='科学', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='（', base='（', pos='記号', pos1='括弧開'),\n",
      " MorphDeco(surface='）', base='）', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='の', base='の', pos='助詞', pos1='連体化'),\n",
      " MorphDeco(surface='一', base='一', pos='名詞', pos1='数'),\n",
      " MorphDeco(surface='分野', base='分野', pos='名詞', pos1='一般'),\n",
      " MorphDeco(surface='」', base='」', pos='記号', pos1='括弧閉'),\n",
      " MorphDeco(surface='を', base='を', pos='助詞', pos1='格助詞'),\n",
      " MorphDeco(surface='指す', base='指す', pos='動詞', pos1='自立'),\n",
      " MorphDeco(surface='語', base='語', pos='名詞', pos1='一般')]\n"
     ]
    }
   ],
   "source": [
    "# デコレータを使うバージョン\n",
    "from pydantic.dataclasses import dataclass\n",
    "import re\n",
    "repattern = re.compile('[\\t,]')\n",
    "# クラスを実装\n",
    "@dataclass\n",
    "class MorphDeco:\n",
    "    # メンバ変数を定義\n",
    "    surface: str\n",
    "    base: str\n",
    "    pos: str\n",
    "    pos1: str\n",
    "\n",
    "    # 1文を受け取ってsurfase, base, pos, pos1を取り出してmorphインスタンスにする関数\n",
    "    @classmethod\n",
    "    def sentence2morph(cls, sentence):\n",
    "        surfase, pos, pos1, *rest = repattern.split(sentence)\n",
    "        base = rest[4]\n",
    "        morph = cls(surfase, base, pos, pos1)\n",
    "        return morph\n",
    "\n",
    "morph_onesentence_list = [] # 1文毎のmorphのリスト\n",
    "is_one_sentence = False # 文が続いているかどうか\n",
    "with open('work/ai.ja.txt.cabocha') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        # *から始まる行はスルー\n",
    "        if line[0] == '*': continue\n",
    "        # EOSが来たら新しい文が次に来るはず\n",
    "        if line == 'EOS':\n",
    "            is_one_sentence = False\n",
    "            continue\n",
    "        morph = MorphDeco.sentence2morph(line)\n",
    "        # 文中なら末尾のリストに突っ込む\n",
    "        if is_one_sentence:\n",
    "            morph_onesentence_list[-1].append(morph)\n",
    "        # 新しい文なら新しいリストを作って文中に入る\n",
    "        else:\n",
    "            morph_onesentence_list.append([morph])\n",
    "            is_one_sentence = True\n",
    "# 表示\n",
    "from pprint import pprint\n",
    "pprint(morph_onesentence_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f55de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 別解\n",
    "# デコレータを使わないバージョン\n",
    "import re\n",
    "repattern = re.compile('[\\t,]')\n",
    "# クラスを実装\n",
    "class Morph:\n",
    "    # メンバ変数を定義\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    # インスタンスの出力を可視化\n",
    "    def __str__(self) -> str:\n",
    "        return f'Morph(surface:{self.surface}, base:{self.base}, pos:{self.pos}, pos1:{self.pos1})'\n",
    "\n",
    "# 1文を受け取ってsurfase, base, pos, pos1を取り出してmorphインスタンスにする関数\n",
    "def sentence2morph(sentence):\n",
    "    surfase, pos, pos1, *rest = repattern.split(sentence)\n",
    "    base = rest[4]\n",
    "    morph = Morph(surfase, base, pos, pos1)\n",
    "    return morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8b9860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morph(surface:人工, base:人工, pos:名詞, pos1:一般)\n",
      "Morph(surface:知能, base:知能, pos:名詞, pos1:一般)\n",
      "Morph(surface:（, base:（, pos:記号, pos1:括弧開)\n",
      "Morph(surface:じん, base:じん, pos:名詞, pos1:一般)\n",
      "Morph(surface:こうち, base:こうち, pos:名詞, pos1:一般)\n",
      "Morph(surface:のう, base:のう, pos:助詞, pos1:終助詞)\n",
      "Morph(surface:、, base:、, pos:記号, pos1:読点)\n",
      "Morph(surface:、, base:、, pos:記号, pos1:読点)\n",
      "Morph(surface:AI, base:*, pos:名詞, pos1:一般)\n",
      "Morph(surface:〈, base:〈, pos:記号, pos1:括弧開)\n",
      "Morph(surface:エーアイ, base:*, pos:名詞, pos1:固有名詞)\n",
      "Morph(surface:〉, base:〉, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:）, base:）, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:と, base:と, pos:助詞, pos1:格助詞)\n",
      "Morph(surface:は, base:は, pos:助詞, pos1:係助詞)\n",
      "Morph(surface:、, base:、, pos:記号, pos1:読点)\n",
      "Morph(surface:「, base:「, pos:記号, pos1:括弧開)\n",
      "Morph(surface:『, base:『, pos:記号, pos1:括弧開)\n",
      "Morph(surface:計算, base:計算, pos:名詞, pos1:サ変接続)\n",
      "Morph(surface:（, base:（, pos:記号, pos1:括弧開)\n",
      "Morph(surface:）, base:）, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:』, base:』, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:という, base:という, pos:助詞, pos1:格助詞)\n",
      "Morph(surface:概念, base:概念, pos:名詞, pos1:一般)\n",
      "Morph(surface:と, base:と, pos:助詞, pos1:並立助詞)\n",
      "Morph(surface:『, base:『, pos:記号, pos1:括弧開)\n",
      "Morph(surface:コンピュータ, base:コンピュータ, pos:名詞, pos1:一般)\n",
      "Morph(surface:（, base:（, pos:記号, pos1:括弧開)\n",
      "Morph(surface:）, base:）, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:』, base:』, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:という, base:という, pos:助詞, pos1:格助詞)\n",
      "Morph(surface:道具, base:道具, pos:名詞, pos1:一般)\n",
      "Morph(surface:を, base:を, pos:助詞, pos1:格助詞)\n",
      "Morph(surface:用い, base:用いる, pos:動詞, pos1:自立)\n",
      "Morph(surface:て, base:て, pos:助詞, pos1:接続助詞)\n",
      "Morph(surface:『, base:『, pos:記号, pos1:括弧開)\n",
      "Morph(surface:知能, base:知能, pos:名詞, pos1:一般)\n",
      "Morph(surface:』, base:』, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:を, base:を, pos:助詞, pos1:格助詞)\n",
      "Morph(surface:研究, base:研究, pos:名詞, pos1:サ変接続)\n",
      "Morph(surface:する, base:する, pos:動詞, pos1:自立)\n",
      "Morph(surface:計算, base:計算, pos:名詞, pos1:サ変接続)\n",
      "Morph(surface:機, base:機, pos:名詞, pos1:接尾)\n",
      "Morph(surface:科学, base:科学, pos:名詞, pos1:一般)\n",
      "Morph(surface:（, base:（, pos:記号, pos1:括弧開)\n",
      "Morph(surface:）, base:）, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:の, base:の, pos:助詞, pos1:連体化)\n",
      "Morph(surface:一, base:一, pos:名詞, pos1:数)\n",
      "Morph(surface:分野, base:分野, pos:名詞, pos1:一般)\n",
      "Morph(surface:」, base:」, pos:記号, pos1:括弧閉)\n",
      "Morph(surface:を, base:を, pos:助詞, pos1:格助詞)\n",
      "Morph(surface:指す, base:指す, pos:動詞, pos1:自立)\n",
      "Morph(surface:語, base:語, pos:名詞, pos1:一般)\n"
     ]
    }
   ],
   "source": [
    "morph_onesentence_list = [] # 1文毎のmorphのリスト\n",
    "is_one_sentence = False # 文が続いているかどうか\n",
    "with open('work/ai.ja.txt.cabocha') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        # *から始まる行はスルー\n",
    "        if line[0] == '*': continue\n",
    "        # EOSが来たら新しい文が次に来るはず\n",
    "        if line == 'EOS':\n",
    "            is_one_sentence = False\n",
    "            continue\n",
    "        # morph = Morph.sentence2morph(line)\n",
    "        morph = sentence2morph(line)\n",
    "        # 文中なら末尾のリストに突っ込む\n",
    "        if is_one_sentence:\n",
    "            morph_onesentence_list[-1].append(morph)\n",
    "        # 新しい文なら新しいリストを作って文中に入る\n",
    "        else:\n",
    "            morph_onesentence_list.append([morph])\n",
    "            is_one_sentence = True\n",
    "\n",
    "# 表示\n",
    "# pprintとかリストのインスタンスの出力だと__str__を通ってくれないっぽい\n",
    "for m in morph_onesentence_list[1]:\n",
    "    print(m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ad4de9",
   "metadata": {},
   "source": [
    "    assert文\n",
    "assert 条件文, 文字列\n",
    "\n",
    "で条件文がTrueのときにエラーを発生させて文字列を表示する\n",
    "\n",
    "デバッグに有用"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5bd44eb",
   "metadata": {},
   "source": [
    "    代入文 :="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c7033bb",
   "metadata": {},
   "source": [
    "    startswith, endswith\n",
    "文字列の最初と最後を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8b7c2",
   "metadata": {},
   "source": [
    "<h2 id=\"41-係り受け解析結果の読み込み文節係り受け\">41. 係り受け解析結果の読み込み（文節・係り受け）</h2>\n",
    "<p>40に加えて，文節を表すクラス<code class=\"language-plaintext highlighter-rouge\">Chunk</code>を実装せよ．このクラスは形態素（<code class=\"language-plaintext highlighter-rouge\">Morph</code>オブジェクト）のリスト（<code class=\"language-plaintext highlighter-rouge\">morphs</code>），係り先文節インデックス番号（<code class=\"language-plaintext highlighter-rouge\">dst</code>），係り元文節インデックス番号のリスト（<code class=\"language-plaintext highlighter-rouge\">srcs</code>）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文を<code class=\"language-plaintext highlighter-rouge\">Chunk</code>オブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5331c43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChunkDeco(index=0, dst=17, morphs=[MorphDeco(surface='人工', base='人工', pos='名詞', pos1='一般'), MorphDeco(surface='知能', base='知能', pos='名詞', pos1='一般')], srcs=[]),\n",
      " ChunkDeco(index=1, dst=17, morphs=[MorphDeco(surface='（', base='（', pos='記号', pos1='括弧開'), MorphDeco(surface='じん', base='じん', pos='名詞', pos1='一般'), MorphDeco(surface='こうち', base='こうち', pos='名詞', pos1='一般'), MorphDeco(surface='のう', base='のう', pos='助詞', pos1='終助詞'), MorphDeco(surface='、', base='、', pos='記号', pos1='読点'), MorphDeco(surface='、', base='、', pos='記号', pos1='読点')], srcs=[]),\n",
      " ChunkDeco(index=2, dst=3, morphs=[MorphDeco(surface='AI', base='*', pos='名詞', pos1='一般')], srcs=[]),\n",
      " ChunkDeco(index=3, dst=17, morphs=[MorphDeco(surface='〈', base='〈', pos='記号', pos1='括弧開'), MorphDeco(surface='エーアイ', base='*', pos='名詞', pos1='固有名詞'), MorphDeco(surface='〉', base='〉', pos='記号', pos1='括弧閉'), MorphDeco(surface='）', base='）', pos='記号', pos1='括弧閉'), MorphDeco(surface='と', base='と', pos='助詞', pos1='格助詞'), MorphDeco(surface='は', base='は', pos='助詞', pos1='係助詞'), MorphDeco(surface='、', base='、', pos='記号', pos1='読点')], srcs=[2]),\n",
      " ChunkDeco(index=4, dst=5, morphs=[MorphDeco(surface='「', base='「', pos='記号', pos1='括弧開'), MorphDeco(surface='『', base='『', pos='記号', pos1='括弧開'), MorphDeco(surface='計算', base='計算', pos='名詞', pos1='サ変接続')], srcs=[]),\n",
      " ChunkDeco(index=5, dst=9, morphs=[MorphDeco(surface='（', base='（', pos='記号', pos1='括弧開'), MorphDeco(surface='）', base='）', pos='記号', pos1='括弧閉'), MorphDeco(surface='』', base='』', pos='記号', pos1='括弧閉'), MorphDeco(surface='という', base='という', pos='助詞', pos1='格助詞')], srcs=[4]),\n",
      " ChunkDeco(index=6, dst=9, morphs=[MorphDeco(surface='概念', base='概念', pos='名詞', pos1='一般'), MorphDeco(surface='と', base='と', pos='助詞', pos1='並立助詞')], srcs=[]),\n",
      " ChunkDeco(index=7, dst=8, morphs=[MorphDeco(surface='『', base='『', pos='記号', pos1='括弧開'), MorphDeco(surface='コンピュータ', base='コンピュータ', pos='名詞', pos1='一般')], srcs=[]),\n",
      " ChunkDeco(index=8, dst=9, morphs=[MorphDeco(surface='（', base='（', pos='記号', pos1='括弧開'), MorphDeco(surface='）', base='）', pos='記号', pos1='括弧閉'), MorphDeco(surface='』', base='』', pos='記号', pos1='括弧閉'), MorphDeco(surface='という', base='という', pos='助詞', pos1='格助詞')], srcs=[7]),\n",
      " ChunkDeco(index=9, dst=10, morphs=[MorphDeco(surface='道具', base='道具', pos='名詞', pos1='一般'), MorphDeco(surface='を', base='を', pos='助詞', pos1='格助詞')], srcs=[5, 6, 8]),\n",
      " ChunkDeco(index=10, dst=12, morphs=[MorphDeco(surface='用い', base='用いる', pos='動詞', pos1='自立'), MorphDeco(surface='て', base='て', pos='助詞', pos1='接続助詞')], srcs=[9]),\n",
      " ChunkDeco(index=11, dst=12, morphs=[MorphDeco(surface='『', base='『', pos='記号', pos1='括弧開'), MorphDeco(surface='知能', base='知能', pos='名詞', pos1='一般'), MorphDeco(surface='』', base='』', pos='記号', pos1='括弧閉'), MorphDeco(surface='を', base='を', pos='助詞', pos1='格助詞')], srcs=[]),\n",
      " ChunkDeco(index=12, dst=13, morphs=[MorphDeco(surface='研究', base='研究', pos='名詞', pos1='サ変接続'), MorphDeco(surface='する', base='する', pos='動詞', pos1='自立')], srcs=[10, 11]),\n",
      " ChunkDeco(index=13, dst=14, morphs=[MorphDeco(surface='計算', base='計算', pos='名詞', pos1='サ変接続'), MorphDeco(surface='機', base='機', pos='名詞', pos1='接尾'), MorphDeco(surface='科学', base='科学', pos='名詞', pos1='一般')], srcs=[12]),\n",
      " ChunkDeco(index=14, dst=15, morphs=[MorphDeco(surface='（', base='（', pos='記号', pos1='括弧開'), MorphDeco(surface='）', base='）', pos='記号', pos1='括弧閉'), MorphDeco(surface='の', base='の', pos='助詞', pos1='連体化')], srcs=[13]),\n",
      " ChunkDeco(index=15, dst=16, morphs=[MorphDeco(surface='一', base='一', pos='名詞', pos1='数'), MorphDeco(surface='分野', base='分野', pos='名詞', pos1='一般'), MorphDeco(surface='」', base='」', pos='記号', pos1='括弧閉'), MorphDeco(surface='を', base='を', pos='助詞', pos1='格助詞')], srcs=[14]),\n",
      " ChunkDeco(index=16, dst=17, morphs=[MorphDeco(surface='指す', base='指す', pos='動詞', pos1='自立')], srcs=[15]),\n",
      " ChunkDeco(index=17, dst=-1, morphs=[MorphDeco(surface='語', base='語', pos='名詞', pos1='一般')], srcs=[0, 1, 3, 16, 17])]\n",
      "人工知能 -> 語\n",
      "（じんこうちのう、、 -> 語\n",
      "AI -> 〈エーアイ〉）とは、\n",
      "〈エーアイ〉）とは、 -> 語\n",
      "「『計算 -> （）』という\n",
      "（）』という -> 道具を\n",
      "概念と -> 道具を\n",
      "『コンピュータ -> （）』という\n",
      "（）』という -> 道具を\n",
      "道具を -> 用いて\n",
      "用いて -> 研究する\n",
      "『知能』を -> 研究する\n",
      "研究する -> 計算機科学\n",
      "計算機科学 -> （）の\n",
      "（）の -> 一分野」を\n",
      "一分野」を -> 指す\n",
      "指す -> 語\n"
     ]
    }
   ],
   "source": [
    "# デコレータを使うバージョン\n",
    "from pydantic.dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "\n",
    "@dataclass\n",
    "class ChunkDeco():\n",
    "    index: int # 何文節めか\n",
    "    dst: int # 係り先番号\n",
    "    morphs: list[MorphDeco] = field(default_factory=list)\n",
    "    # morphs: list[Morph] = []\n",
    "    srcs: list[int] = field(default_factory=list) # 係り元番号のリスト\n",
    "    \n",
    "    # 文をチャンクにする\n",
    "    @classmethod\n",
    "    def sentence2chunk(cls, sentence):\n",
    "        _, index, dst, bunsu, num = sentence.split(' ')\n",
    "        dst = dst.rstrip('D')\n",
    "        chunk = cls(index, dst)\n",
    "        return chunk\n",
    "    \n",
    "    # 係り元を探してsrcsに代入する\n",
    "    def find_dependency(chunks):\n",
    "        for chunk in chunks:\n",
    "            chunks[chunk.dst].srcs.append(chunk.index)\n",
    "        return chunks\n",
    "\n",
    "# Morphを継承して関数を追加\n",
    "class MorphDeco(MorphDeco):\n",
    "    # morphsを1つの文節にする\n",
    "    def morphs2phrase(morphs):\n",
    "        phrase = ''\n",
    "        for morph in morphs:\n",
    "            phrase += morph.surface\n",
    "        return phrase\n",
    "\n",
    "# チャンクの集まり（文ごと）のリストを作る\n",
    "# 問題42で再度使う必要があると思ったから関数にしたけど使わなかったので関数にした意味ないかも\n",
    "def make_chunks_list():\n",
    "    # 文節をchunkにして、1文をchunkのリストにする\n",
    "    chunks_list = [] \n",
    "    with open('work/ai.ja.txt.cabocha') as f:\n",
    "        chunks = [] # 1文のチャンクをまとめたリスト\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            # 文末を表す\n",
    "            if line == 'EOS':\n",
    "                # かかり元をsrcsを代入する\n",
    "                chunks = ChunkDeco.find_dependency(chunks)\n",
    "                chunks_list.append(chunks)\n",
    "                chunks = [] # 初期化\n",
    "                continue\n",
    "            \n",
    "            # 「*」から始まる文のみ取り出す\n",
    "            if line[0] == '*': \n",
    "                chunk = ChunkDeco.sentence2chunk(line)\n",
    "                chunks.append(chunk)\n",
    "            # cabochaで書かれた形態素解析を、文節ごとにmorphsに突っ込む\n",
    "            else:\n",
    "                chunk.morphs.append(MorphDeco.sentence2morph(line))            \n",
    "    return chunks_list\n",
    "\n",
    "chunks_list = make_chunks_list()\n",
    "\n",
    "# 1つめのChunkリストの表示\n",
    "from pprint import pprint\n",
    "pprint(chunks_list[1])\n",
    "# 1つ目の文について、文節ごとの表示をする\n",
    "for cl in chunks_list[1]:\n",
    "    # どこからも係っていない文節はスルー\n",
    "    if cl.dst == -1: continue\n",
    "    # もとの文節\n",
    "    phrase = MorphDeco.morphs2phrase(cl.morphs)\n",
    "    # 係り先の語\n",
    "    dependency_phrase = MorphDeco.morphs2phrase(chunks_list[1][cl.dst].morphs)\n",
    "    print(f'{phrase} -> {dependency_phrase}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ffd23de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk(index:0, dst:17, morphs:[<__main__.Morph object at 0x10754cb20>, <__main__.Morph object at 0x10754cbe0>], srcs:[])\n",
      "Chunk(index:1, dst:17, morphs:[<__main__.Morph object at 0x10754cc40>, <__main__.Morph object at 0x107c4c400>, <__main__.Morph object at 0x107c4c460>, <__main__.Morph object at 0x107c4c4c0>, <__main__.Morph object at 0x107c4c520>, <__main__.Morph object at 0x107c4c580>], srcs:[])\n",
      "Chunk(index:2, dst:3, morphs:[<__main__.Morph object at 0x107c4c640>], srcs:[])\n",
      "Chunk(index:3, dst:17, morphs:[<__main__.Morph object at 0x107c4c700>, <__main__.Morph object at 0x107c4c760>, <__main__.Morph object at 0x107c4c7c0>, <__main__.Morph object at 0x107c4c820>, <__main__.Morph object at 0x107c4c880>, <__main__.Morph object at 0x107c4c8e0>, <__main__.Morph object at 0x107c4c940>], srcs:[2])\n",
      "Chunk(index:4, dst:5, morphs:[<__main__.Morph object at 0x107c4ca00>, <__main__.Morph object at 0x107c4ca60>, <__main__.Morph object at 0x107c4cac0>], srcs:[])\n",
      "Chunk(index:5, dst:9, morphs:[<__main__.Morph object at 0x107c4cb80>, <__main__.Morph object at 0x107c4cbe0>, <__main__.Morph object at 0x107c4cc40>, <__main__.Morph object at 0x107c4cca0>], srcs:[4])\n",
      "Chunk(index:6, dst:9, morphs:[<__main__.Morph object at 0x107c4cd60>, <__main__.Morph object at 0x107c51190>], srcs:[])\n",
      "Chunk(index:7, dst:8, morphs:[<__main__.Morph object at 0x107c510d0>, <__main__.Morph object at 0x107c51070>], srcs:[])\n",
      "Chunk(index:8, dst:9, morphs:[<__main__.Morph object at 0x10750b2e0>, <__main__.Morph object at 0x10750b370>, <__main__.Morph object at 0x10750b3a0>, <__main__.Morph object at 0x10750b760>], srcs:[7])\n",
      "Chunk(index:9, dst:10, morphs:[<__main__.Morph object at 0x10750bdc0>, <__main__.Morph object at 0x10750be80>], srcs:[5, 6, 8])\n",
      "Chunk(index:10, dst:12, morphs:[<__main__.Morph object at 0x10750b520>, <__main__.Morph object at 0x10750bf10>], srcs:[9])\n",
      "Chunk(index:11, dst:12, morphs:[<__main__.Morph object at 0x10750bee0>, <__main__.Morph object at 0x10750b970>, <__main__.Morph object at 0x10750ba60>, <__main__.Morph object at 0x10750ba30>], srcs:[])\n",
      "Chunk(index:12, dst:13, morphs:[<__main__.Morph object at 0x106377fd0>, <__main__.Morph object at 0x106377730>], srcs:[10, 11])\n",
      "Chunk(index:13, dst:14, morphs:[<__main__.Morph object at 0x107556d90>, <__main__.Morph object at 0x107556f40>, <__main__.Morph object at 0x107556af0>], srcs:[12])\n",
      "Chunk(index:14, dst:15, morphs:[<__main__.Morph object at 0x107556e20>, <__main__.Morph object at 0x107556e80>, <__main__.Morph object at 0x107556f10>], srcs:[13])\n",
      "Chunk(index:15, dst:16, morphs:[<__main__.Morph object at 0x107556c70>, <__main__.Morph object at 0x107556d60>, <__main__.Morph object at 0x107c5a040>, <__main__.Morph object at 0x107c5a0a0>], srcs:[14])\n",
      "Chunk(index:16, dst:17, morphs:[<__main__.Morph object at 0x107c5a160>], srcs:[15])\n",
      "Chunk(index:17, dst:-1, morphs:[<__main__.Morph object at 0x107c5a220>], srcs:[0, 1, 3, 16, 17])\n",
      "人工知能 -> 語\n",
      "（じんこうちのう、、 -> 語\n",
      "AI -> 〈エーアイ〉）とは、\n",
      "〈エーアイ〉）とは、 -> 語\n",
      "「『計算 -> （）』という\n",
      "（）』という -> 道具を\n",
      "概念と -> 道具を\n",
      "『コンピュータ -> （）』という\n",
      "（）』という -> 道具を\n",
      "道具を -> 用いて\n",
      "用いて -> 研究する\n",
      "『知能』を -> 研究する\n",
      "研究する -> 計算機科学\n",
      "計算機科学 -> （）の\n",
      "（）の -> 一分野」を\n",
      "一分野」を -> 指す\n",
      "指す -> 語\n"
     ]
    }
   ],
   "source": [
    "# 別解\n",
    "# デコレータを使わないバージョン\n",
    "class Chunk():\n",
    "    def __init__(self, index, dst):\n",
    "        self.index = index\n",
    "        self.dst = dst\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "    # インスタンスの出力を可視化\n",
    "    def __str__(self) -> str:\n",
    "        return f'Chunk(index:{self.index}, dst:{self.dst}, morphs:{self.morphs}, srcs:{self.srcs})'\n",
    "\n",
    "def sentence2chunk(sentence):\n",
    "    _, index, dst, bunsu, num = sentence.split(' ')\n",
    "    index = int(index)\n",
    "    dst = int(dst.rstrip('D'))\n",
    "    chunk = Chunk(index, dst)\n",
    "    return chunk    \n",
    "\n",
    "def find_dependency(chunks):\n",
    "    for chunk in chunks:\n",
    "        chunks[chunk.dst].srcs.append(chunk.index)\n",
    "    return chunks\n",
    "\n",
    "# morphsを1つの文節にする\n",
    "def morphs2phrase(morphs):\n",
    "    phrase = ''\n",
    "    for morph in morphs:\n",
    "        phrase += morph.surface\n",
    "    return phrase\n",
    "\n",
    "def make_chunks_list():\n",
    "    # 文節をchunkにして、1文をchunkのリストにする\n",
    "    chunks_list = [] \n",
    "    with open('work/ai.ja.txt.cabocha') as f:\n",
    "        chunks = []  # 1文のチャンクをまとめたリスト\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            # 文末を表す\n",
    "            if line == 'EOS':\n",
    "                # かかり元をsrcsを代入する\n",
    "                chunks = find_dependency(chunks)\n",
    "                chunks_list.append(chunks)\n",
    "                chunks = []  # 初期化\n",
    "                continue\n",
    "            \n",
    "            # 「*」から始まる文のみ取り出す\n",
    "            if line[0] == '*': \n",
    "                chunk = sentence2chunk(line)\n",
    "                chunks.append(chunk)\n",
    "            # cabochaで書かれた形態素解析を、文節ごとにmorphsに突っ込む\n",
    "            else:\n",
    "                chunk.morphs.append(sentence2morph(line))            \n",
    "    return chunks_list\n",
    "\n",
    "chunks_list = make_chunks_list()\n",
    "\n",
    "# 1つめのChunkリストの表示\n",
    "for c in chunks_list[1]:\n",
    "    print(c)\n",
    "\n",
    "# 1つ目の文について、文節ごとの表示をする\n",
    "for cl in chunks_list[1]:\n",
    "    # どこからも係っていない文節はスルー\n",
    "    if cl.dst == -1: continue\n",
    "    # もとの文節\n",
    "    phrase = morphs2phrase(cl.morphs)\n",
    "    # 係り先の語\n",
    "    dependency_phrase = morphs2phrase(chunks_list[1][cl.dst].morphs)\n",
    "    print(f'{phrase} -> {dependency_phrase}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe31b7",
   "metadata": {},
   "source": [
    "<h2 id=\"42-係り元と係り先の文節の表示\">42. 係り元と係り先の文節の表示</h2>\n",
    "<p>係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "383483c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\t語\n",
      "じんこうちのう\t語\n",
      "AI\tエーアイとは\n",
      "エーアイとは\t語\n",
      "計算\tという\n",
      "という\t道具を\n",
      "概念と\t道具を\n",
      "コンピュータ\tという\n",
      "という\t道具を\n",
      "道具を\t用いて\n",
      "用いて\t研究する\n",
      "知能を\t研究する\n",
      "研究する\t計算機科学\n",
      "計算機科学\tの\n",
      "の\t一分野を\n",
      "一分野を\t指す\n",
      "指す\t語\n"
     ]
    }
   ],
   "source": [
    "# morphsを1つの文節にする\n",
    "def morphs2phrase(morphs, is_eliminate_sign=False):\n",
    "    phrase = ''\n",
    "    for morph in morphs:\n",
    "        # 問題42で記号を取り除くときに使う\n",
    "        if is_eliminate_sign:\n",
    "            if morph.pos == '記号':\n",
    "                continue\n",
    "        phrase += morph.surface\n",
    "    return phrase\n",
    "\n",
    "# 1つ目の文について、文節ごとの表示をする\n",
    "for cl in chunks_list[1]:\n",
    "    # どこからも係っていない文節はスルー\n",
    "    if cl.dst == -1: continue\n",
    "    # もとの文節、記号を取り除くオプション\n",
    "    phrase = morphs2phrase(cl.morphs, is_eliminate_sign=True)\n",
    "    # 係り先の語、記号を取り除くオプション\n",
    "    dependency_phrase = morphs2phrase(chunks_list[1][cl.dst].morphs, is_eliminate_sign=True)\n",
    "    # タブ区切りで出力\n",
    "    print(f'{phrase}\\t{dependency_phrase}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27234920",
   "metadata": {},
   "source": [
    "<h2 id=\"43-名詞を含む文節が動詞を含む文節に係るものを抽出\">43. 名詞を含む文節が動詞を含む文節に係るものを抽出</h2>\n",
    "<p>名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de15a6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "道具を\t用いて\n",
      "知能を\t研究する\n",
      "一分野を\t指す\n"
     ]
    }
   ],
   "source": [
    "def morphs2phrase(morphs, is_eliminate_sign=False):\n",
    "        phrase = ''\n",
    "        for morph in morphs:\n",
    "            # 問題42で記号を取り除くときに使う\n",
    "            if is_eliminate_sign:\n",
    "                if morph.pos == '記号':\n",
    "                    continue\n",
    "            phrase += morph.surface\n",
    "        return phrase\n",
    "\n",
    "# 1つ目の文について、文節ごとの表示をする\n",
    "for cl in chunks_list[1]:\n",
    "    # どこからも係っていない文節はスルー\n",
    "    if cl.dst == -1: continue\n",
    "    # もとの文節のmorphsが「名詞」を含むとき\n",
    "    if '名詞' in [m.pos for m in cl.morphs]:\n",
    "        # かかり先の文節のmorphsが「動詞」を含むとき\n",
    "        if '動詞' in [m.pos for m in chunks_list[1][cl.dst].morphs]:\n",
    "            # もとの文節、記号を取り除くオプション\n",
    "            phrase = morphs2phrase(cl.morphs, is_eliminate_sign=True)\n",
    "            # 係り先の語、記号を取り除くオプション\n",
    "            dependency_phrase = morphs2phrase(chunks_list[1][cl.dst].morphs, is_eliminate_sign=True)\n",
    "            # タブ区切りで出力\n",
    "            print(f'{phrase}\\t{dependency_phrase}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973db103",
   "metadata": {},
   "source": [
    "<h2 id=\"44-係り受け木の可視化\">44. 係り受け木の可視化</h2>\n",
    "<p>与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，<a href=\"http://www.graphviz.org/\">Graphviz</a>等を用いるとよい．</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36fcfd65",
   "metadata": {},
   "source": [
    "    graphvizモジュール\n",
    "* グラフを描画するモジュール\n",
    "* import graphviz\n",
    "* g = graphviz.Digraph(format=写真の拡張子, filename=名前)：有向グラフ\n",
    "  * 無向グラフはDigraphではなく、Graphにする\n",
    "* g.attr('node', shape='square', style='filled')：\n",
    "* g.node(label=表示文字列, name=名前)：ノード作成\n",
    "  * そのまま文字列を書くこともできるが、複数の同名に対処できないので、オプション引数でlabelとnameを使い分けると良い\n",
    "  * オプション引数として以下のものがある\n",
    "  * shape='square'：形\n",
    "  * color='green'：色\n",
    "* g.edge(文字列1, 文字列2)：ノードを接続する\n",
    "  * 文字列1は存在しないといけない\n",
    "  * 文字列2は新しく作っても既存でも良い\n",
    "  * ここで文字列はnodeのときに定義したlabelではなくname！！\n",
    "  * オプション引数は以下\n",
    "  * label=文字列：辺の横に表示する文字列\n",
    "* g.view()：グラフを表示(勝手に保存もするみたい)\n",
    "* g.render(パスと名前)：画像として保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bdb1c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"134pt\" height=\"282pt\"\n",
       " viewBox=\"0.00 0.00 134.00 281.75\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 277.75)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-277.75 130,-277.75 130,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"red\" points=\"46.62,-273.75 7.38,-273.75 7.38,-234.5 46.62,-234.5 46.62,-273.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-249.07\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-179.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-174.45\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-234.04C27,-226.3 27,-217.2 27,-208.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"30.5,-208.72 27,-198.72 23.5,-208.72 30.5,-208.72\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"73\" cy=\"-91\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"73\" y=\"-85.95\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.65,-162.23C42.29,-149.74 51.59,-132.26 59.24,-117.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.63,-119.96 64.23,-109.49 56.44,-116.68 62.63,-119.96\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.5\" y=\"-130.2\" font-family=\"Times,serif\" font-size=\"14.00\">+1</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-179.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-174.45\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M93.86,-161.41C90.27,-149.44 85.38,-133.2 81.25,-119.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.36,-118.62 78.13,-110.05 77.65,-120.64 84.36,-118.62\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"lime\" stroke=\"lime\" points=\"91,-36 55,-36 55,0 91,0 91,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"73\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">end</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M73,-72.81C73,-65.05 73,-55.68 73,-46.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"76.5,-47.03 73,-37.03 69.5,-47.03 76.5,-47.03\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x107c513a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "g = graphviz.Digraph(format='png', filename='test')\n",
    "g.node(label='start', name='0', shape='square', color='red')\n",
    "g.node(label='a', name='1')\n",
    "g.node(label='a', name='2')\n",
    "g.node(label='b', name='3')\n",
    "g.node(label='end', name='4', shape='square', color='lime', style='filled')\n",
    "g.edge('0', '1')\n",
    "g.edge('1', '3', label='+1')\n",
    "g.edge('2', '3')\n",
    "g.edge('3', '4')\n",
    "# g.view()\n",
    "# g.render('test')\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b0a6b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"134pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 134.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 130,-112 130,4 -4,4\"/>\n",
       "<!-- 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.65,-72.76C50.38,-64.46 45.08,-54.15 40.27,-44.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"42.99,-43.44 35.3,-36.15 36.77,-46.64 42.99,-43.44\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.35,-72.76C75.62,-64.46 80.92,-54.15 85.73,-44.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.23,-46.64 90.7,-36.15 83.01,-43.44 89.23,-46.64\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x107584fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "graph = Digraph(format=\"png\")\n",
    "graph.node(name=\"1\",label=\"A\")\n",
    "graph.node(name=\"2\",label=\"B\")\n",
    "graph.node(name=\"3\",label=\"B\")\n",
    "# edgeで参照するのはlabelではなく、nameのほう！\n",
    "graph.edge(\"1\", \"2\")\n",
    "graph.edge(\"1\", \"3\")\n",
    "graph\n",
    "# graph.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee0ac100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "g = graphviz.Digraph(format='png', filename='phrase_dependency')\n",
    "# 1つ目の文について、文節ごとの表示をする\n",
    "for cl in chunks_list[1]:\n",
    "    # どこからも係っていない文節はROOTにつなぐ\n",
    "    if cl.dst == -1:\n",
    "        g.edge(str(cl.index), 'ROOT')\n",
    "        continue\n",
    "    # もとの文節、記号を取り除くオプション\n",
    "    phrase = morphs2phrase(cl.morphs, is_eliminate_sign=True)\n",
    "    # 係り先の語、記号を取り除くオプション\n",
    "    dependency_phrase = morphs2phrase(chunks_list[1][cl.dst].morphs, is_eliminate_sign=True)\n",
    "    # 新しいノードを用意してつなぐ\n",
    "    # 同名の重複に対応するために各ノードにindexで名前をつけて、名前でノードを結ぶ\n",
    "    g.node(label=phrase, name=str(cl.index))\n",
    "    g.node(label=dependency_phrase, name=str(chunks_list[1][cl.dst].index))\n",
    "    g.edge(str(cl.index), str(chunks_list[1][cl.dst].index))\n",
    "# g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3db0b574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"576pt\" height=\"764pt\"\n",
       " viewBox=\"0.00 0.00 575.93 764.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 760)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-760 571.93,-760 571.93,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"47.45\" cy=\"-162\" rx=\"47.45\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"47.45\" y=\"-156.2\" font-family=\"Times,serif\" font-size=\"14.00\">人工知能</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>17</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"265.45\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"265.45\" y=\"-84.2\" font-family=\"Times,serif\" font-size=\"14.00\">語</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;17 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82.96,-149.6C123.85,-136.47 190.57,-115.04 231,-102.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"231.97,-105.1 240.42,-98.71 229.83,-98.44 231.97,-105.1\"/>\n",
       "</g>\n",
       "<!-- ROOT -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>ROOT</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"265.45\" cy=\"-18\" rx=\"36\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"265.45\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">ROOT</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;ROOT -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;ROOT</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M265.45,-71.7C265.45,-64.24 265.45,-55.32 265.45,-46.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.95,-47.1 265.45,-37.1 261.95,-47.1 268.95,-47.1\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"187.45\" cy=\"-162\" rx=\"74.74\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.45\" y=\"-156.2\" font-family=\"Times,serif\" font-size=\"14.00\">じんこうちのう</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;17 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M205.93,-144.41C216.66,-134.78 230.29,-122.55 241.79,-112.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"243.62,-115.39 248.73,-106.11 238.95,-110.18 243.62,-115.39\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"344.45\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"344.45\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">AI</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"344.45\" cy=\"-162\" rx=\"64.02\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"344.45\" y=\"-156.2\" font-family=\"Times,serif\" font-size=\"14.00\">エーアイとは</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M344.45,-215.7C344.45,-208.24 344.45,-199.32 344.45,-190.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"347.95,-191.1 344.45,-181.1 340.95,-191.1 347.95,-191.1\"/>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;17 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M325.72,-144.41C314.78,-134.72 300.87,-122.39 289.16,-112.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.91,-109.89 282.11,-105.88 287.27,-115.13 291.91,-109.89\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"314.45\" cy=\"-738\" rx=\"28.92\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"314.45\" y=\"-732.2\" font-family=\"Times,serif\" font-size=\"14.00\">計算</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"314.45\" cy=\"-666\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"314.45\" y=\"-660.2\" font-family=\"Times,serif\" font-size=\"14.00\">という</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M314.45,-719.7C314.45,-712.24 314.45,-703.32 314.45,-694.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"317.95,-695.1 314.45,-685.1 310.95,-695.1 317.95,-695.1\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"408.45\" cy=\"-594\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"408.45\" y=\"-588.2\" font-family=\"Times,serif\" font-size=\"14.00\">道具を</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M334.38,-650.15C347.59,-640.32 365.13,-627.26 379.82,-616.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"381.61,-618.6 387.54,-609.82 377.43,-612.99 381.61,-618.6\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"408.45\" cy=\"-522\" rx=\"37.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"408.45\" y=\"-516.2\" font-family=\"Times,serif\" font-size=\"14.00\">用いて</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M408.45,-575.7C408.45,-568.24 408.45,-559.32 408.45,-550.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"411.95,-551.1 408.45,-541.1 404.95,-551.1 411.95,-551.1\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"408.45\" cy=\"-666\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"408.45\" y=\"-660.2\" font-family=\"Times,serif\" font-size=\"14.00\">概念と</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M408.45,-647.7C408.45,-640.24 408.45,-631.32 408.45,-622.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"411.95,-623.1 408.45,-613.1 404.95,-623.1 411.95,-623.1\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"502.45\" cy=\"-738\" rx=\"65.48\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.45\" y=\"-732.2\" font-family=\"Times,serif\" font-size=\"14.00\">コンピュータ</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"502.45\" cy=\"-666\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.45\" y=\"-660.2\" font-family=\"Times,serif\" font-size=\"14.00\">という</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M502.45,-719.7C502.45,-712.24 502.45,-703.32 502.45,-694.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"505.95,-695.1 502.45,-685.1 498.95,-695.1 505.95,-695.1\"/>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M482.51,-650.15C469.3,-640.32 451.76,-627.26 437.07,-616.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"439.46,-612.99 429.35,-609.82 435.28,-618.6 439.46,-612.99\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"455.45\" cy=\"-450\" rx=\"47.45\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"455.45\" y=\"-444.2\" font-family=\"Times,serif\" font-size=\"14.00\">研究する</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M419.58,-504.41C425.06,-496.25 431.8,-486.22 437.94,-477.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"441.39,-479.21 444.06,-468.96 435.58,-475.31 441.39,-479.21\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"455.45\" cy=\"-378\" rx=\"56.71\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"455.45\" y=\"-372.2\" font-family=\"Times,serif\" font-size=\"14.00\">計算機科学</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M455.45,-431.7C455.45,-424.24 455.45,-415.32 455.45,-406.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"458.95,-407.1 455.45,-397.1 451.95,-407.1 458.95,-407.1\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"502.45\" cy=\"-522\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.45\" y=\"-516.2\" font-family=\"Times,serif\" font-size=\"14.00\">知能を</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M491.31,-504.41C485.83,-496.25 479.09,-486.22 472.95,-477.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"475.31,-475.31 466.83,-468.96 469.5,-479.21 475.31,-475.31\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>14</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"455.45\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"455.45\" y=\"-300.2\" font-family=\"Times,serif\" font-size=\"14.00\">の</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M455.45,-359.7C455.45,-352.24 455.45,-343.32 455.45,-334.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"458.95,-335.1 455.45,-325.1 451.95,-335.1 458.95,-335.1\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>15</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"455.45\" cy=\"-234\" rx=\"47.45\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"455.45\" y=\"-228.2\" font-family=\"Times,serif\" font-size=\"14.00\">一分野を</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M455.45,-287.7C455.45,-280.24 455.45,-271.32 455.45,-262.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"458.95,-263.1 455.45,-253.1 451.95,-263.1 458.95,-263.1\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>16</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"455.45\" cy=\"-162\" rx=\"28.92\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"455.45\" y=\"-156.2\" font-family=\"Times,serif\" font-size=\"14.00\">指す</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M455.45,-215.7C455.45,-208.24 455.45,-199.32 455.45,-190.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"458.95,-191.1 455.45,-181.1 451.95,-191.1 458.95,-191.1\"/>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M432.67,-150.43C427.69,-148.23 422.42,-145.97 417.45,-144 377.6,-128.17 330.92,-112.28 299.91,-102.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"301.23,-98.51 290.64,-98.73 299.05,-105.16 301.23,-98.51\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x107c1c640>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 表示\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e75a18",
   "metadata": {},
   "source": [
    "<h2 id=\"45-動詞の格パターンの抽出\">45. 動詞の格パターンの抽出</h2>\n",
    "<p>今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい．\n",
    "動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ．\n",
    "ただし，出力は以下の仕様を満たすようにせよ．</p>\n",
    "<ul>\n",
    "<li>動詞を含む文節において，最左の動詞の基本形を述語とする</li>\n",
    "<li>述語に係る助詞を格とする</li>\n",
    "<li>述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる</li>\n",
    "</ul>\n",
    "<p>「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える．\n",
    "この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>作り出す\tで は を\n",
    "</code></pre></div></div>\n",
    "<p>このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．</p>\n",
    "<ul>\n",
    "<li>コーパス中で頻出する述語と格パターンの組み合わせ</li>\n",
    "<li>「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f79d5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "用いる を \n",
      "する て を \n",
      "指す を \n",
      "代わる を に \n",
      "行う て に \n",
      "する と も \n",
      "述べる で は の に て \n",
      "する を で \n",
      "する を \n",
      "する を \n"
     ]
    }
   ],
   "source": [
    "# 形態素の組をを文節に復元する\n",
    "def morphs2phrase(morphs, is_eliminate_sign=False):\n",
    "        phrase = ''\n",
    "        for morph in morphs:\n",
    "            # 問題42で記号を取り除くときに使う\n",
    "            if is_eliminate_sign:\n",
    "                if morph.pos == '記号':\n",
    "                    continue\n",
    "            phrase += morph.surface\n",
    "        return phrase\n",
    "\n",
    "with open('work/corpus.txt', 'w') as fw:\n",
    "    for chunks in chunks_list:\n",
    "        # 1文ごとのチャンク\n",
    "        for chunk in chunks:\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos == '動詞':\n",
    "                    # 動詞の基本形を表示\n",
    "                    fw.write(f'{morph.base} ')\n",
    "                    # 係り元番号を回す\n",
    "                    for i in chunk.srcs:\n",
    "                        # 係り元の形態素を１つずつ見て助詞のみ取り出す\n",
    "                        for op_morph in chunks[i].morphs:\n",
    "                            if op_morph.pos == '助詞':\n",
    "                                fw.write(f'{op_morph.surface} ')\n",
    "                    fw.write('\\n')\n",
    "                    break\n",
    "# 10行表示\n",
    "!cat work/corpus.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "78678d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50 する を \n",
      "  17 する が \n",
      "  15 する に \n",
      "  13 する と \n",
      "   9 よる に \n",
      "   8 行う を \n",
      "   8 する \n",
      "   7 する は を \n",
      "   6 基づく に \n",
      "   6 呼ぶ と \n"
     ]
    }
   ],
   "source": [
    "# 多い順に並べてみた\n",
    "!cat work/corpus.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "6b6d2d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 行う を \n",
      "   4 なる は に \n",
      "   3 なる が と \n",
      "   2 なる に \n",
      "   1 行う を に を \n",
      "   1 行う まで を に \n",
      "   1 行う は を をめぐって て \n",
      "   1 行う は を て \n",
      "   1 行う は を \n",
      "   1 行う により に を \n"
     ]
    }
   ],
   "source": [
    "# 行う、なる、与える　という動詞の格パターンをコーパス中で出現頻度の高い順に並べる\n",
    "!awk '$1 == \"行う\" || $1 == \"なる\" || $1 == \"与える\" {print $0}' work/corpus.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "dc922134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 行う を \n",
      "   4 なる は に \n",
      "   3 なる が と \n",
      "   2 なる に \n",
      "   1 行う を に を \n",
      "   1 行う まで を に \n",
      "   1 行う は を をめぐって て \n",
      "   1 行う は を て \n",
      "   1 行う は を \n",
      "   1 行う により に を \n"
     ]
    }
   ],
   "source": [
    "# 正規表現でマッチして抜き出す\n",
    "!grep -E '^(行う |なる |与える )' work/corpus.txt | sort | uniq -c | sort -rn | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89676108",
   "metadata": {},
   "source": [
    "<h2 id=\"46-動詞の格フレーム情報の抽出\">46. 動詞の格フレーム情報の抽出</h2>\n",
    "<p>45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．</p>\n",
    "<ul>\n",
    "<li>項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）</li>\n",
    "<li>述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる</li>\n",
    "</ul>\n",
    "<p>「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える．\n",
    "この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>作り出す\tで は を\t会議で ジョンマッカーシーは 用語を\n",
    "</code></pre></div></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "319a36a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる を 道具を\n",
      "する て を 用いて 『知能』を\n",
      "指す を 一分野」を\n",
      "代わる を に 知的行動を 人間に\n",
      "行う て に 代わって コンピューターに\n",
      "する と も 研究分野」とも 研究分野」とも\n",
      "述べる で は の に て 解説で、 佐藤理史は 次のように 次のように 述べている\n",
      "する を で 知的能力を コンピュータ上で\n",
      "する を 推論・判断を\n",
      "する を 画像データを\n"
     ]
    }
   ],
   "source": [
    "with open('work/corpus2.txt', 'w') as fw:\n",
    "    for chunks in chunks_list:\n",
    "        # 1文ごとのチャンク\n",
    "        for chunk in chunks:\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos == '動詞':\n",
    "                    # 動詞の基本形を表示\n",
    "                    fw.write(f'{morph.base} ')\n",
    "                    # 係り元の文節をそのまま取り出してリストに入れる\n",
    "                    # for文を２回回すのがもったいないので保存して後で表示\n",
    "                    tmp_phrases = [] # here\n",
    "                    # 係り元番号を回す\n",
    "                    for i in chunk.srcs:\n",
    "                        # 係り元の形態素を１つずつ見て助詞のみ取り出す\n",
    "                        for op_morph in chunks[i].morphs:\n",
    "                            if op_morph.pos == '助詞':\n",
    "                                tmp_phrases.append(morphs2phrase(chunks[i].morphs))\n",
    "                                fw.write(f'{op_morph.surface} ')\n",
    "                    # リストの入れた係り元の文節を空白区切りで取り出す\n",
    "                    fw.write(f'{\" \".join(tmp_phrases)}\\n') # here\n",
    "                    break\n",
    "# 10行表示\n",
    "!cat work/corpus2.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1b81f",
   "metadata": {},
   "source": [
    "<h2 id=\"47-機能動詞構文のマイニング\">47. 機能動詞構文のマイニング</h2>\n",
    "<p>動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．</p>\n",
    "<ul>\n",
    "<li>「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする</li>\n",
    "<li>述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる</li>\n",
    "<li>述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる</li>\n",
    "<li>述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）</li>\n",
    "</ul>\n",
    "<p>例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>学習を行う\tに を\t元に 経験を\n",
    "</code></pre></div></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fae51527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推論をする \n",
      "記述をする と 主体と\n",
      "注目を集める が サポートベクターマシンが\n",
      "経験を行う に を 元に 学習を\n",
      "学習を行う を に 経験を 元に\n",
      "流行を超える \n",
      "学習を繰り返す \n",
      "統計をする で は を に を通して ACT-Rでは ACT-Rでは 推論ルールを 元に 生成規則を通して\n",
      "進化を見せる は て において て 敵対的生成ネットワークは 加えて 生成技術において 見せている\n",
      "開発を行う は エイダ・ラブレスは\n",
      "プログラミングをする は 彼はまた\n",
      "テストをする \n",
      "来談を行う \n",
      "プログラミングをする は アランカルメラウアーは\n",
      "投資を上回る が コストが\n",
      "意味をする から に対して ここから 非構造化データに対して\n",
      "処理を行う \n",
      "意味をする に データに\n",
      "研究を進める て 費やして\n",
      "命令をする で 機構で\n",
      "運転をする に 元に\n",
      "特許をする まで に が 2018年までに 2018年までに 日本が\n",
      "研究をする \n",
      "運転をする て に 基づいて 柔軟に\n",
      "注目を集める から は ことから ファジィは\n",
      "成功を受ける \n",
      "開発を抑える \n",
      "制御をする から 少なさから\n",
      "表現をする \n",
      "進歩を担う \n",
      "専用を使う \n",
      "研究を続ける が て ジェフホーキンスが 向けて\n",
      "行動を用いる は は て これは ものではなく 用いている\n",
      "関連を導き出す \n",
      "認識を持つ \n",
      "注目を集める に 急速に\n",
      "普及を受ける \n",
      "投資を行う に で 全世界的に 民間企業主導で\n",
      "探索を行う で 無報酬で\n",
      "推論をする て 経て\n",
      "共同を始める とも マックスプランク研究所とも\n",
      "研究を行う は て Googleは 行っている\n",
      "研究をする で は で 中国では 中国では 官民一体で\n",
      "実験をする \n",
      "研究をする で 日本で\n",
      "投資をする は まで に 韓国は 2022年までに 2022年までに\n",
      "反乱を起こす て に対して 於いて 人間に対して\n",
      "弾圧を併せ持つ \n",
      "監視を行う まで に 歩行者まで 人工知能に\n",
      "監視を恐れる \n",
      "監視をする \n",
      "差別を認める \n",
      "研究をする \n",
      "展開を変える \n",
      "戦争をする \n",
      "判断を介す から 観点から\n",
      "開発を出す に 2012年に\n",
      "禁止を求める に は が て 4月には 4月には ヒューマン・ライツ・ウォッチが 求めている\n",
      "運用をめぐる \n",
      "開発を行う は をめぐって て 米国中国ロシアは 軍事利用をめぐって 行っている\n",
      "記録をする \n",
      "自律を使う \n",
      "試験を行う \n",
      "追及を受ける て で は とともに と で 暴露されており 公聴会では 公聴会では とともに 拒否すると 整合性で\n",
      "共同をする が Microsoftが\n",
      "監視をする が によって 中国が AIによって\n",
      "解任をする て は 含まれており Google社員らは\n",
      "解散をする は が で Googleは 倫理委員会が 理由で\n",
      "実現をする \n",
      "話をする ば は て よれば 哲学者は している\n",
      "意思を行う \n",
      "勘違いをする \n",
      "議論を行う で は まで て 対談で 須藤は これまで 行ってきました\n"
     ]
    }
   ],
   "source": [
    "with open('work/corpus3.txt', 'w') as fw:\n",
    "    for chunks in chunks_list:\n",
    "        # 1文ごとのチャンク\n",
    "        for chunk in chunks:\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos == '動詞':\n",
    "                    # 係り元番号を回す\n",
    "                    for srcs_i in chunk.srcs:\n",
    "                        # 「サ変接続 + を」がかかるときのみ表示する\n",
    "                        for morph_i, op_morph in enumerate(chunks[srcs_i].morphs):\n",
    "                            if op_morph.pos1 == 'サ変接続' and chunks[srcs_i].morphs[morph_i-1].base == 'を':\n",
    "                                # サ変接続 + を + 動詞\n",
    "                                print(op_morph.base, end='')\n",
    "                                print(chunks[srcs_i].morphs[morph_i-1].base, end='')\n",
    "                                print(morph.base, end=' ')\n",
    "                                \n",
    "                                # 係り元の文節をそのまま取り出してリストに入れる\n",
    "                                # for文を２回回すのがもったいないので保存して後で表示\n",
    "                                tmp_phrases = []\n",
    "                                for srcs_j in chunk.srcs:\n",
    "                                    for m in chunks[srcs_j].morphs:\n",
    "                                        # 自分自身は除く助詞を表示\n",
    "                                        if m.pos == '助詞':\n",
    "                                            # 自らを含む場合はとばす\n",
    "                                            if op_morph.base in morphs2phrase(chunks[srcs_j].morphs, is_eliminate_sign=True):\n",
    "                                                continue\n",
    "                                            print(m.surface, end=' ')\n",
    "                                            tmp_phrases.append(morphs2phrase(chunks[srcs_j].morphs, is_eliminate_sign=True))\n",
    "                                    # fw.write(f'{op_morph.surface} ')\n",
    "                                # リストの入れた係り元の文節を空白区切りで取り出す\n",
    "                                # if tmp_phrases != []:\n",
    "                                print(' '.join(tmp_phrases))\n",
    "                        # fw.write(f'{\" \".join(tmp_phrases)}\\n') # here\n",
    "                    # １個めの動詞で終わらせるためにここで抜ける\n",
    "                    break\n",
    "# 10行表示\n",
    "# !cat work/corpus3.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1744d",
   "metadata": {},
   "source": [
    "<h2 id=\"48-名詞から根へのパスの抽出\">48. 名詞から根へのパスの抽出</h2>\n",
    "<p>文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ．\n",
    "ただし，構文木上のパスは以下の仕様を満たすものとする．</p>\n",
    "<ul>\n",
    "<li>各文節は（表層形の）形態素列で表現する</li>\n",
    "<li>パスの開始文節から終了文節に至るまで，各文節の表現を”<code class=\"language-plaintext highlighter-rouge\"> -&gt; </code>“で連結する</li>\n",
    "</ul>\n",
    "<p>「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える．\n",
    "CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>ジョンマッカーシーは -&gt; 作り出した\n",
    "AIに関する -&gt; 最初の -&gt; 会議で -&gt; 作り出した\n",
    "最初の -&gt; 会議で -&gt; 作り出した\n",
    "会議で -&gt; 作り出した\n",
    "人工知能という -&gt; 用語を -&gt; 作り出した\n",
    "用語を -&gt; 作り出した\n",
    "</code></pre></div></div>\n",
    "<p>KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>ジョンマッカーシーは -&gt; 作り出した\n",
    "ＡＩに -&gt; 関する -&gt; 会議で -&gt; 作り出した\n",
    "会議で -&gt; 作り出した\n",
    "人工知能と -&gt; いう -&gt; 用語を -&gt; 作り出した\n",
    "用語を -&gt; 作り出した\n",
    "</code></pre></div></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "4c6785fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジョンマッカーシーは -> 作り出した\n",
      "AIに関する -> 最初の -> 会議で -> 作り出した\n",
      "最初の -> 会議で -> 作り出した\n",
      "会議で -> 作り出した\n",
      "人工知能という -> 用語を -> 作り出した\n",
      "用語を -> 作り出した\n"
     ]
    }
   ],
   "source": [
    "# 名詞からROOTまでたどって、文字列のリストを返す\n",
    "def following_ROOT(chunks, chunk) -> list[str]:\n",
    "    follow_list = []\n",
    "    # 最初だけwhileの外で\n",
    "    # 名詞を含む文節をリストのはじめに入れる\n",
    "    follow_list.append(morphs2phrase(chunk.morphs, is_eliminate_sign=True))\n",
    "    next_id = chunk.dst # 次の文節のID\n",
    "    # ROOTまでたどり着いたら終了\n",
    "    while next_id != -1:\n",
    "        # 文節をリストに突っ込む\n",
    "        follow_list.append(morphs2phrase(chunks[next_id].morphs, is_eliminate_sign=True))\n",
    "        next_id = chunks[next_id].dst # 次の文節のID\n",
    "    return follow_list\n",
    "\n",
    "# 34行目のジョンマッカーシーの文を表示\n",
    "for chunk in chunks_list[33]:\n",
    "    # 名詞を含む文節を取り出す\n",
    "    if '名詞' in [m.pos for m in chunk.morphs]:\n",
    "        print(' -> '.join(following_ROOT(chunks_list[33], chunk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9262897",
   "metadata": {},
   "source": [
    "<h2 id=\"49-名詞間の係り受けパスの抽出\">49. 名詞間の係り受けパスの抽出</h2>\n",
    "<p>文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が\\(i\\)と\\(j\\)（\\(i &lt; j\\)）のとき，係り受けパスは以下の仕様を満たすものとする．</p>\n",
    "<ul>\n",
    "<li>問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を”<code class=\"language-plaintext highlighter-rouge\"> -&gt; </code>“で連結して表現する</li>\n",
    "<li>文節\\(i\\)と\\(j\\)に含まれる名詞句はそれぞれ，XとYに置換する</li>\n",
    "</ul>\n",
    "<p>また，係り受けパスの形状は，以下の2通りが考えられる．</p>\n",
    "<ul>\n",
    "<li>文節\\(i\\)から構文木の根に至る経路上に文節\\(j\\)が存在する場合: 文節\\(i\\)から文節\\(j\\)のパスを表示</li>\n",
    "<li>上記以外で，文節\\(i\\)と文節\\(j\\)から構文木の根に至る経路上で共通の文節\\(k\\)で交わる場合: 文節\\(i\\)から文節\\(k\\)に至る直前のパスと文節\\(j\\)から文節\\(k\\)に至る直前までのパス，文節\\(k\\)の内容を”<code class=\"language-plaintext highlighter-rouge\"> | </code>“で連結して表示</li>\n",
    "</ul>\n",
    "<p>「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える．\n",
    "CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Xは | Yに関する -&gt; 最初の -&gt; 会議で | 作り出した\n",
    "Xは | Yの -&gt; 会議で | 作り出した\n",
    "Xは | Yで | 作り出した\n",
    "Xは | Yという -&gt; 用語を | 作り出した\n",
    "Xは | Yを | 作り出した\n",
    "Xに関する -&gt; Yの\n",
    "Xに関する -&gt; 最初の -&gt; Yで\n",
    "Xに関する -&gt; 最初の -&gt; 会議で | Yという -&gt; 用語を | 作り出した\n",
    "Xに関する -&gt; 最初の -&gt; 会議で | Yを | 作り出した\n",
    "Xの -&gt; Yで\n",
    "Xの -&gt; 会議で | Yという -&gt; 用語を | 作り出した\n",
    "Xの -&gt; 会議で | Yを | 作り出した\n",
    "Xで | Yという -&gt; 用語を | 作り出した\n",
    "Xで | Yを | 作り出した\n",
    "Xという -&gt; Yを\n",
    "</code></pre></div></div>\n",
    "<p>KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Xは | Yに -&gt; 関する -&gt; 会議で | 作り出した。\n",
    "Xは | Yで | 作り出した。\n",
    "Xは | Yと -&gt; いう -&gt; 用語を | 作り出した。\n",
    "Xは | Yを | 作り出した。\n",
    "Xに -&gt; 関する -&gt; Yで\n",
    "Xに -&gt; 関する -&gt; 会議で | Yと -&gt; いう -&gt; 用語を | 作り出した。\n",
    "Xに -&gt; 関する -&gt; 会議で | Yを | 作り出した。\n",
    "Xで | Yと -&gt; いう -&gt; 用語を | 作り出した。\n",
    "Xで | Yを | 作り出した。\n",
    "Xと -&gt; いう -&gt; Yを\n",
    "</code></pre></div></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "a83172d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
      "Xは | Yの -> 会議で | 作り出した\n",
      "Xは | Yで | 作り出した\n",
      "Xは | Yという -> 用語を | 作り出した\n",
      "Xは | Yを | 作り出した\n",
      "Xに関する -> Yの\n",
      "Xに関する -> 最初の -> Yで\n",
      "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
      "Xの -> Yで\n",
      "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xの -> 会議で | Yを | 作り出した\n",
      "Xで | Yという -> 用語を | 作り出した\n",
      "Xで | Yを | 作り出した\n",
      "Xという -> Yを\n"
     ]
    }
   ],
   "source": [
    "# 名詞からROOTまでたどって、文字列のリストを返す\n",
    "# 名詞をX, Yに置き換える\n",
    "def following_ROOT_XY(chunks, chunk, x_or_y) -> list[str]:\n",
    "    follow_list = []\n",
    "    # 最初だけwhileの外で\n",
    "    # 名詞を含む文節をリストのはじめに入れる\n",
    "    # ここで名詞をX, Yに置き換える\n",
    "    noun_phrase = ''\n",
    "    is_first_noun = True\n",
    "    for morph in chunk.morphs:\n",
    "        # 最初の名詞だけX, Yに変換\n",
    "        if morph.pos == '名詞' and is_first_noun:\n",
    "            noun_phrase += x_or_y\n",
    "            is_first_noun = False\n",
    "        # ２つ目以降のつながっている名詞はスルー\n",
    "        elif morph.pos == '名詞':\n",
    "            continue\n",
    "        elif morph.pos == '記号':\n",
    "            continue\n",
    "        else:\n",
    "            noun_phrase += morph.surface\n",
    "    follow_list.append(noun_phrase)\n",
    "    next_id = chunk.dst # 次の文節のID\n",
    "    # ROOTまでたどり着いたら終了\n",
    "    while next_id != -1:\n",
    "        # 文節をリストに突っ込む\n",
    "        follow_list.append(morphs2phrase(chunks[next_id].morphs, is_eliminate_sign=True))\n",
    "        next_id = chunks[next_id].dst # 次の文節のID\n",
    "    return follow_list\n",
    "\n",
    "# リストXがリストYを包含しているかを判断して、包含地点を返す\n",
    "def inclusion(x_list, y_list, x_list_XY, y_list_XY) -> list:\n",
    "    path_list = []\n",
    "    # YをXが包含しているかどうかを確かめる\n",
    "    for i, y in enumerate(y_list):\n",
    "        # 包含していないので空リストを返して終了\n",
    "        if y not in x_list:\n",
    "            return []\n",
    "    # 包含している場合は、Xを順番にたどっていって合流したら最初だけ表示\n",
    "    for i, x in enumerate(x_list):\n",
    "        # Xが合流していないときはリストに追加\n",
    "        if x not in y_list:\n",
    "            path_list.append(x_list_XY[i])\n",
    "        # XとYが合流したらYの最初を追加して終了\n",
    "        else:\n",
    "            path_list.append(y_list_XY[0])\n",
    "            return path_list\n",
    "        \n",
    "# リストXとリストYが途中で合流するかどうかを判断する関数\n",
    "# （Xの独立しているパス、Yの独立しているパス、合流した単語）を返す\n",
    "def confluence(x_list, y_list, x_list_XY, y_list_XY):\n",
    "    path_x = [] # Xの独立しているパス\n",
    "    path_y = [] # Yの独立しているパス\n",
    "    union = ''  # 合流地点の語\n",
    "    for x in x_list:\n",
    "        # Xの要素がYに入っていたらそこから合流していることになる\n",
    "        if x in y_list:\n",
    "            path_x = x_list_XY[:x_list.index(x)]\n",
    "            path_y = y_list_XY[:y_list.index(x)]\n",
    "            union = x\n",
    "            return (path_x, path_y, union)\n",
    "\n",
    "# 33行目のジョン・マッカーシーの文について、名詞句のみを取り出す\n",
    "noun_chunks = []\n",
    "for chunk in chunks_list[33]:\n",
    "    # 名詞を含むチャンクを抜き出してリストに入れる\n",
    "    if '名詞' in [m.pos for m in chunk.morphs]:\n",
    "        noun_chunks.append(chunk)\n",
    "\n",
    "# 名詞チャンクを被らないように2回回す\n",
    "for i, x in enumerate(noun_chunks):\n",
    "    for j, y in enumerate(noun_chunks[i+1:]):\n",
    "        # それぞれの名詞句がROOTまでのたどるパスを求める\n",
    "        x_list = following_ROOT(chunks_list[33], x)\n",
    "        y_list = following_ROOT(chunks_list[33], y)\n",
    "        # 名詞をX, Yに変換したリスト\n",
    "        x_list_XY = following_ROOT_XY(chunks_list[33], x, 'X')\n",
    "        y_list_XY = following_ROOT_XY(chunks_list[33], y, 'Y')\n",
    "        \n",
    "        # XがYをで包含する場合\n",
    "        path_list = inclusion(x_list, y_list, x_list_XY, y_list_XY)\n",
    "        if path_list != []:\n",
    "            print(' -> '.join(path_list))\n",
    "        # XとYが途中から合流する場合（path_listが空になっているはず）\n",
    "        else:\n",
    "            path_x, path_y, union = confluence(x_list, y_list, x_list_XY, y_list_XY)\n",
    "            print(' -> '.join(path_x), '|', ' -> '.join(path_y), '|', union)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
